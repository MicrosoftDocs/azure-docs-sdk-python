### YamlMime:PythonClass
uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams
name: InputTokenCountParams
fullName: azure.ai.agentserver.core.models.openai.InputTokenCountParams
module: azure.ai.agentserver.core.models.openai
constructor:
  syntax: InputTokenCountParams()
methods:
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.clear
  name: clear
  signature: clear() -> None.  Remove all items from D.
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.copy
  name: copy
  signature: copy() -> a shallow copy of D
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.fromkeys
  name: fromkeys
  summary: Create a new dictionary with keys from iterable and values set to value.
  signature: fromkeys(value=None, /)
  positionalOnlyParameters:
  - name: iterable
    isRequired: true
  - name: value
    defaultValue: None
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.get
  name: get
  summary: Return the value for key if key is in the dictionary, else default.
  signature: get(key, default=None, /)
  positionalOnlyParameters:
  - name: key
    isRequired: true
  - name: default
    defaultValue: None
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.items
  name: items
  signature: items() -> a set-like object providing a view on D's items
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.keys
  name: keys
  signature: keys() -> a set-like object providing a view on D's keys
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.pop
  name: pop
  summary: 'If the key is not found, return the default if given; otherwise,

    raise a KeyError.'
  signature: pop(k, [d]) -> v, remove specified key and return the corresponding value.
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.popitem
  name: popitem
  summary: 'Remove and return a (key, value) pair as a 2-tuple.


    Pairs are returned in LIFO (last-in, first-out) order.

    Raises KeyError if the dict is empty.'
  signature: popitem()
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.setdefault
  name: setdefault
  summary: 'Insert key with a value of default if key is not in the dictionary.


    Return the value for key if key is in the dictionary, else default.'
  signature: setdefault(key, default=None, /)
  positionalOnlyParameters:
  - name: key
    isRequired: true
  - name: default
    defaultValue: None
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.update
  name: update
  summary: 'If E is present and has a .keys() method, then does:  for k in E: D[k]
    = E[k]

    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] =
    v

    In either case, this is followed by: for k in F:  D[k] = F[k]'
  signature: update([E], **F) -> None.  Update D from dict/iterable E and F.
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.values
  name: values
  signature: values() -> an object providing a view on D's values
attributes:
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.conversation
  name: conversation
  summary: 'The conversation that this response belongs to.


    Items from this conversation are prepended to *input_items* for this response

    request. Input items and output items from this response are automatically added

    to this conversation after this response completes.'
  signature: 'conversation: str | ResponseConversationParam | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.input
  name: input
  summary: Text, image, or file inputs to the model, used to generate a response
  signature: 'input: str | Iterable[EasyInputMessageParam | Message | ResponseOutputMessageParam
    | ResponseFileSearchToolCallParam | ResponseComputerToolCallParam | ComputerCallOutput
    | ResponseFunctionWebSearchParam | ResponseFunctionToolCallParam | FunctionCallOutput
    | ResponseReasoningItemParam | ImageGenerationCall | ResponseCodeInterpreterToolCallParam
    | LocalShellCall | LocalShellCallOutput | ShellCall | ShellCallOutput | ApplyPatchCall
    | ApplyPatchCallOutput | McpListTools | McpApprovalRequest | McpApprovalResponse
    | McpCall | ResponseCustomToolCallOutputParam | ResponseCustomToolCallParam |
    ItemReference] | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.instructions
  name: instructions
  summary: 'A system (or developer) message inserted into the model''s context. When
    used

    along with *previous_response_id*, the instructions from a previous response

    will not be carried over to the next response. This makes it simple to swap out

    system (or developer) messages in new responses.'
  signature: 'instructions: str | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.model
  name: model
  summary: 'Model ID used to generate the response, like *gpt-4o* or *o3*.


    OpenAI offers a wide range of models with different capabilities, performance

    characteristics, and price points. Refer to the

    [model guide](https://platform.openai.com/docs/models) to browse and compare

    available models.'
  signature: 'model: str | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.parallel_tool_calls
  name: parallel_tool_calls
  summary: Whether to allow the model to run tool calls in parallel.
  signature: 'parallel_tool_calls: bool | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.previous_response_id
  name: previous_response_id
  summary: 'The unique ID of the previous response to the model.


    Use this to create multi-turn conversations. Learn more about

    [conversation state](https://platform.openai.com/docs/guides/conversation-state).

    Cannot be used in conjunction with *conversation*.'
  signature: 'previous_response_id: str | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.reasoning
  name: reasoning
  summary: '**gpt-5 and o-series models only** Configuration options for

    [reasoning models](https://platform.openai.com/docs/guides/reasoning).'
  signature: 'reasoning: Reasoning | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.text
  name: text
  summary: "Configuration options for a text response from the model.\n\nCan be plain\
    \ text or structured JSON data. Learn more:\n\n* [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\
    \ \n\n* [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) "
  signature: 'text: Text | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.tool_choice
  name: tool_choice
  summary: 'How the model should select which tool (or tools) to use when generating
    a

    response. See the *tools* parameter to see how to specify which tools the model

    can call.'
  signature: 'tool_choice: Literal[''none'', ''auto'', ''required''] | ToolChoiceAllowedParam
    | ToolChoiceTypesParam | ToolChoiceFunctionParam | ToolChoiceMcpParam | ToolChoiceCustomParam
    | ToolChoiceApplyPatchParam | ToolChoiceShellParam | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.tools
  name: tools
  summary: 'An array of tools the model may call while generating a response.


    You can specify which tool to use by setting the *tool_choice* parameter.'
  signature: 'tools: Iterable[FunctionToolParam | FileSearchToolParam | ComputerToolParam
    | WebSearchToolParam | Mcp | CodeInterpreter | ImageGeneration | LocalShell |
    FunctionShellToolParam | CustomToolParam | WebSearchPreviewToolParam | ApplyPatchToolParam]
    | None'
- uid: azure.ai.agentserver.core.models.openai.InputTokenCountParams.truncation
  name: truncation
  summary: 'The truncation strategy to use for the model response.


    * *auto*: If the input to this Response exceeds the model''s context window size,
    the model will truncate the response to fit the context window by dropping items
    from the beginning of the conversation. - *disabled* (default): If the input size
    will exceed the context window size for a model, the request will fail with a
    400 error. '
  signature: 'truncation: Literal[''auto'', ''disabled'']'
