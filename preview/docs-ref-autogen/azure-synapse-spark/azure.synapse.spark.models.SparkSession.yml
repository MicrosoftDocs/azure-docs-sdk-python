### YamlMime:PythonClass
uid: azure.synapse.spark.models.SparkSession
name: SparkSession
fullName: azure.synapse.spark.models.SparkSession
module: azure.synapse.spark.models
inheritances:
- msrest.serialization.Model
summary: 'SparkSession.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'SparkSession(*, id: int, livy_info: SparkSessionState | None = None, name:
    str | None = None, workspace_name: str | None = None, spark_pool_name: str | None
    = None, submitter_name: str | None = None, submitter_id: str | None = None, artifact_id:
    str | None = None, job_type: str | SparkJobType | None = None, result: str | SparkSessionResultType
    | None = None, scheduler: SparkScheduler | None = None, plugin: SparkServicePlugin
    | None = None, errors: List[SparkServiceError] | None = None, tags: Dict[str,
    str] | None = None, app_id: str | None = None, app_info: Dict[str, str] | None
    = None, state: str | LivyStates | None = None, log_lines: List[str] | None = None,
    **kwargs)'
  parameters:
  - name: livy_info
    isRequired: true
    types:
    - <xref:azure.synapse.spark.models.SparkSessionState>
  - name: name
    isRequired: true
    types:
    - <xref:str>
  - name: workspace_name
    isRequired: true
    types:
    - <xref:str>
  - name: spark_pool_name
    isRequired: true
    types:
    - <xref:str>
  - name: submitter_name
    isRequired: true
    types:
    - <xref:str>
  - name: submitter_id
    isRequired: true
    types:
    - <xref:str>
  - name: artifact_id
    isRequired: true
    types:
    - <xref:str>
  - name: job_type
    description: 'The job type. Possible values include: "SparkBatch", "SparkSession".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.synapse.spark.models.SparkJobType>
  - name: result
    description: 'Possible values include: "Uncertain", "Succeeded", "Failed", "Cancelled".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.synapse.spark.models.SparkSessionResultType>
  - name: scheduler
    isRequired: true
    types:
    - <xref:azure.synapse.spark.models.SparkScheduler>
  - name: plugin
    isRequired: true
    types:
    - <xref:azure.synapse.spark.models.SparkServicePlugin>
  - name: errors
    isRequired: true
    types:
    - <xref:list>[<xref:azure.synapse.spark.models.SparkServiceError>]
  - name: tags
    description: A set of tags. Dictionary of `<string>`.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: id
    description: Required.
    isRequired: true
    types:
    - <xref:int>
  - name: app_id
    isRequired: true
    types:
    - <xref:str>
  - name: app_info
    description: Dictionary of `<string>`.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: state
    description: 'The session state. Possible values include: "not_started", "starting",
      "idle",

      "busy", "shutting_down", "error", "dead", "killed", "success", "running", "recovering".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.synapse.spark.models.LivyStates>
  - name: log_lines
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
  keywordOnlyParameters:
  - name: id
    isRequired: true
  - name: livy_info
    isRequired: true
  - name: name
    isRequired: true
  - name: workspace_name
    isRequired: true
  - name: spark_pool_name
    isRequired: true
  - name: submitter_name
    isRequired: true
  - name: submitter_id
    isRequired: true
  - name: artifact_id
    isRequired: true
  - name: job_type
    isRequired: true
  - name: result
    isRequired: true
  - name: scheduler
    isRequired: true
  - name: plugin
    isRequired: true
  - name: errors
    isRequired: true
  - name: tags
    isRequired: true
  - name: app_id
    isRequired: true
  - name: app_info
    isRequired: true
  - name: state
    isRequired: true
  - name: log_lines
    isRequired: true
methods:
- uid: azure.synapse.spark.models.SparkSession.as_dict
  name: as_dict
  summary: "Return a dict that can be JSONify using json.dump.\n\nAdvanced usage might\
    \ optionally use a callback as parameter:\n\nKey is the attribute name used in\
    \ Python. Attr_desc\nis a dict of metadata. Currently contains 'type' with the\n\
    msrest type and 'key' with the RestAPI encoded key.\nValue is the current value\
    \ in this object.\n\nThe string returned will be used to serialize the key.\n\
    If the return type is a list, this is considered hierarchical\nresult dict.\n\n\
    See the three examples in this file:\n\n* attribute_transformer \n\n* full_restapi_key_transformer\
    \ \n\n* last_restapi_key_transformer \n\nIf you want XML serialization, you can\
    \ pass the kwargs is_xml=True."
  signature: as_dict(keep_readonly=True, key_transformer=<function attribute_transformer>,
    **kwargs)
  parameters:
  - name: key_transformer
    description: A key transformer function.
    types:
    - <xref:function>
  - name: keep_readonly
    defaultValue: 'True'
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
- uid: azure.synapse.spark.models.SparkSession.deserialize
  name: deserialize
  summary: Parse a str using the RestAPI syntax and return a model.
  signature: deserialize(data, content_type=None)
  parameters:
  - name: data
    description: A str using RestAPI structure. JSON by default.
    isRequired: true
    types:
    - <xref:str>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  return:
    description: An instance of this model
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.synapse.spark.models.SparkSession.enable_additional_properties_sending
  name: enable_additional_properties_sending
  signature: enable_additional_properties_sending()
- uid: azure.synapse.spark.models.SparkSession.from_dict
  name: from_dict
  summary: 'Parse a dict using given key extractor return a model.


    By default consider key

    extractors (rest_key_case_insensitive_extractor, attribute_key_case_insensitive_extractor

    and last_rest_key_case_insensitive_extractor)'
  signature: from_dict(data, key_extractors=None, content_type=None)
  parameters:
  - name: data
    description: A dict using RestAPI structure
    isRequired: true
    types:
    - <xref:dict>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  - name: key_extractors
    defaultValue: None
  return:
    description: An instance of this model
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.synapse.spark.models.SparkSession.is_xml_model
  name: is_xml_model
  signature: is_xml_model()
- uid: azure.synapse.spark.models.SparkSession.serialize
  name: serialize
  summary: 'Return the JSON that would be sent to azure from this model.


    This is an alias to *as_dict(full_restapi_key_transformer, keep_readonly=False)*.


    If you want XML serialization, you can pass the kwargs is_xml=True.'
  signature: serialize(keep_readonly=False, **kwargs)
  parameters:
  - name: keep_readonly
    description: If you want to serialize the readonly attributes
    defaultValue: 'False'
    types:
    - <xref:bool>
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
- uid: azure.synapse.spark.models.SparkSession.validate
  name: validate
  summary: Validate this model recursively and return a list of ValidationError.
  signature: validate()
  return:
    description: A list of validation error
    types:
    - <xref:list>
