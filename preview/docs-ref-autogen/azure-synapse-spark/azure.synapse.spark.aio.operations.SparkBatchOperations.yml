### YamlMime:PythonClass
uid: azure.synapse.spark.aio.operations.SparkBatchOperations
name: SparkBatchOperations
fullName: azure.synapse.spark.aio.operations.SparkBatchOperations
module: azure.synapse.spark.aio.operations
inheritances:
- builtins.object
summary: 'SparkBatchOperations async operations.


  You should not instantiate this class directly. Instead, you should create a Client
  instance that

  instantiates it for you and attaches it as an attribute.'
constructor:
  syntax: SparkBatchOperations(client, config, serializer, deserializer)
  parameters:
  - name: client
    description: Client for service requests.
    isRequired: true
  - name: config
    description: Configuration of service client.
    isRequired: true
  - name: serializer
    description: An object model serializer.
    isRequired: true
  - name: deserializer
    description: An object model deserializer.
    isRequired: true
variables:
- description: Alias to model classes used in this operation group.
  name: models
methods:
- uid: azure.synapse.spark.aio.operations.SparkBatchOperations.cancel_spark_batch_job
  name: cancel_spark_batch_job
  summary: Cancels a running spark batch job.
  signature: 'cancel_spark_batch_job(batch_id: int, **kwargs: Any) -> None'
  parameters:
  - name: batch_id
    description: Identifier for the batch job.
    isRequired: true
    types:
    - <xref:int>
  - name: cls
    description: A custom type or function that will be passed the direct response
    types:
    - <xref:callable>
  return:
    description: None, or the result of cls(response)
    types:
    - <xref:None>
  exceptions:
  - type: ~azure.core.exceptions.HttpResponseError
- uid: azure.synapse.spark.aio.operations.SparkBatchOperations.create_spark_batch_job
  name: create_spark_batch_job
  summary: Create new spark batch job.
  signature: 'create_spark_batch_job(spark_batch_job_options: azure.synapse.spark.models._models_py3.SparkBatchJobOptions,
    detailed: Optional[bool] = None, **kwargs: Any) -> azure.synapse.spark.models._models_py3.SparkBatchJob'
  parameters:
  - name: spark_batch_job_options
    description: Livy compatible batch job request payload.
    isRequired: true
    types:
    - <xref:azure.synapse.spark.models.SparkBatchJobOptions>
  - name: detailed
    description: 'Optional query param specifying whether detailed response is returned
      beyond

      plain livy.'
    defaultValue: None
    types:
    - <xref:bool>
  - name: cls
    description: A custom type or function that will be passed the direct response
    types:
    - <xref:callable>
  return:
    description: SparkBatchJob, or the result of cls(response)
    types:
    - <xref:azure.synapse.spark.models.SparkBatchJob>
  exceptions:
  - type: ~azure.core.exceptions.HttpResponseError
- uid: azure.synapse.spark.aio.operations.SparkBatchOperations.get_spark_batch_job
  name: get_spark_batch_job
  summary: Gets a single spark batch job.
  signature: 'get_spark_batch_job(batch_id: int, detailed: Optional[bool] = None,
    **kwargs: Any) -> azure.synapse.spark.models._models_py3.SparkBatchJob'
  parameters:
  - name: batch_id
    description: Identifier for the batch job.
    isRequired: true
    types:
    - <xref:int>
  - name: detailed
    description: 'Optional query param specifying whether detailed response is returned
      beyond

      plain livy.'
    defaultValue: None
    types:
    - <xref:bool>
  - name: cls
    description: A custom type or function that will be passed the direct response
    types:
    - <xref:callable>
  return:
    description: SparkBatchJob, or the result of cls(response)
    types:
    - <xref:azure.synapse.spark.models.SparkBatchJob>
  exceptions:
  - type: ~azure.core.exceptions.HttpResponseError
- uid: azure.synapse.spark.aio.operations.SparkBatchOperations.get_spark_batch_jobs
  name: get_spark_batch_jobs
  summary: List all spark batch jobs which are running under a particular spark pool.
  signature: 'get_spark_batch_jobs(from_parameter: Optional[int] = None, size: Optional[int]
    = None, detailed: Optional[bool] = None, **kwargs: Any) -> azure.synapse.spark.models._models_py3.SparkBatchJobCollection'
  parameters:
  - name: from_parameter
    description: Optional param specifying which index the list should begin from.
    defaultValue: None
    types:
    - <xref:int>
  - name: size
    description: 'Optional param specifying the size of the returned list.

      By default it is 20 and that is the maximum.'
    defaultValue: None
    types:
    - <xref:int>
  - name: detailed
    description: 'Optional query param specifying whether detailed response is returned
      beyond

      plain livy.'
    defaultValue: None
    types:
    - <xref:bool>
  - name: cls
    description: A custom type or function that will be passed the direct response
    types:
    - <xref:callable>
  return:
    description: SparkBatchJobCollection, or the result of cls(response)
    types:
    - <xref:azure.synapse.spark.models.SparkBatchJobCollection>
  exceptions:
  - type: ~azure.core.exceptions.HttpResponseError
attributes:
- uid: azure.synapse.spark.aio.operations.SparkBatchOperations.models
  name: models
  signature: 'models = <module ''azure.synapse.spark.models'' from ''C:\\hostedtoolcache\\windows\\Python\\3.6.8\\x64\\lib\\site-packages\\azure\\synapse\\spark\\models\\__init__.py''>'
