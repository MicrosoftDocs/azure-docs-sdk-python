### YamlMime:PythonPackage
uid: azure.ai.ml.parallel
name: parallel
fullName: azure.ai.ml.parallel
type: import
functions:
- uid: azure.ai.ml.parallel.parallel_run_function
  name: parallel_run_function
  summary: "Create a Parallel object which can be used inside dsl.pipeline as a\n\
    function and can also be created as a standalone parallel job.\n\nFor an example\
    \ of using ParallelRunStep, see the notebook\n[https://aka.ms/parallel-example-notebook](https://aka.ms/parallel-example-notebook)\n\
    \n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
    : [], \"backrefs\": [], \"force\": false, \"highlight_args\": {}, \"xml:space\"\
    : \"preserve\", \"language\": \"python\", \"linenos\": false} -->\n\n````python\n\
    \n   from azure.ai.ml import Input, Output, parallel\n\n   parallel_run = parallel_run_function(\n\
    \       name = 'batch_score_with_tabular_input',\n       display_name = 'Batch\
    \ Score with Tabular Dataset',\n       description = 'parallel component for batch\
    \ score',\n       inputs = dict(\n       job_data_path=Input(type=AssetTypes.MLTABLE,\
    \ description='The data to be split and scored in parallel'),\n       score_model=Input(type=AssetTypes.URI_FOLDER,\
    \ description='The model for batch score.')\n       ),\n       outputs = dict(job_output_path=Output(type=AssetTypes.MLTABLE)),\n\
    \       input_data = '${{inputs.job_data_path}}',\n       max_concurrency_per_instance\
    \ = 2,  # Optional, default is 1\n       mini_batch_size = '100',      # optional\n\
    \       mini_batch_error_threshold = 5,  # Optional, allowed failed count on mini\
    \ batch items, default is -1\n       logging_level = 'DEBUG',     # Optional,\
    \ default is INFO\n       error_threshold = 5,       # Optional, allowed failed\
    \ count totally, default is -1\n       retry_settings = dict(max_retries=2, timeout=60),\
    \  # Optional\n       task = RunFunction(\n           code = './src',\n      \
    \     entry_script = 'tabular_batch_inference.py',\n           environment = Environment(\n\
    \               image= 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04',\n\
    \               conda_file='./src/environment_parallel.yml'\n           ),\n \
    \          program_arguments = '--model ${{inputs.score_model}}',\n          \
    \ append_row_to = '${{outputs.job_output_path}}',   # Optional, if not set, summary_only\n\
    \       ),\n   )\n   ````"
  signature: 'parallel_run_function(*, name: Optional[str] = None, description: Optional[str]
    = None, tags: Optional[Dict] = None, properties: Optional[Dict] = None, display_name:
    Optional[str] = None, experiment_name: Optional[str] = None, compute: Optional[str]
    = None, retry_settings: Optional[azure.ai.ml.entities._deployment.deployment_settings.BatchRetrySettings]
    = None, environment_variables: Optional[Dict] = None, logging_level: Optional[str]
    = None, max_concurrency_per_instance: Optional[int] = None, error_threshold: Optional[int]
    = None, mini_batch_error_threshold: Optional[int] = None, task: Optional[azure.ai.ml.entities._job.parallel.run_function.RunFunction]
    = None, mini_batch_size: Optional[str] = None, input_data: Optional[str] = None,
    inputs: Optional[Dict] = None, outputs: Optional[Dict] = None, instance_count:
    Optional[int] = None, instance_type: Optional[str] = None, identity: Optional[Union[azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedIdentity,
    azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.AmlToken]] = None,
    is_deterministic: bool = True, **kwargs) -> azure.ai.ml.entities._builders.parallel.Parallel'
  parameters:
  - name: name
    description: Name of the parallel job or component created
    isRequired: true
    types:
    - <xref:str>
  - name: description
    description: a friendly description of the parallel
    isRequired: true
    types:
    - <xref:str>
  - name: tags
    description: Tags to be attached to this parallel
    isRequired: true
    types:
    - <xref:Dict>
  - name: properties
    description: The asset property dictionary.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: display_name
    description: a friendly name
    isRequired: true
    types:
    - <xref:str>
  - name: experiment_name
    description: Name of the experiment the job will be created under, if None is
      provided, default will be set to current directory name. Will be ignored as
      a pipeline step.
    isRequired: true
    types:
    - <xref:str>
  - name: compute
    description: 'the name of the compute where the parallel job is executed(

      will not be used if the parallel is used as a component/function)'
    isRequired: true
    types:
    - <xref:str>
  - name: retry_settings
    description: parallel component run failed retry
    isRequired: true
    types:
    - <xref:azure.ai.ml.entities.BatchRetrySettings>
  - name: environment_variables
    description: 'A dictionary of environment variables names and values.

      These environment variables are set on the process where user script is being
      executed.'
    isRequired: true
    types:
    - <xref:Dict>[<xref:str>, <xref:str>]
  - name: logging_level
    description: 'A string of the logging level name, which is defined in ''logging''.

      Possible values are ''WARNING'', ''INFO'', and ''DEBUG''. (optional, default
      value is ''INFO''.)

      This value could be set through PipelineParameter.'
    isRequired: true
    types:
    - <xref:str>
  - name: max_concurrency_per_instance
    description: The max parallellism that each compute instance has.
    isRequired: true
    types:
    - <xref:int>
  - name: error_threshold
    description: 'The number of record failures for Tabular Dataset

      and file failures for File Dataset that should be ignored during

      processing. If the error count goes above this value, then the job will be aborted.
      Error

      threshold is for the entire input rather than the individual mini-batch sent
      to run() method.

      The range is [-1, int.max]. -1 indicates ignore all failures during processing.'
    isRequired: true
    types:
    - <xref:int>
  - name: mini_batch_error_threshold
    description: The number of mini batch processing failures should be ignored.
    isRequired: true
    types:
    - <xref:int>
  - name: task
    description: The parallel task.
    isRequired: true
    types:
    - <xref:azure.ai.ml.parallel.RunFunction>
  - name: mini_batch_size
    description: 'For FileDataset input, this field is the number of files a user
      script can process

      in one run() call. For TabularDataset input, this field is the approximate size
      of data the user script

      can process in one run() call. Example values are 1024, 1024KB, 10MB, and 1GB.

      (optional, default value is 10 files for FileDataset and 1MB for TabularDataset.)
      This value could be set

      through PipelineParameter.'
    isRequired: true
    types:
    - <xref:str>
  - name: input_data
    description: The input data.
    isRequired: true
    types:
    - <xref:str>
  - name: inputs
    description: a dict of inputs used by this parallel.
    isRequired: true
    types:
    - <xref:Dict>
  - name: outputs
    description: the outputs of this parallel
    isRequired: true
    types:
    - <xref:Dict>
  - name: instance_count
    description: Optional number of instances or nodes used by the compute target.
      Defaults to 1.
    isRequired: true
    types:
    - <xref:int>
  - name: instance_type
    description: Optional type of VM used as supported by the compute target.
    isRequired: true
    types:
    - <xref:str>
  - name: identity
    description: Identity that training job will use while running on compute.
    isRequired: true
    types:
    - <xref:Union>[<xref:azure.ai.ml.ManagedIdentity>, <xref:azure.ai.ml.AmlToken>]
  - name: is_deterministic
    description: Specify whether the parallel will return same output given same input.
      If a parallel (component) is deterministic, when use it as a node/step in a
      pipeline, it will reuse results from a previous submitted job in current workspace
      which has same inputs and settings. In this case, this step will not use any
      compute resource. Default to be True, specify is_deterministic=False if you
      would like to avoid such reuse behavior.
    isRequired: true
    types:
    - <xref:bool>
  remarks: "To use parallel_run_function:\n\n* Create a <xref:azure.ai.ml.entities._builders.Parallel>\
    \ object to specify how parallel run is performed, with parameters to control\
    \ batch size,number of nodes per compute target, and a reference to your custom\
    \ Python script. \n\n* Build pipeline with the parallel object as a function.\
    \ defines inputs and outputs for the step. \n\n* Sumbit the pipeline to run. \n"
classes:
- azure.ai.ml.parallel.ParallelJob
- azure.ai.ml.parallel.RunFunction
