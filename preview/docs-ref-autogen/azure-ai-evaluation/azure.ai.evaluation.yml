### YamlMime:PythonPackage
uid: azure.ai.evaluation
name: evaluation
fullName: azure.ai.evaluation
type: rootImport
functions:
- uid: azure.ai.evaluation.evaluate
  name: evaluate
  summary: "Evaluates target or data with built-in or custom evaluators. If both target\
    \ and data are provided,\n   data will be run through target function and then\
    \ results will be evaluated.\n\nEvaluate API can be used as follows:\n\n<!-- literal_block\
    \ {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\"\
    : [], \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\"\
    , \"highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation\
    \ import evaluate, RelevanceEvaluator, CoherenceEvaluator\n\n\n   model_config\
    \ = {\n       \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n\
    \       \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n       \"azure_deployment\"\
    : os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n   }\n\n   coherence_eval = CoherenceEvaluator(model_config=model_config)\n\
    \   relevance_eval = RelevanceEvaluator(model_config=model_config)\n\n   path\
    \ = \"evaluate_test_data.jsonl\"\n   result = evaluate(\n       data=path,\n \
    \      evaluators={\n           \"coherence\": coherence_eval,\n           \"\
    relevance\": relevance_eval,\n       },\n       evaluator_config={\n         \
    \  \"coherence\": {\n               \"column_mapping\": {\n                  \
    \ \"response\": \"${data.response}\",\n                   \"query\": \"${data.query}\"\
    ,\n               },\n           },\n           \"relevance\": {\n           \
    \    \"column_mapping\": {\n                   \"response\": \"${data.response}\"\
    ,\n                   \"context\": \"${data.context}\",\n                   \"\
    query\": \"${data.query}\",\n               },\n           },\n       },\n   )\n\
    \   ````"
  signature: 'evaluate(*, data: str | PathLike, evaluators: Dict[str, Callable], evaluation_name:
    str | None = None, target: Callable | None = None, evaluator_config: Dict[str,
    EvaluatorConfig] | None = None, azure_ai_project: AzureAIProject | None = None,
    output_path: str | PathLike | None = None, **kwargs) -> EvaluationResult'
  keywordOnlyParameters:
  - name: data
    description: 'Path to the data to be evaluated or passed to target if target is
      set.

      Only .jsonl format files are supported.  *target* and *data* both cannot be
      None. Required.'
    types:
    - <xref:str>
  - name: evaluators
    description: 'Evaluators to be used for evaluation. It should be a dictionary
      with key as alias for evaluator

      and value as the evaluator function. Required.'
    types:
    - <xref:typing.Dict>[<xref:str>, <xref:typing.Callable>]
  - name: evaluation_name
    description: Display name of the evaluation.
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: target
    description: Target to be evaluated. *target* and *data* both cannot be None
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:typing.Callable>]
  - name: evaluator_config
    description: 'Configuration for evaluators. The configuration should be a dictionary
      with evaluator

      names as keys and a values that are dictionaries containing the column mappings.
      The column mappings should

      be a dictionary with keys as the column names in the evaluator input and values
      as the column names in the

      input data or data generated by target.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>[<xref:str>, <xref:azure.ai.evaluation.EvaluatorConfig>]]
  - name: output_path
    description: 'The local folder or file path to save evaluation results to if set.
      If folder path is provided

      the results will be saved to a file named *evaluation_results.json* in the folder.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: azure_ai_project
    description: Logs evaluation results to AI Studio if set.
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:azure.ai.evaluation.AzureAIProject>]
  return:
    description: Evaluation results.
    types:
    - <xref:azure.ai.evaluation.EvaluationResult>
classes:
- azure.ai.evaluation.AzureAIProject
- azure.ai.evaluation.AzureOpenAIModelConfiguration
- azure.ai.evaluation.BleuScoreEvaluator
- azure.ai.evaluation.CoherenceEvaluator
- azure.ai.evaluation.ContentSafetyEvaluator
- azure.ai.evaluation.ContentSafetyMultimodalEvaluator
- azure.ai.evaluation.Conversation
- azure.ai.evaluation.EvaluationResult
- azure.ai.evaluation.EvaluatorConfig
- azure.ai.evaluation.F1ScoreEvaluator
- azure.ai.evaluation.FluencyEvaluator
- azure.ai.evaluation.GleuScoreEvaluator
- azure.ai.evaluation.GroundednessEvaluator
- azure.ai.evaluation.GroundednessProEvaluator
- azure.ai.evaluation.HateUnfairnessEvaluator
- azure.ai.evaluation.HateUnfairnessMultimodalEvaluator
- azure.ai.evaluation.IndirectAttackEvaluator
- azure.ai.evaluation.Message
- azure.ai.evaluation.MeteorScoreEvaluator
- azure.ai.evaluation.OpenAIModelConfiguration
- azure.ai.evaluation.ProtectedMaterialEvaluator
- azure.ai.evaluation.ProtectedMaterialMultimodalEvaluator
- azure.ai.evaluation.QAEvaluator
- azure.ai.evaluation.RelevanceEvaluator
- azure.ai.evaluation.RetrievalEvaluator
- azure.ai.evaluation.RougeScoreEvaluator
- azure.ai.evaluation.SelfHarmEvaluator
- azure.ai.evaluation.SelfHarmMultimodalEvaluator
- azure.ai.evaluation.SexualEvaluator
- azure.ai.evaluation.SexualMultimodalEvaluator
- azure.ai.evaluation.SimilarityEvaluator
- azure.ai.evaluation.ViolenceEvaluator
- azure.ai.evaluation.ViolenceMultimodalEvaluator
packages:
- azure.ai.evaluation.simulator
enums:
- azure.ai.evaluation.RougeType
