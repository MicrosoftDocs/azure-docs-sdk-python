### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.search.documents.indexes.models.AnalyzeTextOptions.to_analyze_request
  class: azure.search.documents.indexes.models.AnalyzeTextOptions
  fullName: azure.search.documents.indexes.models.AnalyzeTextOptions
  inheritance:
  - inheritance:
    - type: builtins.object
    type: msrest.serialization.Model
  langs:
  - python
  module: azure.search.documents.indexes.models
  name: AnalyzeTextOptions
  summary: 'Specifies some text and analysis components used to break that text into
    tokens.


    All required parameters must be populated in order to send to Azure.'
  syntax:
    content: AnalyzeTextOptions(**kwargs)
    parameters:
    - description: Required. The text to break into tokens.
      id: text
      type:
      - str
    - description: 'The name of the analyzer to use to break the given text. If this
        parameter is

        not specified, you must specify a tokenizer instead. The tokenizer and analyzer
        parameters are

        mutually exclusive. Possible values include: "ar.microsoft", "ar.lucene",
        "hy.lucene",

        "bn.microsoft", "eu.lucene", "bg.microsoft", "bg.lucene", "ca.microsoft",
        "ca.lucene", "zh-

        Hans.microsoft", "zh-Hans.lucene", "zh-Hant.microsoft", "zh-Hant.lucene",
        "hr.microsoft",

        "cs.microsoft", "cs.lucene", "da.microsoft", "da.lucene", "nl.microsoft",
        "nl.lucene",

        "en.microsoft", "en.lucene", "et.microsoft", "fi.microsoft", "fi.lucene",
        "fr.microsoft",

        "fr.lucene", "gl.lucene", "de.microsoft", "de.lucene", "el.microsoft", "el.lucene",

        "gu.microsoft", "he.microsoft", "hi.microsoft", "hi.lucene", "hu.microsoft",
        "hu.lucene",

        "is.microsoft", "id.microsoft", "id.lucene", "ga.lucene", "it.microsoft",
        "it.lucene",

        "ja.microsoft", "ja.lucene", "kn.microsoft", "ko.microsoft", "ko.lucene",
        "lv.microsoft",

        "lv.lucene", "lt.microsoft", "ml.microsoft", "ms.microsoft", "mr.microsoft",
        "nb.microsoft",

        "no.lucene", "fa.lucene", "pl.microsoft", "pl.lucene", "pt-BR.microsoft",
        "pt-BR.lucene", "pt-

        PT.microsoft", "pt-PT.lucene", "pa.microsoft", "ro.microsoft", "ro.lucene",
        "ru.microsoft",

        "ru.lucene", "sr-cyrillic.microsoft", "sr-latin.microsoft", "sk.microsoft",
        "sl.microsoft",

        "es.microsoft", "es.lucene", "sv.microsoft", "sv.lucene", "ta.microsoft",
        "te.microsoft",

        "th.microsoft", "th.lucene", "tr.microsoft", "tr.lucene", "uk.microsoft",
        "ur.microsoft",

        "vi.microsoft", "standard.lucene", "standardasciifolding.lucene", "keyword",
        "pattern",

        "simple", "stop", "whitespace".'
      id: analyzer_name
      type:
      - str
      - azure.search.documents.indexes.models.LexicalAnalyzerName
    - description: 'The name of the tokenizer to use to break the given text. If this
        parameter

        is not specified, you must specify an analyzer instead. The tokenizer and
        analyzer parameters

        are mutually exclusive. Possible values include: "classic", "edgeNGram", "keyword_v2",

        "letter", "lowercase", "microsoft_language_tokenizer", "microsoft_language_stemming_tokenizer",

        "nGram", "path_hierarchy_v2", "pattern", "standard_v2", "uax_url_email", "whitespace".'
      id: tokenizer_name
      type:
      - str
      - azure.search.documents.indexes.models.LexicalTokenizerName
    - description: 'An optional list of token filters to use when breaking the given
        text.

        This parameter can only be set when using the tokenizer parameter.'
      id: token_filters
      type:
      - list[str
      - azure.search.documents.indexes.models.TokenFilterName]
    - description: 'An optional list of character filters to use when breaking the
        given text.

        This parameter can only be set when using the tokenizer parameter.'
      id: char_filters
      type:
      - list[str]
  type: class
  uid: azure.search.documents.indexes.models.AnalyzeTextOptions
- class: azure.search.documents.indexes.models.AnalyzeTextOptions
  fullName: azure.search.documents.indexes.models.AnalyzeTextOptions.to_analyze_request
  langs:
  - python
  module: azure.search.documents.indexes.models
  name: to_analyze_request()
  namewithoutparameters: to_analyze_request
  syntax:
    content: to_analyze_request()
    parameters: []
  type: method
  uid: azure.search.documents.indexes.models.AnalyzeTextOptions.to_analyze_request
references:
- fullName: azure.search.documents.indexes.models.AnalyzeTextOptions.to_analyze_request
  isExternal: false
  name: to_analyze_request()
  parent: azure.search.documents.indexes.models.AnalyzeTextOptions
  uid: azure.search.documents.indexes.models.AnalyzeTextOptions.to_analyze_request
- fullName: list[str
  name: list[str
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  uid: list[str
- fullName: azure.search.documents.indexes.models.TokenFilterName]
  name: TokenFilterName]
  spec.python:
  - fullName: azure.search.documents.indexes.models.TokenFilterName
    name: TokenFilterName
    uid: azure.search.documents.indexes.models.TokenFilterName
  - fullName: ']'
    name: ']'
  uid: azure.search.documents.indexes.models.TokenFilterName]
- fullName: list[str]
  name: list[str]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  - fullName: ']'
    name: ']'
  uid: list[str]
