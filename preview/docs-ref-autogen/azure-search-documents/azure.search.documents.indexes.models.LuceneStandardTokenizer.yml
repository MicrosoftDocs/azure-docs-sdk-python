### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.LuceneStandardTokenizer
name: LuceneStandardTokenizer
fullName: azure.search.documents.indexes.models.LuceneStandardTokenizer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalTokenizer
summary: 'Breaks text following the Unicode Text Segmentation rules. This tokenizer
  is implemented using

  Apache Lucene.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'LuceneStandardTokenizer(*, name: str, max_token_length: int = 255, **kwargs:
    Any)'
  parameters:
  - name: name
    description: 'The name of the tokenizer. It must only contain letters, digits,
      spaces, dashes

      or underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters. Required.'
    types:
    - <xref:str>
  - name: max_token_length
    description: 'The maximum token length. Default is 255. Tokens longer than the

      maximum length are split.'
    types:
    - <xref:int>
variables:
- description: Identifies the concrete type of the tokenizer. Required.
  name: odata_type
  types:
  - <xref:str>
- description: 'The name of the tokenizer. It must only contain letters, digits, spaces,
    dashes or

    underscores, can only start and end with alphanumeric characters, and is limited
    to 128

    characters. Required.'
  name: name
  types:
  - <xref:str>
- description: 'The maximum token length. Default is 255. Tokens longer than the

    maximum length are split.'
  name: max_token_length
  types:
  - <xref:int>
