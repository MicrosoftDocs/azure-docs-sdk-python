### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.LuceneStandardTokenizer
name: LuceneStandardTokenizer
fullName: azure.search.documents.indexes.models.LuceneStandardTokenizer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalTokenizer
summary: 'Breaks text following the Unicode Text Segmentation rules. This tokenizer
  is implemented using Apache Lucene.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'LuceneStandardTokenizer(*, name: str, max_token_length: Optional[int] =
    255, **kwargs)'
  parameters:
  - name: name
    description: 'Required. The name of the tokenizer. It must only contain letters,
      digits,

      spaces, dashes or underscores, can only start and end with alphanumeric characters,
      and is

      limited to 128 characters.'
    types:
    - <xref:str>
  - name: max_token_length
    description: 'The maximum token length. Default is 255. Tokens longer than the

      maximum length are split.'
    types:
    - <xref:int>
variables:
- description: 'Required. Identifies the concrete type of the tokenizer.Constant filled
    by

    server.'
  name: odata_type
  types:
  - <xref:str>
- description: 'Required. The name of the tokenizer. It must only contain letters,
    digits, spaces,

    dashes or underscores, can only start and end with alphanumeric characters, and
    is limited to

    128 characters.'
  name: name
  types:
  - <xref:str>
- description: 'The maximum token length. Default is 255. Tokens longer than the

    maximum length are split.'
  name: max_token_length
  types:
  - <xref:int>
