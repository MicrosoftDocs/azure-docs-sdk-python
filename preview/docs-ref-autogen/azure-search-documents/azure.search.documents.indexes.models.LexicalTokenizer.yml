### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.LexicalTokenizer
name: LexicalTokenizer
fullName: azure.search.documents.indexes.models.LexicalTokenizer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated._serialization.Model
summary: 'Base type for tokenizers.


  You probably want to use the sub-classes and not this class directly. Known sub-classes
  are:

  ClassicTokenizer, EdgeNGramTokenizer, KeywordTokenizer, KeywordTokenizerV2,

  MicrosoftLanguageStemmingTokenizer, MicrosoftLanguageTokenizer, NGramTokenizer,

  PathHierarchyTokenizerV2, PatternTokenizer, LuceneStandardTokenizer, LuceneStandardTokenizerV2,

  UaxUrlEmailTokenizer


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'LexicalTokenizer(*, name: str, **kwargs: Any)'
  parameters:
  - name: name
    description: 'The name of the tokenizer. It must only contain letters, digits,
      spaces, dashes

      or underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters. Required.'
    types:
    - <xref:str>
variables:
- description: Identifies the concrete type of the tokenizer. Required.
  name: odata_type
  types:
  - <xref:str>
- description: 'The name of the tokenizer. It must only contain letters, digits, spaces,
    dashes or

    underscores, can only start and end with alphanumeric characters, and is limited
    to 128

    characters. Required.'
  name: name
  types:
  - <xref:str>
