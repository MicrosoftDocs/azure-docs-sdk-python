### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.LuceneStandardAnalyzer
name: LuceneStandardAnalyzer
fullName: azure.search.documents.indexes.models.LuceneStandardAnalyzer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalAnalyzer
summary: 'Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase
  filter and stop filter.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'LuceneStandardAnalyzer(*, name: str, max_token_length: int | None = 255,
    stopwords: List[str] | None = None, **kwargs)'
variables:
- description: 'Required. Identifies the concrete type of the analyzer.Constant filled
    by

    server.'
  name: odata_type
  types:
  - <xref:azure.search.documents.indexes.models.str>
- description: 'Required. The name of the analyzer. It must only contain letters,
    digits, spaces,

    dashes or underscores, can only start and end with alphanumeric characters, and
    is limited to

    128 characters.'
  name: name
  types:
  - <xref:azure.search.documents.indexes.models.str>
- description: 'The maximum token length. Default is 255. Tokens longer than the

    maximum length are split. The maximum token length that can be used is 300 characters.'
  name: max_token_length
  types:
  - <xref:azure.search.documents.indexes.models.int>
- description: A list of stopwords.
  name: stopwords
  types:
  - <xref:azure.search.documents.indexes.models.list>[<xref:azure.search.documents.indexes.models.str>]
