### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.LuceneStandardAnalyzer
name: LuceneStandardAnalyzer
fullName: azure.search.documents.indexes.models.LuceneStandardAnalyzer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalAnalyzer
summary: 'Standard Apache Lucene analyzer; Composed of the standard tokenizer, lowercase
  filter and stop

  filter.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'LuceneStandardAnalyzer(*, name: str, max_token_length: int = 255, stopwords:
    List[str] | None = None, **kwargs: Any)'
  parameters:
  - name: name
    description: 'The name of the analyzer. It must only contain letters, digits,
      spaces, dashes

      or underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters. Required.'
    types:
    - <xref:str>
  - name: max_token_length
    description: 'The maximum token length. Default is 255. Tokens longer than the

      maximum length are split. The maximum token length that can be used is 300 characters.'
    types:
    - <xref:int>
  - name: stopwords
    description: A list of stopwords.
    types:
    - <xref:list>[<xref:str>]
variables:
- description: Identifies the concrete type of the analyzer. Required.
  name: odata_type
  types:
  - <xref:str>
- description: 'The name of the analyzer. It must only contain letters, digits, spaces,
    dashes or

    underscores, can only start and end with alphanumeric characters, and is limited
    to 128

    characters. Required.'
  name: name
  types:
  - <xref:str>
- description: 'The maximum token length. Default is 255. Tokens longer than the

    maximum length are split. The maximum token length that can be used is 300 characters.'
  name: max_token_length
  types:
  - <xref:int>
- description: A list of stopwords.
  name: stopwords
  types:
  - <xref:list>[<xref:str>]
