### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.CustomNormalizer
name: CustomNormalizer
fullName: azure.search.documents.indexes.models.CustomNormalizer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalNormalizer
summary: 'Allows you to configure normalization for filterable, sortable, and facetable
  fields, which by

  default operate with strict matching. This is a user-defined configuration consisting
  of at

  least one or more filters, which modify the token that is stored.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'CustomNormalizer(*, name: str, token_filters: List[str | _models.TokenFilterName]
    | None = None, char_filters: List[str | _models.CharFilterName] | None = None,
    **kwargs: Any)'
  parameters:
  - name: name
    description: 'The name of the normalizer. It must only contain letters, digits,
      spaces, dashes

      or underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters. It cannot end in ''.microsoft'' nor ''.lucene'', nor be named ''asciifolding'',

      ''standard'', ''lowercase'', ''uppercase'', or ''elision''. Required.'
    types:
    - <xref:str>
  - name: token_filters
    description: 'A list of token filters used to filter out or modify the input token.

      For example, you can specify a lowercase filter that converts all characters
      to lowercase. The

      filters are run in the order in which they are listed.'
    types:
    - <xref:list>[<xref:str>
    - <xref:search_service_client.models.TokenFilterName>]
  - name: char_filters
    description: 'A list of character filters used to prepare input text before it
      is

      processed. For instance, they can replace certain characters or symbols. The
      filters are run in

      the order in which they are listed.'
    types:
    - <xref:list>[<xref:str>
    - <xref:search_service_client.models.CharFilterName>]
variables:
- description: Identifies the concrete type of the normalizer. Required.
  name: odata_type
  types:
  - <xref:str>
- description: 'The name of the normalizer. It must only contain letters, digits,
    spaces, dashes or

    underscores, can only start and end with alphanumeric characters, and is limited
    to 128

    characters. It cannot end in ''.microsoft'' nor ''.lucene'', nor be named ''asciifolding'',

    ''standard'', ''lowercase'', ''uppercase'', or ''elision''. Required.'
  name: name
  types:
  - <xref:str>
- description: 'A list of token filters used to filter out or modify the input token.
    For

    example, you can specify a lowercase filter that converts all characters to lowercase.
    The

    filters are run in the order in which they are listed.'
  name: token_filters
  types:
  - <xref:list>[<xref:str>
  - <xref:search_service_client.models.TokenFilterName>]
- description: 'A list of character filters used to prepare input text before it is

    processed. For instance, they can replace certain characters or symbols. The filters
    are run in

    the order in which they are listed.'
  name: char_filters
  types:
  - <xref:list>[<xref:str>
  - <xref:search_service_client.models.CharFilterName>]
