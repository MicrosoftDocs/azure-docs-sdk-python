### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: azure.search.documents.indexes.models.CustomAnalyzer
  fullName: azure.search.documents.indexes.models.CustomAnalyzer
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: msrest.serialization.Model
    type: azure.search.documents.indexes._internal._generated.models._models_py3.LexicalAnalyzer
  langs:
  - python
  module: azure.search.documents.indexes.models
  name: CustomAnalyzer
  summary: 'Allows you to take control over the process of converting text into indexable/searchable
    tokens.

    It''s a user-defined configuration consisting of a single predefined tokenizer
    and one or more filters.

    The tokenizer is responsible for breaking text into tokens, and the filters for
    modifying tokens

    emitted by the tokenizer.


    All required parameters must be populated in order to send to Azure.'
  syntax:
    content: CustomAnalyzer(**kwargs)
    parameters:
    - description: 'Required. Identifies the concrete type of the analyzer.Constant
        filled by

        server.'
      id: odata_type
      type:
      - str
    - description: 'Required. The name of the analyzer. It must only contain letters,
        digits, spaces,

        dashes or underscores, can only start and end with alphanumeric characters,
        and is limited to

        128 characters.'
      id: name
      type:
      - str
    - description: 'Required. The name of the tokenizer to use to divide continuous
        text into a

        sequence of tokens, such as breaking a sentence into words. Possible values
        include: "classic",

        "edgeNGram", "keyword_v2", "letter", "lowercase", "microsoft_language_tokenizer",

        "microsoft_language_stemming_tokenizer", "nGram", "path_hierarchy_v2", "pattern",

        "standard_v2", "uax_url_email", "whitespace".'
      id: tokenizer_name
      type:
      - str
      - azure.search.documents.indexes.models.LexicalTokenizerName
    - description: 'A list of token filters used to filter out or modify the tokens
        generated

        by a tokenizer. For example, you can specify a lowercase filter that converts
        all characters to

        lowercase. The filters are run in the order in which they are listed.'
      id: token_filters
      type:
      - list[str
      - azure.search.documents.indexes.models.TokenFilterName]
    - description: 'A list of character filters used to prepare input text before
        it is

        processed by the tokenizer. For instance, they can replace certain characters
        or symbols. The

        filters are run in the order in which they are listed.'
      id: char_filters
      type:
      - list[str]
  type: class
  uid: azure.search.documents.indexes.models.CustomAnalyzer
references:
- fullName: list[str
  name: list[str
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  uid: list[str
- fullName: azure.search.documents.indexes.models.TokenFilterName]
  name: TokenFilterName]
  spec.python:
  - fullName: azure.search.documents.indexes.models.TokenFilterName
    name: TokenFilterName
    uid: azure.search.documents.indexes.models.TokenFilterName
  - fullName: ']'
    name: ']'
  uid: azure.search.documents.indexes.models.TokenFilterName]
- fullName: list[str]
  name: list[str]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  - fullName: ']'
    name: ']'
  uid: list[str]
