### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.MicrosoftLanguageTokenizer
name: MicrosoftLanguageTokenizer
fullName: azure.search.documents.indexes.models.MicrosoftLanguageTokenizer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalTokenizer
summary: 'Divides text using language-specific rules.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'MicrosoftLanguageTokenizer(*, name: str, max_token_length: int = 255, is_search_tokenizer:
    bool = False, language: str | _models.MicrosoftTokenizerLanguage | None = None,
    **kwargs: Any)'
  parameters:
  - name: name
    description: 'The name of the tokenizer. It must only contain letters, digits,
      spaces, dashes

      or underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters. Required.'
    types:
    - <xref:str>
  - name: max_token_length
    description: 'The maximum token length. Tokens longer than the maximum length
      are

      split. Maximum token length that can be used is 300 characters. Tokens longer
      than 300

      characters are first split into tokens of length 300 and then each of those
      tokens is split

      based on the max token length set. Default is 255.'
    types:
    - <xref:int>
  - name: is_search_tokenizer
    description: 'A value indicating how the tokenizer is used. Set to true if used

      as the search tokenizer, set to false if used as the indexing tokenizer. Default
      is false.'
    types:
    - <xref:bool>
  - name: language
    description: 'The language to use. The default is English. Known values are: "bangla",

      "bulgarian", "catalan", "chineseSimplified", "chineseTraditional", "croatian",
      "czech",

      "danish", "dutch", "english", "french", "german", "greek", "gujarati", "hindi",
      "icelandic",

      "indonesian", "italian", "japanese", "kannada", "korean", "malay", "malayalam",
      "marathi",

      "norwegianBokmaal", "polish", "portuguese", "portugueseBrazilian", "punjabi",
      "romanian",

      "russian", "serbianCyrillic", "serbianLatin", "slovenian", "spanish", "swedish",
      "tamil",

      "telugu", "thai", "ukrainian", "urdu", and "vietnamese".'
    types:
    - <xref:str>
    - <xref:search_service_client.models.MicrosoftTokenizerLanguage>
variables:
- description: Identifies the concrete type of the tokenizer. Required.
  name: odata_type
  types:
  - <xref:str>
- description: 'The name of the tokenizer. It must only contain letters, digits, spaces,
    dashes or

    underscores, can only start and end with alphanumeric characters, and is limited
    to 128

    characters. Required.'
  name: name
  types:
  - <xref:str>
- description: 'The maximum token length. Tokens longer than the maximum length are

    split. Maximum token length that can be used is 300 characters. Tokens longer
    than 300

    characters are first split into tokens of length 300 and then each of those tokens
    is split

    based on the max token length set. Default is 255.'
  name: max_token_length
  types:
  - <xref:int>
- description: 'A value indicating how the tokenizer is used. Set to true if used
    as

    the search tokenizer, set to false if used as the indexing tokenizer. Default
    is false.'
  name: is_search_tokenizer
  types:
  - <xref:bool>
- description: 'The language to use. The default is English. Known values are: "bangla",

    "bulgarian", "catalan", "chineseSimplified", "chineseTraditional", "croatian",
    "czech",

    "danish", "dutch", "english", "french", "german", "greek", "gujarati", "hindi",
    "icelandic",

    "indonesian", "italian", "japanese", "kannada", "korean", "malay", "malayalam",
    "marathi",

    "norwegianBokmaal", "polish", "portuguese", "portugueseBrazilian", "punjabi",
    "romanian",

    "russian", "serbianCyrillic", "serbianLatin", "slovenian", "spanish", "swedish",
    "tamil",

    "telugu", "thai", "ukrainian", "urdu", and "vietnamese".'
  name: language
  types:
  - <xref:str>
  - <xref:search_service_client.models.MicrosoftTokenizerLanguage>
