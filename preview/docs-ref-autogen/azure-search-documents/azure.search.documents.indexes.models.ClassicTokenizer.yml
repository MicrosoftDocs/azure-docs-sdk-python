### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.ClassicTokenizer
name: ClassicTokenizer
fullName: azure.search.documents.indexes.models.ClassicTokenizer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalTokenizer
summary: 'Grammar-based tokenizer that is suitable for processing most European-language
  documents. This

  tokenizer is implemented using Apache Lucene.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'ClassicTokenizer(*, name: str, max_token_length: int = 255, **kwargs: Any)'
  parameters:
  - name: name
    description: 'The name of the tokenizer. It must only contain letters, digits,
      spaces, dashes

      or underscores, can only start and end with alphanumeric characters, and is
      limited to 128

      characters. Required.'
    types:
    - <xref:str>
  - name: max_token_length
    description: 'The maximum token length. Default is 255. Tokens longer than the

      maximum length are split. The maximum token length that can be used is 300 characters.'
    types:
    - <xref:int>
variables:
- description: Identifies the concrete type of the tokenizer. Required.
  name: odata_type
  types:
  - <xref:str>
- description: 'The name of the tokenizer. It must only contain letters, digits, spaces,
    dashes or

    underscores, can only start and end with alphanumeric characters, and is limited
    to 128

    characters. Required.'
  name: name
  types:
  - <xref:str>
- description: 'The maximum token length. Default is 255. Tokens longer than the

    maximum length are split. The maximum token length that can be used is 300 characters.'
  name: max_token_length
  types:
  - <xref:int>
