### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.NGramTokenizer
name: NGramTokenizer
fullName: azure.search.documents.indexes.models.NGramTokenizer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalTokenizer
summary: 'Tokenizes the input into n-grams of the given size(s). This tokenizer is
  implemented using Apache Lucene.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'NGramTokenizer(*, name: str, min_gram: int | None = 1, max_gram: int |
    None = 2, token_chars: List[str | TokenCharacterKind] | None = None, **kwargs)'
variables:
- description: 'Required. Identifies the concrete type of the tokenizer.Constant filled
    by

    server.'
  name: odata_type
  types:
  - <xref:azure.search.documents.indexes.models.str>
- description: 'Required. The name of the tokenizer. It must only contain letters,
    digits, spaces,

    dashes or underscores, can only start and end with alphanumeric characters, and
    is limited to

    128 characters.'
  name: name
  types:
  - <xref:azure.search.documents.indexes.models.str>
- description: 'The minimum n-gram length. Default is 1. Maximum is 300. Must be less
    than the

    value of maxGram.'
  name: min_gram
  types:
  - <xref:azure.search.documents.indexes.models.int>
- description: The maximum n-gram length. Default is 2. Maximum is 300.
  name: max_gram
  types:
  - <xref:azure.search.documents.indexes.models.int>
- description: Character classes to keep in the tokens.
  name: token_chars
  types:
  - <xref:azure.search.documents.indexes.models.list>[<xref:azure.search.documents.indexes.models.str>
  - <xref:azure.search.documents.indexes.models.TokenCharacterKind>]
