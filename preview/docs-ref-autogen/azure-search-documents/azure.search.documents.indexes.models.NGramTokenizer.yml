### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.NGramTokenizer
name: NGramTokenizer
fullName: azure.search.documents.indexes.models.NGramTokenizer
module: azure.search.documents.indexes.models
inheritances:
- azure.search.documents.indexes._generated.models._models_py3.LexicalTokenizer
summary: 'Tokenizes the input into n-grams of the given size(s). This tokenizer is
  implemented using Apache Lucene.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'NGramTokenizer(*, name: str, min_gram: Optional[int] = 1, max_gram: Optional[int]
    = 2, token_chars: Optional[List[Union[str, azure.search.documents.indexes._generated.models._search_client_enums.TokenCharacterKind]]]
    = None, **kwargs)'
variables:
- description: 'Required. Identifies the concrete type of the tokenizer.Constant filled
    by

    server.'
  name: odata_type
  types:
  - <xref:str>
- description: 'Required. The name of the tokenizer. It must only contain letters,
    digits, spaces,

    dashes or underscores, can only start and end with alphanumeric characters, and
    is limited to

    128 characters.'
  name: name
  types:
  - <xref:str>
- description: 'The minimum n-gram length. Default is 1. Maximum is 300. Must be less
    than the

    value of maxGram.'
  name: min_gram
  types:
  - <xref:int>
- description: The maximum n-gram length. Default is 2. Maximum is 300.
  name: max_gram
  types:
  - <xref:int>
- description: Character classes to keep in the tokens.
  name: token_chars
  types:
  - <xref:list>[<xref:str>
  - <xref:azure.search.documents.indexes.models.TokenCharacterKind>]
