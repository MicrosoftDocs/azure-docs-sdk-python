### YamlMime:PythonClass
uid: azure.synapse.artifacts.models.BigDataPoolResourceInfo
name: BigDataPoolResourceInfo
fullName: azure.synapse.artifacts.models.BigDataPoolResourceInfo
module: azure.synapse.artifacts.models
inheritances:
- azure.synapse.artifacts.models._models_py3.TrackedResource
summary: 'A Big Data pool.


  Variables are only populated by the server, and will be ignored when sending a request.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'BigDataPoolResourceInfo(*, location: str, tags: Dict[str, str] | None =
    None, provisioning_state: str | None = None, auto_scale: _models.AutoScaleProperties
    | None = None, creation_date: datetime | None = None, auto_pause: _models.AutoPauseProperties
    | None = None, is_compute_isolation_enabled: bool | None = None, session_level_packages_enabled:
    bool | None = None, cache_size: int | None = None, dynamic_executor_allocation:
    _models.DynamicExecutorAllocation | None = None, spark_events_folder: str | None
    = None, node_count: int | None = None, library_requirements: _models.LibraryRequirements
    | None = None, custom_libraries: List[_models.LibraryInfo] | None = None, spark_config_properties:
    _models.LibraryRequirements | None = None, spark_version: str | None = None, default_spark_log_folder:
    str | None = None, node_size: str | _models.NodeSize | None = None, node_size_family:
    str | _models.NodeSizeFamily | None = None, **kwargs)'
  parameters:
  - name: tags
    description: Resource tags.
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: location
    description: The geo-location where the resource lives. Required.
    types:
    - <xref:str>
  - name: provisioning_state
    description: The state of the Big Data pool.
    types:
    - <xref:str>
  - name: auto_scale
    description: Auto-scaling properties.
    types:
    - <xref:azure.synapse.artifacts.models.AutoScaleProperties>
  - name: creation_date
    description: The time when the Big Data pool was created.
    types:
    - <xref:datetime.datetime>
  - name: auto_pause
    description: Auto-pausing properties.
    types:
    - <xref:azure.synapse.artifacts.models.AutoPauseProperties>
  - name: is_compute_isolation_enabled
    description: Whether compute isolation is required or not.
    types:
    - <xref:bool>
  - name: session_level_packages_enabled
    description: Whether session level packages enabled.
    types:
    - <xref:bool>
  - name: cache_size
    description: The cache size.
    types:
    - <xref:int>
  - name: dynamic_executor_allocation
    description: Dynamic Executor Allocation.
    types:
    - <xref:azure.synapse.artifacts.models.DynamicExecutorAllocation>
  - name: spark_events_folder
    description: The Spark events folder.
    types:
    - <xref:str>
  - name: node_count
    description: The number of nodes in the Big Data pool.
    types:
    - <xref:int>
  - name: library_requirements
    description: Library version requirements.
    types:
    - <xref:azure.synapse.artifacts.models.LibraryRequirements>
  - name: custom_libraries
    description: List of custom libraries/packages associated with the spark pool.
    types:
    - <xref:list>[<xref:azure.synapse.artifacts.models.LibraryInfo>]
  - name: spark_config_properties
    description: Spark configuration file to specify additional properties.
    types:
    - <xref:azure.synapse.artifacts.models.LibraryRequirements>
  - name: spark_version
    description: The Apache Spark version.
    types:
    - <xref:str>
  - name: default_spark_log_folder
    description: The default folder where Spark logs will be written.
    types:
    - <xref:str>
  - name: node_size
    description: 'The level of compute power that each node in the Big Data pool has.
      Known

      values are: "None", "Small", "Medium", "Large", "XLarge", "XXLarge", and "XXXLarge".'
    types:
    - <xref:str>
    - <xref:azure.synapse.artifacts.models.NodeSize>
  - name: node_size_family
    description: 'The kind of nodes that the Big Data pool provides. Known values
      are:

      "None" and "MemoryOptimized".'
    types:
    - <xref:str>
    - <xref:azure.synapse.artifacts.models.NodeSizeFamily>
variables:
- description: 'Fully qualified resource ID for the resource. Ex -

    /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}.'
  name: id
  types:
  - <xref:str>
- description: The name of the resource.
  name: name
  types:
  - <xref:str>
- description: 'The type of the resource. E.g. "Microsoft.Compute/virtualMachines"
    or

    "Microsoft.Storage/storageAccounts".'
  name: type
  types:
  - <xref:str>
- description: Resource tags.
  name: tags
  types:
  - <xref:dict>[<xref:str>, <xref:str>]
- description: The geo-location where the resource lives. Required.
  name: location
  types:
  - <xref:str>
- description: The state of the Big Data pool.
  name: provisioning_state
  types:
  - <xref:str>
- description: Auto-scaling properties.
  name: auto_scale
  types:
  - <xref:azure.synapse.artifacts.models.AutoScaleProperties>
- description: The time when the Big Data pool was created.
  name: creation_date
  types:
  - <xref:datetime.datetime>
- description: Auto-pausing properties.
  name: auto_pause
  types:
  - <xref:azure.synapse.artifacts.models.AutoPauseProperties>
- description: Whether compute isolation is required or not.
  name: is_compute_isolation_enabled
  types:
  - <xref:bool>
- description: Whether session level packages enabled.
  name: session_level_packages_enabled
  types:
  - <xref:bool>
- description: The cache size.
  name: cache_size
  types:
  - <xref:int>
- description: Dynamic Executor Allocation.
  name: dynamic_executor_allocation
  types:
  - <xref:azure.synapse.artifacts.models.DynamicExecutorAllocation>
- description: The Spark events folder.
  name: spark_events_folder
  types:
  - <xref:str>
- description: The number of nodes in the Big Data pool.
  name: node_count
  types:
  - <xref:int>
- description: Library version requirements.
  name: library_requirements
  types:
  - <xref:azure.synapse.artifacts.models.LibraryRequirements>
- description: List of custom libraries/packages associated with the spark pool.
  name: custom_libraries
  types:
  - <xref:list>[<xref:azure.synapse.artifacts.models.LibraryInfo>]
- description: Spark configuration file to specify additional properties.
  name: spark_config_properties
  types:
  - <xref:azure.synapse.artifacts.models.LibraryRequirements>
- description: The Apache Spark version.
  name: spark_version
  types:
  - <xref:str>
- description: The default folder where Spark logs will be written.
  name: default_spark_log_folder
  types:
  - <xref:str>
- description: 'The level of compute power that each node in the Big Data pool has.
    Known

    values are: "None", "Small", "Medium", "Large", "XLarge", "XXLarge", and "XXXLarge".'
  name: node_size
  types:
  - <xref:str>
  - <xref:azure.synapse.artifacts.models.NodeSize>
- description: 'The kind of nodes that the Big Data pool provides. Known values are:

    "None" and "MemoryOptimized".'
  name: node_size_family
  types:
  - <xref:str>
  - <xref:azure.synapse.artifacts.models.NodeSizeFamily>
- description: The time when the Big Data pool was updated successfully.
  name: last_succeeded_timestamp
  types:
  - <xref:datetime.datetime>
methods:
- uid: azure.synapse.artifacts.models.BigDataPoolResourceInfo.as_dict
  name: as_dict
  summary: "Return a dict that can be JSONify using json.dump.\n\nAdvanced usage might\
    \ optionally use a callback as parameter:\n\nKey is the attribute name used in\
    \ Python. Attr_desc\nis a dict of metadata. Currently contains 'type' with the\n\
    msrest type and 'key' with the RestAPI encoded key.\nValue is the current value\
    \ in this object.\n\nThe string returned will be used to serialize the key.\n\
    If the return type is a list, this is considered hierarchical\nresult dict.\n\n\
    See the three examples in this file:\n\n* attribute_transformer \n\n* full_restapi_key_transformer\
    \ \n\n* last_restapi_key_transformer \n\nIf you want XML serialization, you can\
    \ pass the kwargs is_xml=True."
  signature: as_dict(keep_readonly=True, key_transformer=<function attribute_transformer>,
    **kwargs)
  parameters:
  - name: key_transformer
    description: A key transformer function.
    types:
    - <xref:function>
  - name: keep_readonly
    defaultValue: 'True'
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
- uid: azure.synapse.artifacts.models.BigDataPoolResourceInfo.deserialize
  name: deserialize
  summary: Parse a str using the RestAPI syntax and return a model.
  signature: deserialize(data, content_type=None)
  parameters:
  - name: data
    description: A str using RestAPI structure. JSON by default.
    isRequired: true
    types:
    - <xref:str>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  return:
    description: An instance of this model
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.synapse.artifacts.models.BigDataPoolResourceInfo.enable_additional_properties_sending
  name: enable_additional_properties_sending
  signature: enable_additional_properties_sending()
- uid: azure.synapse.artifacts.models.BigDataPoolResourceInfo.from_dict
  name: from_dict
  summary: 'Parse a dict using given key extractor return a model.


    By default consider key

    extractors (rest_key_case_insensitive_extractor, attribute_key_case_insensitive_extractor

    and last_rest_key_case_insensitive_extractor)'
  signature: from_dict(data, key_extractors=None, content_type=None)
  parameters:
  - name: data
    description: A dict using RestAPI structure
    isRequired: true
    types:
    - <xref:dict>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  - name: key_extractors
    defaultValue: None
  return:
    description: An instance of this model
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.synapse.artifacts.models.BigDataPoolResourceInfo.is_xml_model
  name: is_xml_model
  signature: is_xml_model()
- uid: azure.synapse.artifacts.models.BigDataPoolResourceInfo.serialize
  name: serialize
  summary: 'Return the JSON that would be sent to azure from this model.


    This is an alias to *as_dict(full_restapi_key_transformer, keep_readonly=False)*.


    If you want XML serialization, you can pass the kwargs is_xml=True.'
  signature: serialize(keep_readonly=False, **kwargs)
  parameters:
  - name: keep_readonly
    description: If you want to serialize the readonly attributes
    defaultValue: 'False'
    types:
    - <xref:bool>
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
