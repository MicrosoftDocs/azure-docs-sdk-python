### YamlMime:PythonClass
uid: azure.synapse.artifacts.models.AzureDatabricksLinkedService
name: AzureDatabricksLinkedService
fullName: azure.synapse.artifacts.models.AzureDatabricksLinkedService
module: azure.synapse.artifacts.models
inheritances:
- azure.synapse.artifacts.models._models_py3.LinkedService
summary: 'Azure Databricks linked service.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'AzureDatabricksLinkedService(*, domain: MutableMapping[str, Any], additional_properties:
    Dict[str, MutableMapping[str, Any]] | None = None, connect_via: _models.IntegrationRuntimeReference
    | None = None, description: str | None = None, parameters: Dict[str, _models.ParameterSpecification]
    | None = None, annotations: List[MutableMapping[str, Any]] | None = None, access_token:
    _models.SecretBase | None = None, authentication: MutableMapping[str, Any] | None
    = None, workspace_resource_id: MutableMapping[str, Any] | None = None, existing_cluster_id:
    MutableMapping[str, Any] | None = None, instance_pool_id: MutableMapping[str,
    Any] | None = None, new_cluster_version: MutableMapping[str, Any] | None = None,
    new_cluster_num_of_worker: MutableMapping[str, Any] | None = None, new_cluster_node_type:
    MutableMapping[str, Any] | None = None, new_cluster_spark_conf: Dict[str, MutableMapping[str,
    Any]] | None = None, new_cluster_spark_env_vars: Dict[str, MutableMapping[str,
    Any]] | None = None, new_cluster_custom_tags: Dict[str, MutableMapping[str, Any]]
    | None = None, new_cluster_log_destination: MutableMapping[str, Any] | None =
    None, new_cluster_driver_node_type: MutableMapping[str, Any] | None = None, new_cluster_init_scripts:
    MutableMapping[str, Any] | None = None, new_cluster_enable_elastic_disk: MutableMapping[str,
    Any] | None = None, encrypted_credential: MutableMapping[str, Any] | None = None,
    policy_id: MutableMapping[str, Any] | None = None, credential: _models.CredentialReference
    | None = None, **kwargs: Any)'
  parameters:
  - name: additional_properties
    description: 'Unmatched properties from the message are deserialized to this

      collection.'
    types:
    - <xref:dict>[<xref:str>, <xref:JSON>]
  - name: connect_via
    description: The integration runtime reference.
    types:
    - <xref:azure.synapse.artifacts.models.IntegrationRuntimeReference>
  - name: description
    description: Linked service description.
    types:
    - <xref:str>
  - name: parameters
    description: Parameters for linked service.
    types:
    - <xref:dict>[<xref:str>, <xref:azure.synapse.artifacts.models.ParameterSpecification>]
  - name: annotations
    description: List of tags that can be used for describing the linked service.
    types:
    - <xref:list>[<xref:JSON>]
  - name: domain
    description: '`<REGION>`.azuredatabricks.net, domain name of your Databricks

      deployment. Type: string (or Expression with resultType string). Required.'
    types:
    - <xref:JSON>
  - name: access_token
    description: 'Access token for databricks REST API. Refer to

      [https://docs.azuredatabricks.net/api/latest/authentication.html](https://docs.azuredatabricks.net/api/latest/authentication.html).
      Type: string (or Expression

      with resultType string).'
    types:
    - <xref:azure.synapse.artifacts.models.SecretBase>
  - name: authentication
    description: 'Required to specify MSI, if using Workspace resource id for databricks

      REST API. Type: string (or Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: workspace_resource_id
    description: 'Workspace resource id for databricks REST API. Type: string (or

      Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: existing_cluster_id
    description: 'The id of an existing interactive cluster that will be used for

      all runs of this activity. Type: string (or Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: instance_pool_id
    description: 'The id of an existing instance pool that will be used for all runs

      of this activity. Type: string (or Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: new_cluster_version
    description: 'If not using an existing interactive cluster, this specifies the

      Spark version of a new job cluster or instance pool nodes created for each run
      of this

      activity. Required if instancePoolId is specified. Type: string (or Expression
      with resultType

      string).'
    types:
    - <xref:JSON>
  - name: new_cluster_num_of_worker
    description: 'If not using an existing interactive cluster, this

      specifies the number of worker nodes to use for the new job cluster or instance
      pool. For new

      job clusters, this a string-formatted Int32, like ''1'' means numOfWorker is
      1 or ''1:10'' means

      auto-scale from 1 (min) to 10 (max). For instance pools, this is a string-formatted
      Int32, and

      can only specify a fixed number of worker nodes, such as ''2''. Required if
      newClusterVersion is

      specified. Type: string (or Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: new_cluster_node_type
    description: 'The node type of the new job cluster. This property is required

      if newClusterVersion is specified and instancePoolId is not specified. If instancePoolId
      is

      specified, this property is ignored. Type: string (or Expression with resultType
      string).'
    types:
    - <xref:JSON>
  - name: new_cluster_spark_conf
    description: 'A set of optional, user-specified Spark configuration

      key-value pairs.'
    types:
    - <xref:dict>[<xref:str>, <xref:JSON>]
  - name: new_cluster_spark_env_vars
    description: 'A set of optional, user-specified Spark environment

      variables key-value pairs.'
    types:
    - <xref:dict>[<xref:str>, <xref:JSON>]
  - name: new_cluster_custom_tags
    description: 'Additional tags for cluster resources. This property is

      ignored in instance pool configurations.'
    types:
    - <xref:dict>[<xref:str>, <xref:JSON>]
  - name: new_cluster_log_destination
    description: 'Specify a location to deliver Spark driver, worker, and

      event logs. Type: string (or Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: new_cluster_driver_node_type
    description: 'The driver node type for the new job cluster. This

      property is ignored in instance pool configurations. Type: string (or Expression
      with

      resultType string).'
    types:
    - <xref:JSON>
  - name: new_cluster_init_scripts
    description: 'User-defined initialization scripts for the new cluster.

      Type: array of strings (or Expression with resultType array of strings).'
    types:
    - <xref:JSON>
  - name: new_cluster_enable_elastic_disk
    description: 'Enable the elastic disk on the new cluster. This

      property is now ignored, and takes the default elastic disk behavior in Databricks
      (elastic

      disks are always enabled). Type: boolean (or Expression with resultType boolean).'
    types:
    - <xref:JSON>
  - name: encrypted_credential
    description: 'The encrypted credential used for authentication. Credentials

      are encrypted using the integration runtime credential manager. Type: string
      (or Expression

      with resultType string).'
    types:
    - <xref:JSON>
  - name: policy_id
    description: 'The policy id for limiting the ability to configure clusters based
      on a

      user defined set of rules. Type: string (or Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: credential
    description: The credential reference containing authentication information.
    types:
    - <xref:azure.synapse.artifacts.models.CredentialReference>
variables:
- description: 'Unmatched properties from the message are deserialized to this

    collection.'
  name: additional_properties
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: Type of linked service. Required.
  name: type
  types:
  - <xref:str>
- description: The integration runtime reference.
  name: connect_via
  types:
  - <xref:azure.synapse.artifacts.models.IntegrationRuntimeReference>
- description: Linked service description.
  name: description
  types:
  - <xref:str>
- description: Parameters for linked service.
  name: parameters
  types:
  - <xref:dict>[<xref:str>, <xref:azure.synapse.artifacts.models.ParameterSpecification>]
- description: List of tags that can be used for describing the linked service.
  name: annotations
  types:
  - <xref:list>[<xref:JSON>]
- description: '`<REGION>`.azuredatabricks.net, domain name of your Databricks deployment.

    Type: string (or Expression with resultType string). Required.'
  name: domain
  types:
  - <xref:JSON>
- description: 'Access token for databricks REST API. Refer to

    [https://docs.azuredatabricks.net/api/latest/authentication.html](https://docs.azuredatabricks.net/api/latest/authentication.html).
    Type: string (or Expression

    with resultType string).'
  name: access_token
  types:
  - <xref:azure.synapse.artifacts.models.SecretBase>
- description: 'Required to specify MSI, if using Workspace resource id for databricks

    REST API. Type: string (or Expression with resultType string).'
  name: authentication
  types:
  - <xref:JSON>
- description: 'Workspace resource id for databricks REST API. Type: string (or

    Expression with resultType string).'
  name: workspace_resource_id
  types:
  - <xref:JSON>
- description: 'The id of an existing interactive cluster that will be used for all

    runs of this activity. Type: string (or Expression with resultType string).'
  name: existing_cluster_id
  types:
  - <xref:JSON>
- description: 'The id of an existing instance pool that will be used for all runs
    of

    this activity. Type: string (or Expression with resultType string).'
  name: instance_pool_id
  types:
  - <xref:JSON>
- description: 'If not using an existing interactive cluster, this specifies the

    Spark version of a new job cluster or instance pool nodes created for each run
    of this

    activity. Required if instancePoolId is specified. Type: string (or Expression
    with resultType

    string).'
  name: new_cluster_version
  types:
  - <xref:JSON>
- description: 'If not using an existing interactive cluster, this specifies

    the number of worker nodes to use for the new job cluster or instance pool. For
    new job

    clusters, this a string-formatted Int32, like ''1'' means numOfWorker is 1 or
    ''1:10'' means

    auto-scale from 1 (min) to 10 (max). For instance pools, this is a string-formatted
    Int32, and

    can only specify a fixed number of worker nodes, such as ''2''. Required if newClusterVersion
    is

    specified. Type: string (or Expression with resultType string).'
  name: new_cluster_num_of_worker
  types:
  - <xref:JSON>
- description: 'The node type of the new job cluster. This property is required if

    newClusterVersion is specified and instancePoolId is not specified. If instancePoolId
    is

    specified, this property is ignored. Type: string (or Expression with resultType
    string).'
  name: new_cluster_node_type
  types:
  - <xref:JSON>
- description: 'A set of optional, user-specified Spark configuration key-value

    pairs.'
  name: new_cluster_spark_conf
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: 'A set of optional, user-specified Spark environment variables

    key-value pairs.'
  name: new_cluster_spark_env_vars
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: 'Additional tags for cluster resources. This property is ignored

    in instance pool configurations.'
  name: new_cluster_custom_tags
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: 'Specify a location to deliver Spark driver, worker, and

    event logs. Type: string (or Expression with resultType string).'
  name: new_cluster_log_destination
  types:
  - <xref:JSON>
- description: 'The driver node type for the new job cluster. This property

    is ignored in instance pool configurations. Type: string (or Expression with resultType

    string).'
  name: new_cluster_driver_node_type
  types:
  - <xref:JSON>
- description: 'User-defined initialization scripts for the new cluster. Type:

    array of strings (or Expression with resultType array of strings).'
  name: new_cluster_init_scripts
  types:
  - <xref:JSON>
- description: 'Enable the elastic disk on the new cluster. This

    property is now ignored, and takes the default elastic disk behavior in Databricks
    (elastic

    disks are always enabled). Type: boolean (or Expression with resultType boolean).'
  name: new_cluster_enable_elastic_disk
  types:
  - <xref:JSON>
- description: 'The encrypted credential used for authentication. Credentials are

    encrypted using the integration runtime credential manager. Type: string (or Expression
    with

    resultType string).'
  name: encrypted_credential
  types:
  - <xref:JSON>
- description: 'The policy id for limiting the ability to configure clusters based
    on a user

    defined set of rules. Type: string (or Expression with resultType string).'
  name: policy_id
  types:
  - <xref:JSON>
- description: The credential reference containing authentication information.
  name: credential
  types:
  - <xref:azure.synapse.artifacts.models.CredentialReference>
methods:
- uid: azure.synapse.artifacts.models.AzureDatabricksLinkedService.as_dict
  name: as_dict
  summary: "Return a dict that can be serialized using json.dump.\n\nAdvanced usage\
    \ might optionally use a callback as parameter:\n\nKey is the attribute name used\
    \ in Python. Attr_desc\nis a dict of metadata. Currently contains 'type' with\
    \ the\nmsrest type and 'key' with the RestAPI encoded key.\nValue is the current\
    \ value in this object.\n\nThe string returned will be used to serialize the key.\n\
    If the return type is a list, this is considered hierarchical\nresult dict.\n\n\
    See the three examples in this file:\n\n* attribute_transformer \n\n* full_restapi_key_transformer\
    \ \n\n* last_restapi_key_transformer \n\nIf you want XML serialization, you can\
    \ pass the kwargs is_xml=True."
  signature: 'as_dict(keep_readonly: bool = True, key_transformer: ~typing.Callable[[str,
    ~typing.Dict[str, ~typing.Any], ~typing.Any], ~typing.Any] = <function attribute_transformer>,
    **kwargs: ~typing.Any) -> MutableMapping[str, Any]'
  parameters:
  - name: key_transformer
    description: A key transformer function.
    types:
    - <xref:function>
  - name: keep_readonly
    defaultValue: 'True'
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
- uid: azure.synapse.artifacts.models.AzureDatabricksLinkedService.deserialize
  name: deserialize
  summary: Parse a str using the RestAPI syntax and return a model.
  signature: 'deserialize(data: Any, content_type: str | None = None) -> ModelType'
  parameters:
  - name: data
    description: A str using RestAPI structure. JSON by default.
    isRequired: true
    types:
    - <xref:str>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  return:
    description: An instance of this model
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.synapse.artifacts.models.AzureDatabricksLinkedService.enable_additional_properties_sending
  name: enable_additional_properties_sending
  signature: enable_additional_properties_sending() -> None
- uid: azure.synapse.artifacts.models.AzureDatabricksLinkedService.from_dict
  name: from_dict
  summary: 'Parse a dict using given key extractor return a model.


    By default consider key

    extractors (rest_key_case_insensitive_extractor, attribute_key_case_insensitive_extractor

    and last_rest_key_case_insensitive_extractor)'
  signature: 'from_dict(data: Any, key_extractors: Callable[[str, Dict[str, Any],
    Any], Any] | None = None, content_type: str | None = None) -> ModelType'
  parameters:
  - name: data
    description: A dict using RestAPI structure
    isRequired: true
    types:
    - <xref:dict>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  - name: key_extractors
    defaultValue: None
  return:
    description: An instance of this model
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.synapse.artifacts.models.AzureDatabricksLinkedService.is_xml_model
  name: is_xml_model
  signature: is_xml_model() -> bool
- uid: azure.synapse.artifacts.models.AzureDatabricksLinkedService.serialize
  name: serialize
  summary: 'Return the JSON that would be sent to azure from this model.


    This is an alias to *as_dict(full_restapi_key_transformer, keep_readonly=False)*.


    If you want XML serialization, you can pass the kwargs is_xml=True.'
  signature: 'serialize(keep_readonly: bool = False, **kwargs: Any) -> MutableMapping[str,
    Any]'
  parameters:
  - name: keep_readonly
    description: If you want to serialize the readonly attributes
    defaultValue: 'False'
    types:
    - <xref:bool>
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
