### YamlMime:PythonClass
uid: azure.mgmt.synapse.models.BigDataPoolResourceInfo
name: BigDataPoolResourceInfo
fullName: azure.mgmt.synapse.models.BigDataPoolResourceInfo
module: azure.mgmt.synapse.models
inheritances:
- azure.mgmt.synapse.models._models_py3.TrackedResource
summary: 'A Big Data pool.


  Variables are only populated by the server, and will be ignored when sending a request.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'BigDataPoolResourceInfo(*, location: str, tags: Optional[Dict[str, str]]
    = None, provisioning_state: Optional[str] = None, auto_scale: Optional[azure.mgmt.synapse.models._models_py3.AutoScaleProperties]
    = None, auto_pause: Optional[azure.mgmt.synapse.models._models_py3.AutoPauseProperties]
    = None, is_compute_isolation_enabled: Optional[bool] = None, session_level_packages_enabled:
    Optional[bool] = None, cache_size: Optional[int] = None, dynamic_executor_allocation:
    Optional[azure.mgmt.synapse.models._models_py3.DynamicExecutorAllocation] = None,
    spark_events_folder: Optional[str] = None, node_count: Optional[int] = None, library_requirements:
    Optional[azure.mgmt.synapse.models._models_py3.LibraryRequirements] = None, custom_libraries:
    Optional[List[azure.mgmt.synapse.models._models_py3.LibraryInfo]] = None, spark_config_properties:
    Optional[azure.mgmt.synapse.models._models_py3.SparkConfigProperties] = None,
    spark_version: Optional[str] = None, default_spark_log_folder: Optional[str] =
    None, node_size: Optional[Union[str, azure.mgmt.synapse.models._synapse_management_client_enums.NodeSize]]
    = None, node_size_family: Optional[Union[str, azure.mgmt.synapse.models._synapse_management_client_enums.NodeSizeFamily]]
    = None, **kwargs)'
variables:
- description: 'Fully qualified resource ID for the resource. Ex -

    /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}.'
  name: id
  types:
  - <xref:str>
- description: The name of the resource.
  name: name
  types:
  - <xref:str>
- description: 'The type of the resource. E.g. "Microsoft.Compute/virtualMachines"
    or

    "Microsoft.Storage/storageAccounts".'
  name: type
  types:
  - <xref:str>
- description: A set of tags. Resource tags.
  name: tags
  types:
  - <xref:dict>[<xref:str>, <xref:str>]
- description: Required. The geo-location where the resource lives.
  name: location
  types:
  - <xref:str>
- description: The state of the Big Data pool.
  name: provisioning_state
  types:
  - <xref:str>
- description: Auto-scaling properties.
  name: auto_scale
  types:
  - <xref:azure.mgmt.synapse.models.AutoScaleProperties>
- description: The time when the Big Data pool was created.
  name: creation_date
  types:
  - <xref:datetime.datetime>
- description: Auto-pausing properties.
  name: auto_pause
  types:
  - <xref:azure.mgmt.synapse.models.AutoPauseProperties>
- description: Whether compute isolation is required or not.
  name: is_compute_isolation_enabled
  types:
  - <xref:bool>
- description: Whether session level packages enabled.
  name: session_level_packages_enabled
  types:
  - <xref:bool>
- description: The cache size.
  name: cache_size
  types:
  - <xref:int>
- description: Dynamic Executor Allocation.
  name: dynamic_executor_allocation
  types:
  - <xref:azure.mgmt.synapse.models.DynamicExecutorAllocation>
- description: The Spark events folder.
  name: spark_events_folder
  types:
  - <xref:str>
- description: The number of nodes in the Big Data pool.
  name: node_count
  types:
  - <xref:int>
- description: Library version requirements.
  name: library_requirements
  types:
  - <xref:azure.mgmt.synapse.models.LibraryRequirements>
- description: List of custom libraries/packages associated with the spark pool.
  name: custom_libraries
  types:
  - <xref:list>[<xref:azure.mgmt.synapse.models.LibraryInfo>]
- description: Spark configuration file to specify additional properties.
  name: spark_config_properties
  types:
  - <xref:azure.mgmt.synapse.models.SparkConfigProperties>
- description: The Apache Spark version.
  name: spark_version
  types:
  - <xref:str>
- description: The default folder where Spark logs will be written.
  name: default_spark_log_folder
  types:
  - <xref:str>
- description: 'The level of compute power that each node in the Big Data pool has.
    Possible

    values include: "None", "Small", "Medium", "Large", "XLarge", "XXLarge", "XXXLarge".'
  name: node_size
  types:
  - <xref:str>
  - <xref:azure.mgmt.synapse.models.NodeSize>
- description: 'The kind of nodes that the Big Data pool provides. Possible values

    include: "None", "MemoryOptimized", "HardwareAcceleratedFPGA", "HardwareAcceleratedGPU".'
  name: node_size_family
  types:
  - <xref:str>
  - <xref:azure.mgmt.synapse.models.NodeSizeFamily>
- description: The time when the Big Data pool was updated successfully.
  name: last_succeeded_timestamp
  types:
  - <xref:datetime.datetime>
