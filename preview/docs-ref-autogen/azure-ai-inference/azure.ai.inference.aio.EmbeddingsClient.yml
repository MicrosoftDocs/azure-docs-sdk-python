### YamlMime:PythonClass
uid: azure.ai.inference.aio.EmbeddingsClient
name: EmbeddingsClient
fullName: azure.ai.inference.aio.EmbeddingsClient
module: azure.ai.inference.aio
inheritances:
- azure.ai.inference.aio._client.EmbeddingsClient
summary: EmbeddingsClient.
constructor:
  syntax: 'EmbeddingsClient(endpoint: str, credential: AzureKeyCredential | AsyncTokenCredential,
    *, dimensions: int | None = None, encoding_format: str | EmbeddingEncodingFormat
    | None = None, input_type: str | EmbeddingInputType | None = None, model: str
    | None = None, model_extras: Dict[str, Any] | None = None, **kwargs: Any)'
  parameters:
  - name: endpoint
    description: Service endpoint URL for AI model inference. Required.
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: 'Credential used to authenticate requests to the service. Is either
      a

      AzureKeyCredential type or a AsyncTokenCredential type. Required.'
    isRequired: true
    types:
    - <xref:azure.core.credentials.AzureKeyCredential>
    - <xref:azure.core.credentials_async.AsyncTokenCredential>
  keywordOnlyParameters:
  - name: dimensions
    description: 'Optional. The number of dimensions the resulting output embeddings
      should

      have. Default value is None.'
    types:
    - <xref:int>
  - name: encoding_format
    description: 'Optional. The desired format for the returned embeddings.

      Known values are:

      "base64", "binary", "float", "int8", "ubinary", and "uint8". Default value is
      None.'
    types:
    - <xref:str>
    - <xref:azure.ai.inference.models.EmbeddingEncodingFormat>
  - name: input_type
    description: 'Optional. The type of the input. Known values are:

      "text", "query", and "document". Default value is None.'
    types:
    - <xref:str>
    - <xref:azure.ai.inference.models.EmbeddingInputType>
  - name: model
    description: 'ID of the specific AI model to use, if more than one model is available
      on the

      endpoint. Default value is None.'
    types:
    - <xref:str>
  - name: model_extras
    description: 'Additional, model-specific parameters that are not in the

      standard request payload. They will be added as-is to the root of the JSON in
      the request body.

      How the service handles these extra parameters depends on the value of the

      `extra-parameters` request header. Default value is None.'
    types:
    - <xref:dict>[<xref:str>, <xref:typing.Any>]
  - name: api_version
    description: 'The API version to use for this operation. Default value is

      "2024-05-01-preview". Note that overriding this default value may result in
      unsupported

      behavior.'
    types:
    - <xref:str>
methods:
- uid: azure.ai.inference.aio.EmbeddingsClient.close
  name: close
  signature: async close() -> None
- uid: azure.ai.inference.aio.EmbeddingsClient.embed
  name: embed
  summary: 'Return the embedding vectors for given text prompts.

    The method makes a REST API call to the */embeddings* route on the given endpoint.'
  signature: 'async embed(*, input: List[str], dimensions: int | None = None, encoding_format:
    str | _models.EmbeddingEncodingFormat | None = None, input_type: str | _models.EmbeddingInputType
    | None = None, model: str | None = None, model_extras: Dict[str, Any] | None =
    None, **kwargs: Any) -> _models.EmbeddingsResult'
  parameters:
  - name: body
    description: 'Is either a MutableMapping[str, Any] type (like a dictionary) or
      a IO[bytes] type

      that specifies the full request payload. Required.'
    isRequired: true
    types:
    - <xref:JSON>
    - <xref:typing.IO>[<xref:bytes>]
  keywordOnlyParameters:
  - name: input
    description: 'Input text to embed, encoded as a string or array of tokens.

      To embed multiple inputs in a single request, pass an array

      of strings or array of token arrays. Required.'
    types:
    - <xref:list>[<xref:str>]
  - name: dimensions
    description: 'Optional. The number of dimensions the resulting output embeddings
      should

      have. Default value is None.'
    types:
    - <xref:int>
  - name: encoding_format
    description: 'Optional. The desired format for the returned embeddings.

      Known values are:

      "base64", "binary", "float", "int8", "ubinary", and "uint8". Default value is
      None.'
    types:
    - <xref:str>
    - <xref:azure.ai.inference.models.EmbeddingEncodingFormat>
  - name: input_type
    description: 'Optional. The type of the input. Known values are:

      "text", "query", and "document". Default value is None.'
    types:
    - <xref:str>
    - <xref:azure.ai.inference.models.EmbeddingInputType>
  - name: model
    description: 'ID of the specific AI model to use, if more than one model is available
      on the

      endpoint. Default value is None.'
    types:
    - <xref:str>
  - name: model_extras
    description: 'Additional, model-specific parameters that are not in the

      standard request payload. They will be added as-is to the root of the JSON in
      the request body.

      How the service handles these extra parameters depends on the value of the

      `extra-parameters` request header. Default value is None.'
    types:
    - <xref:dict>[<xref:str>, <xref:typing.Any>]
  return:
    description: EmbeddingsResult. The EmbeddingsResult is compatible with MutableMapping
    types:
    - <xref:azure.ai.inference.models.EmbeddingsResult>
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
- uid: azure.ai.inference.aio.EmbeddingsClient.get_model_info
  name: get_model_info
  summary: 'Returns information about the AI model.

    The method makes a REST API call to the `/info` route on the given endpoint.

    This method will only work when using Serverless API or Managed Compute endpoint.

    It will not work for GitHub Models endpoint or Azure OpenAI endpoint.'
  signature: 'async get_model_info(**kwargs: Any) -> ModelInfo'
  return:
    description: ModelInfo. The ModelInfo is compatible with MutableMapping
    types:
    - <xref:azure.ai.inference.models.ModelInfo>
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
- uid: azure.ai.inference.aio.EmbeddingsClient.send_request
  name: send_request
  summary: 'Runs the network request through the client''s chained policies.


    ```


    >>> from azure.core.rest import HttpRequest

    >>> request = HttpRequest("GET", "https://www.example.org/")

    <HttpRequest [GET], url: ''https://www.example.org/''>

    >>> response = await client.send_request(request)

    <AsyncHttpResponse: 200 OK>

    ```


    For more information on this code flow, see [https://aka.ms/azsdk/dpcodegen/python/send_request](https://aka.ms/azsdk/dpcodegen/python/send_request)'
  signature: 'send_request(request: HttpRequest, *, stream: bool = False, **kwargs:
    Any) -> Awaitable[AsyncHttpResponse]'
  parameters:
  - name: request
    description: The network request you want to make. Required.
    isRequired: true
    types:
    - <xref:azure.core.rest.HttpRequest>
  keywordOnlyParameters:
  - name: stream
    description: Whether the response payload will be streamed. Defaults to False.
    types:
    - <xref:bool>
  return:
    description: The response of your network call. Does not do error handling on
      your response.
    types:
    - <xref:azure.core.rest.AsyncHttpResponse>
