### YamlMime:PythonPackage
uid: azure.ai.inference.aio
name: aio
fullName: azure.ai.inference.aio
type: import
functions:
- uid: azure.ai.inference.aio.load_client
  name: load_client
  summary: 'Load a client from a given endpoint URL. The method makes a REST API call
    to the */info* route

    on the given endpoint, to determine the model type and therefore which client
    to instantiate.

    This method will only work when using Serverless API or Managed Compute endpoint.

    It will not work for GitHub Models endpoint or Azure OpenAI endpoint.

    Keyword arguments are passed through to the client constructor (you can set keywords
    such as

    *api_version*, *user_agent*, *logging_enable* etc. on the client constructor).'
  signature: 'async load_client(endpoint: str, credential: AzureKeyCredential | AsyncTokenCredential,
    **kwargs: Any) -> ChatCompletionsClient | EmbeddingsClient | ImageEmbeddingsClient'
  parameters:
  - name: endpoint
    description: Service endpoint URL for AI model inference. Required.
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: 'Credential used to authenticate requests to the service. Is either
      a

      AzureKeyCredential type or a AsyncTokenCredential type. Required.'
    isRequired: true
    types:
    - <xref:azure.core.credentials.AzureKeyCredential>
    - <xref:azure.core.credentials_async.AsyncTokenCredential>
  return:
    description: The appropriate asynchronous client associated with the given endpoint
    types:
    - <xref:azure.ai.inference.aio.ChatCompletionsClient>
    - <xref:azure.ai.inference.aio.EmbeddingsClient>
    - <xref:azure.ai.inference.aio.ImageEmbeddingsClient>
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
classes:
- azure.ai.inference.aio.ChatCompletionsClient
- azure.ai.inference.aio.EmbeddingsClient
- azure.ai.inference.aio.ImageEmbeddingsClient
