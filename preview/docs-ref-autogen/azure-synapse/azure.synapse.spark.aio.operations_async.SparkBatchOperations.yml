### YamlMime:PythonClass
uid: azure.synapse.spark.aio.operations_async.SparkBatchOperations
name: SparkBatchOperations
fullName: azure.synapse.spark.aio.operations_async.SparkBatchOperations
module: azure.synapse.spark.aio.operations_async
inheritances:
- builtins.object
summary: 'SparkBatchOperations async operations.


  You should not instantiate this class directly. Instead, you should create a Client
  instance that

  instantiates it for you and attaches it as an attribute.'
constructor:
  syntax: SparkBatchOperations(client, config, serializer, deserializer)
  parameters:
  - name: client
    description: Client for service requests.
    isRequired: true
  - name: config
    description: Configuration of service client.
    isRequired: true
  - name: serializer
    description: An object model serializer.
    isRequired: true
  - name: deserializer
    description: An object model deserializer.
    isRequired: true
variables:
- description: Alias to model classes used in this operation group.
  name: models
methods:
- uid: azure.synapse.spark.aio.operations_async.SparkBatchOperations.cancel_spark_batch_job
  name: cancel_spark_batch_job
  summary: Cancels a running spark batch job.
  signature: 'async cancel_spark_batch_job(batch_id: int, **kwargs) -> None'
  parameters:
  - name: batch_id
    description: Identifier for the batch job.
    isRequired: true
    types:
    - <xref:int>
  keywordOnlyParameters:
  - name: cls
    description: A custom type or function that will be passed the direct response
    types:
    - <xref:callable>
  return:
    description: None, or the result of cls(response)
    types:
    - <xref:None>
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
- uid: azure.synapse.spark.aio.operations_async.SparkBatchOperations.create_spark_batch_job
  name: create_spark_batch_job
  summary: Create new spark batch job.
  signature: 'async create_spark_batch_job(spark_batch_job_options: SparkBatchJobOptions,
    detailed: bool | None = None, **kwargs) -> SparkBatchJob'
  parameters:
  - name: spark_batch_job_options
    description: Livy compatible batch job request payload.
    isRequired: true
    types:
    - <xref:azure.synapse.spark.models.SparkBatchJobOptions>
  - name: detailed
    description: 'Optional query param specifying whether detailed response is returned
      beyond

      plain livy.'
    defaultValue: None
    types:
    - <xref:bool>
  keywordOnlyParameters:
  - name: cls
    description: A custom type or function that will be passed the direct response
    types:
    - <xref:callable>
  return:
    description: SparkBatchJob, or the result of cls(response)
    types:
    - <xref:azure.synapse.spark.models.SparkBatchJob>
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
- uid: azure.synapse.spark.aio.operations_async.SparkBatchOperations.get_spark_batch_job
  name: get_spark_batch_job
  summary: Gets a single spark batch job.
  signature: 'async get_spark_batch_job(batch_id: int, detailed: bool | None = None,
    **kwargs) -> SparkBatchJob'
  parameters:
  - name: batch_id
    description: Identifier for the batch job.
    isRequired: true
    types:
    - <xref:int>
  - name: detailed
    description: 'Optional query param specifying whether detailed response is returned
      beyond

      plain livy.'
    defaultValue: None
    types:
    - <xref:bool>
  keywordOnlyParameters:
  - name: cls
    description: A custom type or function that will be passed the direct response
    types:
    - <xref:callable>
  return:
    description: SparkBatchJob, or the result of cls(response)
    types:
    - <xref:azure.synapse.spark.models.SparkBatchJob>
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
- uid: azure.synapse.spark.aio.operations_async.SparkBatchOperations.get_spark_batch_jobs
  name: get_spark_batch_jobs
  summary: List all spark batch jobs which are running under a particular spark pool.
  signature: 'async get_spark_batch_jobs(from_parameter: int | None = None, size:
    int | None = None, detailed: bool | None = None, **kwargs) -> SparkBatchJobCollection'
  parameters:
  - name: from_parameter
    description: Optional param specifying which index the list should begin from.
    defaultValue: None
    types:
    - <xref:int>
  - name: size
    description: 'Optional param specifying the size of the returned list.

      By default it is 20 and that is the maximum.'
    defaultValue: None
    types:
    - <xref:int>
  - name: detailed
    description: 'Optional query param specifying whether detailed response is returned
      beyond

      plain livy.'
    defaultValue: None
    types:
    - <xref:bool>
  keywordOnlyParameters:
  - name: cls
    description: A custom type or function that will be passed the direct response
    types:
    - <xref:callable>
  return:
    description: SparkBatchJobCollection, or the result of cls(response)
    types:
    - <xref:azure.synapse.spark.models.SparkBatchJobCollection>
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
attributes:
- uid: azure.synapse.spark.aio.operations_async.SparkBatchOperations.models
  name: models
  signature: models = <module 'azure.synapse.spark.models' from 'C:\\hostedtoolcache\\windows\\Python\\3.11.9\\x64\\Lib\\site-packages\\azure\\synapse\\spark\\models\\__init__.py'>
