### YamlMime:PythonClass
uid: azure.storage.filedatalake.DataLakeFileClient
name: DataLakeFileClient
fullName: azure.storage.filedatalake.DataLakeFileClient
module: azure.storage.filedatalake
inheritances:
- azure.storage.filedatalake._path_client.PathClient
summary: A client to interact with the DataLake file, even if the file may not yet
  exist.
constructor:
  syntax: 'DataLakeFileClient(account_url: str, file_system_name: str, file_path:
    str, credential: str | Dict[str, str] | AzureNamedKeyCredential | AzureSasCredential
    | TokenCredential | None = None, **kwargs: Any)'
  parameters:
  - name: account_url
    description: The URI to the storage account.
    isRequired: true
    types:
    - <xref:str>
  - name: file_system_name
    description: The file system for the directory or files.
    isRequired: true
    types:
    - <xref:str>
  - name: file_path
    description: 'The whole file path, so that to interact with a specific file.

      eg. "{directory}/{subdirectory}/{file}"'
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: 'The credentials with which to authenticate. This is optional if
      the

      account URL already has a SAS token. The value can be a SAS token string,

      an instance of a AzureSasCredential or AzureNamedKeyCredential from azure.core.credentials,

      an account shared access key, or an instance of a TokenCredentials class from
      azure.identity.

      If the resource URI already contains a SAS token, this will be ignored in favor
      of an explicit credential

      - except in the case of AzureSasCredential, where the conflicting SAS tokens
      will raise a ValueError.

      If using an instance of AzureNamedKeyCredential, "name" should be the storage
      account name, and "key"

      should be the storage account key.'
    defaultValue: None
    types:
    - <xref:azure.core.credentials.AzureNamedKeyCredential>
    - <xref:azure.core.credentials.AzureSasCredential>
    - <xref:azure.core.credentials.TokenCredential>
    - <xref:str>
    - <xref:dict>[<xref:str>, <xref:str>]
    - <xref:None>
  keywordOnlyParameters:
  - name: api_version
    description: 'The Storage API version to use for requests. Default value is the
      most recent service version that is

      compatible with the current SDK. Setting to an older version may result in reduced
      feature compatibility.'
    types:
    - <xref:str>
  - name: audience
    description: 'The audience to use when requesting tokens for Azure Active Directory

      authentication. Only has an effect when credential is of type TokenCredential.
      The value could be

      [https://storage.azure.com/](https://storage.azure.com/) (default) or [https:/](https:/)/<account>.blob.core.windows.net.'
    types:
    - <xref:str>
variables:
- description: The full endpoint URL to the file system, including SAS token if used.
  name: url
  types:
  - <xref:str>
- description: The full primary endpoint URL.
  name: primary_endpoint
  types:
  - <xref:str>
- description: The hostname of the primary endpoint.
  name: primary_hostname
  types:
  - <xref:str>
examples:
- "Creating the DataLakeServiceClient from connection string.<!--[!code-python[Main](les\\\
  datalake_samples_instantiate_client.py )]-->\n\n<!-- literal_block {\"ids\": [],\
  \ \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\"\
  : \"C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\
  \\py2docfx\\\\dist_temp\\\\215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\
  \\datalake_samples_instantiate_client.py\", \"xml:space\": \"preserve\", \"force\"\
  : false, \"language\": \"python\", \"highlight_args\": {\"linenostart\": 1}, \"\
  linenos\": false} -->\n\n````python\n\n   from azure.storage.filedatalake import\
  \ DataLakeFileClient\n   DataLakeFileClient.from_connection_string(connection_string,\
  \ \"myfilesystem\", \"mydirectory\", \"myfile\")\n\n   ````\n"
methods:
- uid: azure.storage.filedatalake.DataLakeFileClient.acquire_lease
  name: acquire_lease
  summary: 'Requests a new lease. If the file or directory does not have an active
    lease,

    the DataLake service creates a lease on the file/directory and returns a new

    lease ID.'
  signature: 'acquire_lease(lease_duration: int | None = -1, lease_id: str | None
    = None, **kwargs) -> DataLakeLeaseClient'
  parameters:
  - name: lease_duration
    description: 'Specifies the duration of the lease, in seconds, or negative one

      (-1) for a lease that never expires. A non-infinite lease can be

      between 15 and 60 seconds. A lease duration cannot be changed

      using renew or change. Default is -1 (infinite lease).'
    isRequired: true
    types:
    - <xref:int>
  - name: lease_id
    description: 'Proposed lease ID, in a GUID string format. The DataLake service
      returns

      400 (Invalid request) if the proposed lease ID is not in the correct format.'
    isRequired: true
    types:
    - <xref:str>
  keywordOnlyParameters:
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: A DataLakeLeaseClient object, that can be run in a context manager.
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
- uid: azure.storage.filedatalake.DataLakeFileClient.append_data
  name: append_data
  summary: Append data to the file.
  signature: 'append_data(data: bytes | str | Iterable[AnyStr] | IO[AnyStr], offset:
    int, length: int | None = None, **kwargs) -> Dict[str, str | datetime | int]'
  parameters:
  - name: data
    description: Content to be appended to file
    isRequired: true
    types:
    - <xref:bytes>, <xref:str>, <xref:typing.Iterable>[<xref:typing.AnyStr>]
    - <xref:typing.IO>[<xref:typing.AnyStr>]
  - name: offset
    description: start position of the data to be appended to.
    isRequired: true
    types:
    - <xref:int>
  - name: length
    description: Size of the data in bytes.
    isRequired: true
    types:
    - <xref:int>
    - <xref:None>
  keywordOnlyParameters:
  - name: flush
    description: If true, will commit the data after it is appended.
    types:
    - <xref:bool>
  - name: validate_content
    description: 'If true, calculates an MD5 hash of the block content. The storage

      service checks the hash of the content that has arrived

      with the hash that was sent. This is primarily valuable for detecting

      bitflips on the wire if using http instead of https as https (the default)

      will already validate. Note that this MD5 hash is not stored with the

      file.'
    types:
    - <xref:bool>
  - name: lease_action
    description: 'Used to perform lease operations along with appending data.


      "acquire" - Acquire a lease.

      "auto-renew" - Re-new an existing lease.

      "release" - Release the lease once the operation is complete. Requires *flush=True*.

      "acquire-release" - Acquire a lease and release it once the operations is complete.
      Requires *flush=True*.'
    types:
    - <xref:typing.Literal>["acquire", "auto-renew", "release", "acquire-release"]
  - name: lease_duration
    description: 'Valid if *lease_action* is set to "acquire" or "acquire-release".


      Specifies the duration of the lease, in seconds, or negative one

      (-1) for a lease that never expires. A non-infinite lease can be

      between 15 and 60 seconds. A lease duration cannot be changed

      using renew or change. Default is -1 (infinite lease).'
    types:
    - <xref:int>
  - name: lease
    description: 'Required if the file has an active lease or if *lease_action* is
      set to "acquire" or "acquire-release".

      If the file has an existing lease, this will be used to access the file. If
      acquiring a new lease,

      this will be used as the new lease id.

      Value can be a DataLakeLeaseClient object or the lease ID as a string.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: cpk
    description: 'Encrypts the data on the service-side with the given key.

      Use of customer-provided keys must be done over HTTPS.'
    types:
    - <xref:azure.storage.filedatalake.CustomerProvidedEncryptionKey>
  return:
    description: dict of the response header.
    types:
    - <xref:dict>[<xref:str>, <xref:str>], <xref:dict>[<xref:str>, <xref:datetime.datetime>],
    - <xref:dict>[<xref:str>, <xref:int>]
  examples:
  - "Append data to the file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   file_client.append_data(data=file_content[2048:3072], offset=2048, length=1024)\n\
    \n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.close
  name: close
  summary: 'This method is to close the sockets opened by the client.

    It need not be used when using with a context manager.'
  signature: close() -> None
- uid: azure.storage.filedatalake.DataLakeFileClient.create_file
  name: create_file
  summary: Create a new file.
  signature: 'create_file(content_settings: ContentSettings | None = None, metadata:
    Dict[str, str] | None = None, **kwargs) -> Dict[str, str | datetime]'
  parameters:
  - name: content_settings
    description: ContentSettings object used to set path properties.
    isRequired: true
    types:
    - <xref:azure.storage.filedatalake.ContentSettings>
  - name: metadata
    description: Name-value pairs associated with the file as metadata.
    isRequired: true
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>[<xref:str>, <xref:str>]]
  keywordOnlyParameters:
  - name: lease
    description: 'Required if the file has an active lease. Value can be a DataLakeLeaseClient
      object

      or the lease ID as a string.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: umask
    description: 'Optional and only valid if Hierarchical Namespace is enabled for
      the account.

      When creating a file or directory and the parent folder does not have a default
      ACL,

      the umask restricts the permissions of the file or directory to be created.

      The resulting permission is given by p & ^u, where p is the permission and u
      is the umask.

      For example, if p is 0777 and u is 0057, then the resulting permission is 0720.

      The default permission is 0777 for a directory and 0666 for a file. The default
      umask is 0027.

      The umask must be specified in 4-digit octal notation (e.g. 0766).'
    types:
    - <xref:str>
  - name: owner
    description: The owner of the file or directory.
    types:
    - <xref:str>
  - name: group
    description: The owning group of the file or directory.
    types:
    - <xref:str>
  - name: acl
    description: 'Sets POSIX access control rights on files and directories. The value
      is a

      comma-separated list of access control entries. Each access control entry (ACE)
      consists of a

      scope, a type, a user or group identifier, and permissions in the format

      "[scope:][type]:[id]:[permissions]".'
    types:
    - <xref:str>
  - name: lease_id
    description: 'Proposed lease ID, in a GUID string format. The DataLake service
      returns

      400 (Invalid request) if the proposed lease ID is not in the correct format.'
    types:
    - <xref:str>
  - name: lease_duration
    description: 'Specifies the duration of the lease, in seconds, or negative one

      (-1) for a lease that never expires. A non-infinite lease can be

      between 15 and 60 seconds. A lease duration cannot be changed

      using renew or change.'
    types:
    - <xref:int>
  - name: expires_on
    description: 'The time to set the file to expiry.

      If the type of expires_on is an int, expiration time will be set

      as the number of milliseconds elapsed from creation time.

      If the type of expires_on is datetime, expiration time will be set

      absolute to the time provided. If no time zone info is provided, this

      will be interpreted as UTC.'
    types:
    - <xref:datetime>
    - <xref:int>
  - name: permissions
    description: 'Optional and only valid if Hierarchical Namespace

      is enabled for the account. Sets POSIX access permissions for the file

      owner, the file owning group, and others. Each class may be granted

      read, write, or execute permission.  The sticky bit is also supported.

      Both symbolic (rwxrw-rw-) and 4-digit octal notation (e.g. 0766) are

      supported.'
    types:
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: cpk
    description: 'Encrypts the data on the service-side with the given key.

      Use of customer-provided keys must be done over HTTPS.'
    types:
    - <xref:azure.storage.filedatalake.CustomerProvidedEncryptionKey>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  - name: encryption_context
    description: Specifies the encryption context to set on the file.
    types:
    - <xref:str>
  return:
    description: response dict (Etag and last modified).
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
    - <xref:dict>[<xref:str>, <xref:datetime.datetime>]
  examples:
  - "Create file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   file_client = filesystem_client.get_file_client(file_name)\n   file_client.create_file()\n\
    \n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.delete_file
  name: delete_file
  summary: Marks the specified file for deletion.
  signature: delete_file(**kwargs) -> None
  keywordOnlyParameters:
  - name: lease
    description: 'Required if the file has an active lease. Value can be a LeaseClient
      object

      or the lease ID as a string.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: None.
    types:
    - <xref:None>
  examples:
  - "Delete file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   new_client.delete_file()\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.download_file
  name: download_file
  summary: 'Downloads a file to the StorageStreamDownloader. The readall() method
    must

    be used to read all the content, or readinto() must be used to download the file
    into

    a stream. Using chunks() returns an iterator which allows the user to iterate
    over the content in chunks.'
  signature: 'download_file(offset: int | None = None, length: int | None = None,
    **kwargs: Any) -> StorageStreamDownloader'
  parameters:
  - name: offset
    description: 'Start of byte range to use for downloading a section of the file.

      Must be set if length is provided.'
    isRequired: true
    types:
    - <xref:int>
  - name: length
    description: 'Number of bytes to read from the stream. This is optional, but

      should be supplied for optimal performance.'
    isRequired: true
    types:
    - <xref:int>
  keywordOnlyParameters:
  - name: lease
    description: 'If specified, download only succeeds if the file''s lease is active

      and matches this ID. Required if the file has an active lease.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: cpk
    description: 'Decrypts the data on the service-side with the given key.

      Use of customer-provided keys must be done over HTTPS.

      Required if the file was created with a Customer-Provided Key.'
    types:
    - <xref:azure.storage.filedatalake.CustomerProvidedEncryptionKey>
  - name: max_concurrency
    description: The number of parallel connections with which to download.
    types:
    - <xref:int>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).
      This method may make multiple calls to the service and

      the timeout will apply to each call individually.'
    types:
    - <xref:int>
  return:
    description: A streaming object (StorageStreamDownloader)
    types:
    - <xref:azure.storage.filedatalake.StorageStreamDownloader>
  examples:
  - "Return the downloaded data.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   download = file_client.download_file()\n   downloaded_bytes = download.readall()\n\
    \n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.exists
  name: exists
  summary: Returns True if a file exists and returns False otherwise.
  signature: 'exists(**kwargs: Any) -> bool'
  keywordOnlyParameters:
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: True if a file exists, otherwise returns False.
    types:
    - <xref:bool>
- uid: azure.storage.filedatalake.DataLakeFileClient.flush_data
  name: flush_data
  summary: Commit the previous appended data.
  signature: 'flush_data(offset: int, retain_uncommitted_data: bool | None = False,
    **kwargs) -> Dict[str, str | datetime]'
  parameters:
  - name: offset
    description: 'offset is equal to the length of the file after commit

      the previous appended data.'
    isRequired: true
    types:
    - <xref:int>
  - name: retain_uncommitted_data
    description: 'Valid only for flush operations.  If

      "true", uncommitted data is retained after the flush operation

      completes; otherwise, the uncommitted data is deleted after the flush

      operation.  The default is false.  Data at offsets less than the

      specified position are written to the file when flush succeeds, but

      this optional parameter allows data after the flush position to be

      retained for a future flush operation.'
    isRequired: true
    types:
    - <xref:bool>
  keywordOnlyParameters:
  - name: content_settings
    description: ContentSettings object used to set path properties.
    types:
    - <xref:azure.storage.filedatalake.ContentSettings>
  - name: close
    description: 'Azure Storage Events allow applications to receive

      notifications when files change. When Azure Storage Events are

      enabled, a file changed event is raised. This event has a property

      indicating whether this is the final change to distinguish the

      difference between an intermediate flush to a file stream and the

      final close of a file stream. The close query parameter is valid only

      when the action is "flush" and change notifications are enabled. If

      the value of close is "true" and the flush operation completes

      successfully, the service raises a file change notification with a

      property indicating that this is the final update (the file stream has

      been closed). If "false" a change notification is raised indicating

      the file has changed. The default is false. This query parameter is

      set to true by the Hadoop ABFS driver to indicate that the file stream

      has been closed."'
    types:
    - <xref:bool>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: lease_action
    description: 'Used to perform lease operations along with appending data.


      "acquire" - Acquire a lease.

      "auto-renew" - Re-new an existing lease.

      "release" - Release the lease once the operation is complete.

      "acquire-release" - Acquire a lease and release it once the operations is complete.'
    types:
    - <xref:typing.Literal>["acquire", "auto-renew", "release", "acquire-release"]
  - name: lease_duration
    description: 'Valid if *lease_action* is set to "acquire" or "acquire-release".


      Specifies the duration of the lease, in seconds, or negative one

      (-1) for a lease that never expires. A non-infinite lease can be

      between 15 and 60 seconds. A lease duration cannot be changed

      using renew or change. Default is -1 (infinite lease).'
    types:
    - <xref:int>
  - name: lease
    description: 'Required if the file has an active lease or if *lease_action* is
      set to "acquire" or "acquire-release".

      If the file has an existing lease, this will be used to access the file. If
      acquiring a new lease,

      this will be used as the new lease id.

      Value can be a DataLakeLeaseClient object or the lease ID as a string.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: cpk
    description: 'Encrypts the data on the service-side with the given key.

      Use of customer-provided keys must be done over HTTPS.'
    types:
    - <xref:azure.storage.filedatalake.CustomerProvidedEncryptionKey>
  return:
    description: response header in dict
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
    - <xref:dict>[<xref:str>, <xref:datetime.datetime>]
  examples:
  - "Commit the previous appended data.<!--[!code-python[Main](les\\datalake_samples_file_system.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\\datalake_samples_file_system.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   with open(SOURCE_FILE, \"rb\") as data:\n       file_client = file_system_client.get_file_client(\"\
    myfile\")\n       file_client.create_file()\n       file_client.append_data(data,\
    \ 0)\n       file_client.flush_data(data.tell())\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.from_connection_string
  name: from_connection_string
  summary: Create DataLakeFileClient from a Connection String.
  signature: 'from_connection_string(conn_str: str, file_system_name: str, file_path:
    str, credential: str | Dict[str, str] | AzureNamedKeyCredential | AzureSasCredential
    | TokenCredential | None = None, **kwargs: Any) -> Self'
  parameters:
  - name: conn_str
    description: A connection string to an Azure Storage account.
    isRequired: true
    types:
    - <xref:str>
  - name: file_system_name
    description: The name of file system to interact with.
    isRequired: true
    types:
    - <xref:str>
  - name: file_path
    description: 'The whole file path, so that to interact with a specific file.

      eg. "{directory}/{subdirectory}/{file}"'
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: 'The credentials with which to authenticate. This is optional if
      the

      account URL already has a SAS token, or the connection string already has shared

      access key values. The value can be a SAS token string,

      an instance of a AzureSasCredential or AzureNamedKeyCredential from azure.core.credentials,

      an account shared access key, or an instance of a TokenCredentials class from
      azure.identity.

      Credentials provided here will take precedence over those in the connection
      string.

      If using an instance of AzureNamedKeyCredential, "name" should be the storage
      account name, and "key"

      should be the storage account key.'
    defaultValue: None
    types:
    - <xref:azure.core.credentials.AzureNamedKeyCredential>
    - <xref:azure.core.credentials.AzureSasCredential>
    - <xref:azure.core.credentials.TokenCredential>
    - <xref:str>
    - <xref:dict>[<xref:str>, <xref:str>]
    - <xref:None>
  keywordOnlyParameters:
  - name: audience
    description: 'The audience to use when requesting tokens for Azure Active Directory

      authentication. Only has an effect when credential is of type TokenCredential.
      The value could be

      [https://storage.azure.com/](https://storage.azure.com/) (default) or [https:/](https:/)/<account>.blob.core.windows.net.'
    types:
    - <xref:str>
  return:
    description: A DataLakeFileClient.
    types:
    - <xref:azure.storage.filedatalake.DataLakeFileClient>
- uid: azure.storage.filedatalake.DataLakeFileClient.get_access_control
  name: get_access_control
  signature: 'get_access_control(upn: bool | None = None, **kwargs) -> Dict[str, Any]'
  parameters:
  - name: upn
    description: 'Optional.

      Valid only when Hierarchical Namespace is

      enabled for the account. If "true", the user identity values returned

      in the x-ms-owner, x-ms-group, and x-ms-acl response headers will be

      transformed from Azure Active Directory Object IDs to User Principal

      Names.  If "false", the values will be returned as Azure Active

      Directory Object IDs. The default value is false. Note that group and

      application Object IDs are not translated because they do not have

      unique friendly names.'
    isRequired: true
    types:
    - <xref:bool>
  keywordOnlyParameters:
  - name: lease
    description: 'Required if the file/directory has an active lease. Value can be
      a LeaseClient object

      or the lease ID as a string.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: response dict containing access control options with no modifications.
    types:
    - <xref:dict>[<xref:str>, <xref:typing.Any>]
- uid: azure.storage.filedatalake.DataLakeFileClient.get_file_properties
  name: get_file_properties
  summary: 'Returns all user-defined metadata, standard HTTP properties, and

    system properties for the file. It does not return the content of the file.'
  signature: 'get_file_properties(**kwargs: Any) -> FileProperties'
  keywordOnlyParameters:
  - name: lease
    description: 'Required if the directory or file has an active lease. Value can
      be a DataLakeLeaseClient object

      or the lease ID as a string.'
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: cpk
    description: 'Decrypts the data on the service-side with the given key.

      Use of customer-provided keys must be done over HTTPS.

      Required if the file was created with a customer-provided key.'
    types:
    - <xref:azure.storage.filedatalake.CustomerProvidedEncryptionKey>
  - name: upn
    description: 'If True, the user identity values returned in the x-ms-owner, x-ms-group,

      and x-ms-acl response headers will be transformed from Azure Active Directory
      Object IDs to User

      Principal Names in the owner, group, and acl fields of

      <xref:azure.storage.filedatalake.FileProperties>. If False, the values will
      be returned

      as Azure Active Directory Object IDs. The default value is False. Note that
      group and application

      Object IDs are not translate because they do not have unique friendly names.'
    types:
    - <xref:bool>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: All user-defined metadata, standard HTTP properties, and system properties
      for the file.
    types:
    - <xref:azure.storage.filedatalake.FileProperties>
  examples:
  - "Getting the properties for a file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   properties = file_client.get_file_properties()\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.query_file
  name: query_file
  summary: 'Enables users to select/project on datalake file data by providing simple
    query expressions.

    This operations returns a DataLakeFileQueryReader, users need to use readall()
    or readinto() to get query data.'
  signature: 'query_file(query_expression: str, **kwargs: Any) -> DataLakeFileQueryReader'
  parameters:
  - name: query_expression
    description: 'Required. a query statement.

      eg. Select * from DataLakeStorage'
    isRequired: true
    types:
    - <xref:str>
  keywordOnlyParameters:
  - name: on_error
    description: A function to be called on any processing errors returned by the
      service.
    types:
    - <xref:typing.Callable>[<xref:azure.storage.filedatalake.DataLakeFileQueryError>]
  - name: file_format
    description: 'Optional. Defines the serialization of the data currently stored
      in the file. The default is to

      treat the file data as CSV data formatted in the default dialect. This can be
      overridden with

      a custom DelimitedTextDialect, or DelimitedJsonDialect or "ParquetDialect" (passed
      as a string or enum).

      These dialects can be passed through their respective classes, the QuickQueryDialect
      enum or as a string.'
    types:
    - <xref:azure.storage.filedatalake.DelimitedTextDialect>
    - <xref:azure.storage.filedatalake.DelimitedJsonDialect>
    - <xref:azure.storage.filedatalake.QuickQueryDialect>
    - <xref:str>
  - name: output_format
    description: 'Optional. Defines the output serialization for the data stream.
      By default the data will be returned

      as it is represented in the file. By providing an output format,

      the file data will be reformatted according to that profile.

      This value can be a DelimitedTextDialect or a DelimitedJsonDialect or ArrowDialect.

      These dialects can be passed through their respective classes, the QuickQueryDialect
      enum or as a string.'
    types:
    - <xref:azure.storage.filedatalake.DelimitedTextDialect>
    - <xref:azure.storage.filedatalake.DelimitedJsonDialect>
    - <xref:list>[<xref:azure.storage.filedatalake.ArrowDialect>]
    - <xref:azure.storage.filedatalake.QuickQueryDialect>
    - <xref:str>
  - name: lease
    description: 'Required if the file has an active lease. Value can be a DataLakeLeaseClient
      object

      or the lease ID as a string.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: cpk
    description: 'Decrypts the data on the service-side with the given key.

      Use of customer-provided keys must be done over HTTPS.

      Required if the file was created with a Customer-Provided Key.'
    types:
    - <xref:azure.storage.filedatalake.CustomerProvidedEncryptionKey>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: A streaming object (DataLakeFileQueryReader)
    types:
    - <xref:azure.storage.filedatalake.DataLakeFileQueryReader>
  examples:
  - "select/project on datalake file data by providing simple query expressions.<!--[!code-python[Main](les\\\
    datalake_samples_query.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
    : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\\
    hostedtoolcache\\\\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\
    \\py2docfx\\\\dist_temp\\\\215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\
    \\datalake_samples_query.py\", \"xml:space\": \"preserve\", \"force\": false,\
    \ \"language\": \"python\", \"highlight_args\": {\"linenostart\": 1}, \"linenos\"\
    : false} -->\n\n````python\n\n   errors = []\n   def on_error(error):\n      \
    \ errors.append(error)\n\n   # upload the csv file\n   file_client = datalake_service_client.get_file_client(filesystem_name,\
    \ \"csvfile\")\n   file_client.upload_data(CSV_DATA, overwrite=True)\n\n   # select\
    \ the second column of the csv file\n   query_expression = \"SELECT _2 from DataLakeStorage\"\
    \n   input_format = DelimitedTextDialect(delimiter=',', quotechar='\"', lineterminator='\\\
    n', escapechar=\"\", has_header=False)\n   output_format = DelimitedJsonDialect(delimiter='\\\
    n')\n   reader = file_client.query_file(query_expression, on_error=on_error, file_format=input_format,\
    \ output_format=output_format)\n   content = reader.readall()\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.remove_access_control_recursive
  name: remove_access_control_recursive
  summary: Removes the Access Control on a path and sub-paths.
  signature: 'remove_access_control_recursive(acl: str, **kwargs: Any) -> AccessControlChangeResult'
  parameters:
  - name: acl
    description: 'Removes POSIX access control rights on files and directories.

      The value is a comma-separated list of access control entries. Each

      access control entry (ACE) consists of a scope, a type, and a user or

      group identifier in the format "[scope:][type]:[id]".'
    isRequired: true
    types:
    - <xref:str>
  keywordOnlyParameters:
  - name: progress_hook
    description: 'Callback where the caller can track progress of the operation

      as well as collect paths that failed to change Access Control.'
    types:
    - <xref:func>(<xref:azure.storage.filedatalake.AccessControlChanges>)
  - name: continuation_token
    description: Optional continuation token that can be used to resume previously
      stopped operation.
    types:
    - <xref:str>
  - name: batch_size
    description: 'Optional. If data set size exceeds batch size then operation will
      be split into multiple

      requests so that progress can be tracked. Batch size should be between 1 and
      2000.

      The default when unspecified is 2000.'
    types:
    - <xref:int>
  - name: max_batches
    description: 'Optional. Defines maximum number of batches that single change Access
      Control operation can execute.

      If maximum is reached before all sub-paths are processed then,

      continuation token can be used to resume operation.

      Empty value indicates that maximum number of batches in unbound and operation
      continues till end.'
    types:
    - <xref:int>
  - name: continue_on_failure
    description: 'If set to False, the operation will terminate quickly on encountering
      user errors (4XX).

      If True, the operation will ignore user errors and proceed with the operation
      on other sub-entities of

      the directory.

      Continuation token will only be returned when continue_on_failure is True in
      case of user errors.

      If not set the default value is False for this.'
    types:
    - <xref:bool>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: 'A summary of the recursive operations, including the count of successes
      and failures,

      as well as a continuation token in case the operation was terminated prematurely.'
    types:
    - <xref:azure.storage.filedatalake.AccessControlChangeResult>
  exceptions:
  - type: azure.core.exceptions.AzureError
    description: User can restart the operation using continuation_token field of
      AzureError if the token is available.
- uid: azure.storage.filedatalake.DataLakeFileClient.rename_file
  name: rename_file
  summary: Rename the source file.
  signature: 'rename_file(new_name: str, **kwargs: Any) -> DataLakeFileClient'
  parameters:
  - name: new_name
    description: 'the new file name the user want to rename to.

      The value must have the following format: "{filesystem}/{directory}/{subdirectory}/{file}".'
    isRequired: true
    types:
    - <xref:str>
  keywordOnlyParameters:
  - name: content_settings
    description: ContentSettings object used to set path properties.
    types:
    - <xref:azure.storage.filedatalake.ContentSettings>
  - name: source_lease
    description: 'A lease ID for the source path. If specified,

      the source path must have an active lease and the lease ID must

      match.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: lease
    description: 'Required if the file/directory has an active lease. Value can be
      a LeaseClient object

      or the lease ID as a string.'
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: source_if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: source_if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: source_etag
    description: 'The source ETag value, or the wildcard character (*). Used to check
      if the resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: source_match_condition
    description: The source match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: the renamed file client
    types:
    - <xref:azure.storage.filedatalake.DataLakeFileClient>
  examples:
  - "Rename the source file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    215\\\\azure-storage-file-datalake-12.16.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   new_client = file_client.rename_file(file_client.file_system_name + '/' +\
    \ 'newname')\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.set_access_control
  name: set_access_control
  summary: Set the owner, group, permissions, or access control list for a path.
  signature: 'set_access_control(owner: str | None = None, group: str | None = None,
    permissions: str | None = None, acl: str | None = None, **kwargs) -> Dict[str,
    str | datetime]'
  parameters:
  - name: owner
    description: Optional. The owner of the file or directory.
    isRequired: true
    types:
    - <xref:str>
  - name: group
    description: Optional. The owning group of the file or directory.
    isRequired: true
    types:
    - <xref:str>
  - name: permissions
    description: 'Optional and only valid if Hierarchical Namespace

      is enabled for the account. Sets POSIX access permissions for the file

      owner, the file owning group, and others. Each class may be granted

      read, write, or execute permission.  The sticky bit is also supported.

      Both symbolic (rwxrw-rw-) and 4-digit octal notation (e.g. 0766) are

      supported.

      permissions and acl are mutually exclusive.'
    isRequired: true
    types:
    - <xref:str>
  - name: acl
    description: 'Sets POSIX access control rights on files and directories.

      The value is a comma-separated list of access control entries. Each

      access control entry (ACE) consists of a scope, a type, a user or

      group identifier, and permissions in the format

      "[scope:][type]:[id]:[permissions]".

      permissions and acl are mutually exclusive.'
    isRequired: true
    types:
    - <xref:str>
  keywordOnlyParameters:
  - name: lease
    description: 'Required if the file/directory has an active lease. Value can be
      a LeaseClient object

      or the lease ID as a string.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: response dict containing access control options (Etag and last modified).
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
    - <xref:dict>[<xref:str>, <xref:datetime.datetime>]
- uid: azure.storage.filedatalake.DataLakeFileClient.set_access_control_recursive
  name: set_access_control_recursive
  summary: Sets the Access Control on a path and sub-paths.
  signature: 'set_access_control_recursive(acl: str, **kwargs: Any) -> AccessControlChangeResult'
  parameters:
  - name: acl
    description: 'Sets POSIX access control rights on files and directories.

      The value is a comma-separated list of access control entries. Each

      access control entry (ACE) consists of a scope, a type, a user or

      group identifier, and permissions in the format

      "[scope:][type]:[id]:[permissions]".'
    isRequired: true
    types:
    - <xref:str>
  keywordOnlyParameters:
  - name: progress_hook
    description: 'Callback where the caller can track progress of the operation

      as well as collect paths that failed to change Access Control.'
    types:
    - <xref:func>(<xref:azure.storage.filedatalake.AccessControlChanges>)
  - name: continuation_token
    description: Optional continuation token that can be used to resume previously
      stopped operation.
    types:
    - <xref:str>
  - name: batch_size
    description: 'Optional. If data set size exceeds batch size then operation will
      be split into multiple

      requests so that progress can be tracked. Batch size should be between 1 and
      2000.

      The default when unspecified is 2000.'
    types:
    - <xref:int>
  - name: max_batches
    description: 'Optional. Defines maximum number of batches that single change Access
      Control operation can execute.

      If maximum is reached before all sub-paths are processed,

      then continuation token can be used to resume operation.

      Empty value indicates that maximum number of batches in unbound and operation
      continues till end.'
    types:
    - <xref:int>
  - name: continue_on_failure
    description: 'If set to False, the operation will terminate quickly on encountering
      user errors (4XX).

      If True, the operation will ignore user errors and proceed with the operation
      on other sub-entities of

      the directory.

      Continuation token will only be returned when continue_on_failure is True in
      case of user errors.

      If not set the default value is False for this.'
    types:
    - <xref:bool>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: 'A summary of the recursive operations, including the count of successes
      and failures,

      as well as a continuation token in case the operation was terminated prematurely.'
    types:
    - <xref:azure.storage.filedatalake.AccessControlChangeResult>
  exceptions:
  - type: azure.core.exceptions.AzureError
    description: User can restart the operation using continuation_token field of
      AzureError if the token is available.
- uid: azure.storage.filedatalake.DataLakeFileClient.set_file_expiry
  name: set_file_expiry
  summary: Sets the time a file will expire and be deleted.
  signature: 'set_file_expiry(expiry_options: str, expires_on: datetime | int | None
    = None, **kwargs) -> None'
  parameters:
  - name: expiry_options
    description: 'Required. Indicates mode of the expiry time.

      Possible values include: ''NeverExpire'', ''RelativeToCreation'', ''RelativeToNow'',
      ''Absolute'''
    isRequired: true
    types:
    - <xref:str>
  - name: expires_on
    description: 'The time to set the file to expiry.

      When expiry_options is RelativeTo*, expires_on should be an int in milliseconds.

      If the type of expires_on is datetime, it should be in UTC time.'
    isRequired: true
    types:
    - <xref:datetime>
    - <xref:int>
  keywordOnlyParameters:
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    types:
    - <xref:None>
- uid: azure.storage.filedatalake.DataLakeFileClient.set_http_headers
  name: set_http_headers
  summary: 'Sets system properties on the file or directory.


    If one property is set for the content_settings, all properties will be overridden.'
  signature: 'set_http_headers(content_settings: ContentSettings | None = None, **kwargs)
    -> Dict[str, Any]'
  parameters:
  - name: content_settings
    description: ContentSettings object used to set file/directory properties.
    isRequired: true
    types:
    - <xref:azure.storage.filedatalake.ContentSettings>
  keywordOnlyParameters:
  - name: lease
    description: 'If specified, set_file_system_metadata only succeeds if the

      file system''s lease is active and matches this ID.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: file/directory-updated property dict (Etag and last modified)
    types:
    - <xref:dict>[<xref:str>, <xref:typing.Any>]
- uid: azure.storage.filedatalake.DataLakeFileClient.set_metadata
  name: set_metadata
  summary: 'Sets one or more user-defined name-value pairs for the specified

    file system. Each call to this operation replaces all existing metadata

    attached to the file system. To remove all metadata from the file system,

    call this operation with no metadata dict.'
  signature: 'set_metadata(metadata: Dict[str, str], **kwargs) -> Dict[str, str |
    datetime]'
  parameters:
  - name: metadata
    description: 'A dict containing name-value pairs to associate with the file system
      as

      metadata. Example: {''category'':''test''}'
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  keywordOnlyParameters:
  - name: lease
    description: 'If specified, set_file_system_metadata only succeeds if the

      file system''s lease is active and matches this ID.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: cpk
    description: 'Encrypts the data on the service-side with the given key.

      Use of customer-provided keys must be done over HTTPS.'
    types:
    - <xref:azure.storage.filedatalake.CustomerProvidedEncryptionKey>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: file system-updated property dict (Etag and last modified).
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
    - <xref:dict>[<xref:str>, <xref:datetime.datetime>]
- uid: azure.storage.filedatalake.DataLakeFileClient.update_access_control_recursive
  name: update_access_control_recursive
  summary: Modifies the Access Control on a path and sub-paths.
  signature: 'update_access_control_recursive(acl: str, **kwargs: Any) -> AccessControlChangeResult'
  parameters:
  - name: acl
    description: 'Modifies POSIX access control rights on files and directories.

      The value is a comma-separated list of access control entries. Each

      access control entry (ACE) consists of a scope, a type, a user or

      group identifier, and permissions in the format

      "[scope:][type]:[id]:[permissions]".'
    isRequired: true
    types:
    - <xref:str>
  keywordOnlyParameters:
  - name: progress_hook
    description: 'Callback where the caller can track progress of the operation

      as well as collect paths that failed to change Access Control.'
    types:
    - <xref:func>(<xref:azure.storage.filedatalake.AccessControlChanges>)
  - name: continuation_token
    description: Optional continuation token that can be used to resume previously
      stopped operation.
    types:
    - <xref:str>
  - name: batch_size
    description: 'Optional. If data set size exceeds batch size then operation will
      be split into multiple

      requests so that progress can be tracked. Batch size should be between 1 and
      2000.

      The default when unspecified is 2000.'
    types:
    - <xref:int>
  - name: max_batches
    description: 'Optional. Defines maximum number of batches that single change Access
      Control operation can execute.

      If maximum is reached before all sub-paths are processed,

      then continuation token can be used to resume operation.

      Empty value indicates that maximum number of batches in unbound and operation
      continues till end.'
    types:
    - <xref:int>
  - name: continue_on_failure
    description: 'If set to False, the operation will terminate quickly on encountering
      user errors (4XX).

      If True, the operation will ignore user errors and proceed with the operation
      on other sub-entities of

      the directory.

      Continuation token will only be returned when continue_on_failure is True in
      case of user errors.

      If not set the default value is False for this.'
    types:
    - <xref:bool>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).'
    types:
    - <xref:int>
  return:
    description: 'A summary of the recursive operations, including the count of successes
      and failures,

      as well as a continuation token in case the operation was terminated prematurely.'
    types:
    - <xref:azure.storage.filedatalake.AccessControlChangeResult>
  exceptions:
  - type: azure.core.exceptions.AzureError
    description: User can restart the operation using continuation_token field of
      AzureError if the token is available.
- uid: azure.storage.filedatalake.DataLakeFileClient.upload_data
  name: upload_data
  summary: Upload data to a file.
  signature: 'upload_data(data: bytes | str | Iterable | IO, length: int | None =
    None, overwrite: bool | None = False, **kwargs) -> Dict[str, Any]'
  parameters:
  - name: data
    description: Content to be uploaded to file
    isRequired: true
    types:
    - <xref:bytes>, <xref:str>, <xref:typing.Iterable>[<xref:typing.AnyStr>]
    - <xref:typing.IO>[<xref:typing.AnyStr>]
  - name: length
    description: Size of the data in bytes.
    isRequired: true
    types:
    - <xref:int>
  - name: overwrite
    description: to overwrite an existing file or not.
    isRequired: true
    types:
    - <xref:bool>
  keywordOnlyParameters:
  - name: content_settings
    description: ContentSettings object used to set path properties.
    types:
    - <xref:azure.storage.filedatalake.ContentSettings>
  - name: metadata
    description: Name-value pairs associated with the blob as metadata.
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>[<xref:str>, <xref:str>]]
  - name: lease
    description: 'Required if the blob has an active lease. Value can be a DataLakeLeaseClient
      object

      or the lease ID as a string.'
    types:
    - <xref:azure.storage.filedatalake.DataLakeLeaseClient>
    - <xref:str>
  - name: umask
    description: 'Optional and only valid if Hierarchical Namespace is enabled for
      the account.

      When creating a file or directory and the parent folder does not have a default
      ACL,

      the umask restricts the permissions of the file or directory to be created.

      The resulting permission is given by p & ^u, where p is the permission and u
      is the umask.

      For example, if p is 0777 and u is 0057, then the resulting permission is 0720.

      The default permission is 0777 for a directory and 0666 for a file. The default
      umask is 0027.

      The umask must be specified in 4-digit octal notation (e.g. 0766).'
    types:
    - <xref:str>
  - name: permissions
    description: 'Optional and only valid if Hierarchical Namespace

      is enabled for the account. Sets POSIX access permissions for the file

      owner, the file owning group, and others. Each class may be granted

      read, write, or execute permission.  The sticky bit is also supported.

      Both symbolic (rwxrw-rw-) and 4-digit octal notation (e.g. 0766) are

      supported.'
    types:
    - <xref:str>
  - name: if_modified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only

      if the resource has been modified since the specified time.'
    types:
    - <xref:datetime.datetime>
  - name: if_unmodified_since
    description: 'A DateTime value. Azure expects the date value passed in to be UTC.

      If timezone is included, any non-UTC datetimes will be converted to UTC.

      If a date is passed in without timezone info, it is assumed to be UTC.

      Specify this header to perform the operation only if

      the resource has not been modified since the specified date/time.'
    types:
    - <xref:datetime.datetime>
  - name: validate_content
    description: 'If true, calculates an MD5 hash for each chunk of the file. The
      storage

      service checks the hash of the content that has arrived with the hash

      that was sent. This is primarily valuable for detecting bitflips on

      the wire if using http instead of https, as https (the default), will

      already validate. Note that this MD5 hash is not stored with the

      blob. Also note that if enabled, the memory-efficient upload algorithm

      will not be used because computing the MD5 hash requires buffering

      entire blocks, and doing so defeats the purpose of the memory-efficient algorithm.'
    types:
    - <xref:bool>
  - name: etag
    description: 'An ETag value, or the wildcard character (*). Used to check if the
      resource has changed,

      and act according to the condition specified by the *match_condition* parameter.'
    types:
    - <xref:str>
  - name: match_condition
    description: The match condition to use upon the etag.
    types:
    - <xref:azure.core.MatchConditions>
  - name: cpk
    description: 'Encrypts the data on the service-side with the given key.

      Use of customer-provided keys must be done over HTTPS.'
    types:
    - <xref:azure.storage.filedatalake.CustomerProvidedEncryptionKey>
  - name: timeout
    description: 'Sets the server-side timeout for the operation in seconds. For more
      details see

      [https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations](https://learn.microsoft.com/rest/api/storageservices/setting-timeouts-for-blob-service-operations).

      This value is not tracked or validated on the client. To configure client-side
      network timesouts

      see [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-file-datalake#other-client--per-operation-configuration).
      This method may make multiple calls to the service and

      the timeout will apply to each call individually.'
    types:
    - <xref:int>
  - name: chunk_size
    description: 'The maximum chunk size for uploading a file in chunks.

      Defaults to 100*1024*1024, or 100MB.'
    types:
    - <xref:int>
  - name: encryption_context
    description: Specifies the encryption context to set on the file.
    types:
    - <xref:str>
  return:
    description: response dict (Etag and last modified).
    types:
    - <xref:dict>[<xref:str>, <xref:typing.Any>]
attributes:
- uid: azure.storage.filedatalake.DataLakeFileClient.api_version
  name: api_version
  summary: The version of the Storage API used for requests.
  return:
    types:
    - <xref:str>
- uid: azure.storage.filedatalake.DataLakeFileClient.location_mode
  name: location_mode
  summary: 'The location mode that the client is currently using.


    By default this will be "primary". Options include "primary" and "secondary".'
  return:
    types:
    - <xref:str>
- uid: azure.storage.filedatalake.DataLakeFileClient.primary_endpoint
  name: primary_endpoint
  summary: The full primary endpoint URL.
  return:
    types:
    - <xref:str>
- uid: azure.storage.filedatalake.DataLakeFileClient.primary_hostname
  name: primary_hostname
  summary: The hostname of the primary endpoint.
  return:
    types:
    - <xref:str>
- uid: azure.storage.filedatalake.DataLakeFileClient.secondary_endpoint
  name: secondary_endpoint
  summary: 'The full secondary endpoint URL if configured.


    If not available a ValueError will be raised. To explicitly specify a secondary
    hostname, use the optional

    *secondary_hostname* keyword argument on instantiation.'
  return:
    types:
    - <xref:str>
  exceptions:
  - type: ValueError
- uid: azure.storage.filedatalake.DataLakeFileClient.secondary_hostname
  name: secondary_hostname
  summary: 'The hostname of the secondary endpoint.


    If not available this will be None. To explicitly specify a secondary hostname,
    use the optional

    *secondary_hostname* keyword argument on instantiation.'
  return:
    types:
    - <xref:typing.Optional>[<xref:str>]
- uid: azure.storage.filedatalake.DataLakeFileClient.url
  name: url
  summary: 'The full endpoint URL to this entity, including SAS token if used.


    This could be either the primary endpoint,

    or the secondary endpoint depending on the current <xref:azure.storage.filedatalake.DataLakeFileClient.location_mode>.

    :returns: The full endpoint URL to this entity, including SAS token if used.

    :rtype: str'
