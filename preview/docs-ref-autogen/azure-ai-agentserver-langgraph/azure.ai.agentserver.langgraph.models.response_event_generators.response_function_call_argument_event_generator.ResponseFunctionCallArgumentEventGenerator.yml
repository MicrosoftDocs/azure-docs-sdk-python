### YamlMime:PythonClass
uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator
name: ResponseFunctionCallArgumentEventGenerator
fullName: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator
module: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator
constructor:
  syntax: 'ResponseFunctionCallArgumentEventGenerator(logger, parent: ResponseEventGenerator,
    item_id, message_id, output_index: int, *, hitl_helper: HumanInTheLoopHelper =
    None)'
  parameters:
  - name: logger
    isRequired: true
  - name: parent
    isRequired: true
  - name: item_id
    isRequired: true
  - name: message_id
    isRequired: true
  - name: output_index
    isRequired: true
  keywordOnlyParameters:
  - name: hitl_helper
    defaultValue: None
methods:
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.aggregate_content
  name: aggregate_content
  summary: 'Aggregate the content for this layer.

    It is called by its child processor to pass up aggregated content.'
  signature: aggregate_content()
  return:
    description: content from child processor
    types:
    - <xref:str> | <xref:dict>
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.get_tool_call_info
  name: get_tool_call_info
  signature: 'get_tool_call_info(message: AIMessage | HumanMessage | ChatMessage |
    SystemMessage | FunctionMessage | ToolMessage | AIMessageChunk | HumanMessageChunk
    | ChatMessageChunk | SystemMessageChunk | FunctionMessageChunk | ToolMessageChunk
    | Interrupt)'
  parameters:
  - name: message
    isRequired: true
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.has_finish_reason
  name: has_finish_reason
  signature: 'has_finish_reason(message: AIMessage | HumanMessage | ChatMessage |
    SystemMessage | FunctionMessage | ToolMessage | AIMessageChunk | HumanMessageChunk
    | ChatMessageChunk | SystemMessageChunk | FunctionMessageChunk | ToolMessageChunk)
    -> bool'
  parameters:
  - name: message
    isRequired: true
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.on_end
  name: on_end
  signature: 'on_end(message: AIMessage | HumanMessage | ChatMessage | SystemMessage
    | FunctionMessage | ToolMessage | AIMessageChunk | HumanMessageChunk | ChatMessageChunk
    | SystemMessageChunk | FunctionMessageChunk | ToolMessageChunk, context: LanggraphRunContext,
    stream_state: StreamEventState) -> tuple[bool, List[azure.ai.agentserver.core.models.projects._models.ResponseStreamEvent]]'
  parameters:
  - name: message
    isRequired: true
  - name: context
    isRequired: true
  - name: stream_state
    isRequired: true
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.on_start
  name: on_start
  signature: 'on_start(event: AIMessage | HumanMessage | ChatMessage | SystemMessage
    | FunctionMessage | ToolMessage | AIMessageChunk | HumanMessageChunk | ChatMessageChunk
    | SystemMessageChunk | FunctionMessageChunk | ToolMessageChunk, run_details, stream_state:
    StreamEventState) -> tuple[bool, List[azure.ai.agentserver.core.models.projects._models.ResponseStreamEvent]]'
  parameters:
  - name: event
    isRequired: true
  - name: run_details
    isRequired: true
  - name: stream_state
    isRequired: true
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.process
  name: process
  signature: 'process(message: AIMessage | HumanMessage | ChatMessage | SystemMessage
    | FunctionMessage | ToolMessage | AIMessageChunk | HumanMessageChunk | ChatMessageChunk
    | SystemMessageChunk | FunctionMessageChunk | ToolMessageChunk | Interrupt, run_details,
    stream_state: StreamEventState) -> tuple[bool, azure.ai.agentserver.langgraph.models.response_event_generators.response_event_generator.ResponseEventGenerator,
    List[azure.ai.agentserver.core.models.projects._models.ResponseStreamEvent]]'
  parameters:
  - name: message
    isRequired: true
  - name: run_details
    isRequired: true
  - name: stream_state
    isRequired: true
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.should_end
  name: should_end
  signature: 'should_end(event: AIMessage | HumanMessage | ChatMessage | SystemMessage
    | FunctionMessage | ToolMessage | AIMessageChunk | HumanMessageChunk | ChatMessageChunk
    | SystemMessageChunk | FunctionMessageChunk | ToolMessageChunk) -> bool'
  parameters:
  - name: event
    isRequired: true
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.try_process_message
  name: try_process_message
  signature: 'try_process_message(message, context: LanggraphRunContext, stream_state:
    StreamEventState) -> tuple[bool, azure.ai.agentserver.langgraph.models.response_event_generators.response_event_generator.ResponseEventGenerator,
    List[azure.ai.agentserver.core.models.projects._models.ResponseStreamEvent]]'
  parameters:
  - name: message
    isRequired: true
  - name: context
    isRequired: true
  - name: stream_state
    isRequired: true
attributes:
- uid: azure.ai.agentserver.langgraph.models.response_event_generators.response_function_call_argument_event_generator.ResponseFunctionCallArgumentEventGenerator.started
  name: started
  signature: 'started: bool = False'
