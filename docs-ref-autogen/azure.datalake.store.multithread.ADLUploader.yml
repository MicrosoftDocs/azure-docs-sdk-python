### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.datalake.store.multithread.ADLUploader.active
  - azure.datalake.store.multithread.ADLUploader.clear_saved
  - azure.datalake.store.multithread.ADLUploader.load
  - azure.datalake.store.multithread.ADLUploader.run
  - azure.datalake.store.multithread.ADLUploader.save
  - azure.datalake.store.multithread.ADLUploader.successful
  class: azure.datalake.store.multithread.ADLUploader
  fullName: azure.datalake.store.multithread.ADLUploader
  inheritance:
  - type: builtins.object
  langs:
  - python
  module: azure.datalake.store.multithread
  name: ADLUploader
  seealsoContent: "See also: @azure.datalake.store.transfer.ADLTransferClient \n"
  source:
    id: ADLUploader
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 316
  summary: 'Upload local file(s) using chunks and threads


    Launches multiple threads for efficient uploading, with *chunksize*

    assigned to each. The path can be a single file, a directory

    of files or a glob pattern.








    '
  syntax:
    content: ADLUploader(adlfs, rpath, lpath, nthreads=None, chunksize=268435456,
      buffersize=4194304, blocksize=4194304, client=None, run=True, overwrite=False,
      verbose=False, progress_callback=None)
    parameters:
    - description: ''
      id: adlfs
      type:
      - ADL filesystem instance
    - description: 'remote path to upload to; if multiple files, this is the dircetory

        root to write within

        '
      id: rpath
      type:
      - str
    - description: 'local path. Can be single file, directory (in which case, upload

        recursively) or glob pattern. Recursive glob patterns using **** are

        not supported.

        '
      id: lpath
      type:
      - str
    - description: 'Number of threads to use. If None, uses the number of cores.

        '
      id: nthreads
      type:
      - int [None]
    - description: 'Number of bytes for a chunk. Large files are split into chunks.
        Files

        smaller than this number will always be transferred in a single thread.

        '
      id: chunksize
      type:
      - int [2**28]
    - description: 'Number of bytes for internal buffer. This block cannot be bigger
        than

        a chunk and cannot be smaller than a block.

        '
      id: buffersize
      type:
      - int [2**22]
    - description: 'Number of bytes for a block. Within each chunk, we write a smaller

        block for each API call. This block cannot be bigger than a chunk.

        '
      id: blocksize
      type:
      - int [2**22]
    - description: 'Set an instance of ADLTransferClient when finer-grained control
        over

        transfer parameters is needed. Ignores *nthreads* and *chunksize*

        set by constructor.

        '
      id: client
      type:
      - ADLTransferClient [None]
    - description: 'Whether to begin executing immediately.

        '
      id: run
      type:
      - bool [True]
    - description: 'Whether to forcibly overwrite existing files/directories. If False
        and

        remote path is a directory, will quit regardless if any files would be

        overwritten or not. If True, only matching filenames are actually

        overwritten.

        '
      id: overwrite
      type:
      - bool [False]
    - description: 'Callback for progress with signature function(current, total)
        where

        current is the number of bytes transfered so far, and total is the

        size of the blob, or None if the total size is unknown.

        '
      id: progress_callback
      type:
      - callable [None]
  type: class
  uid: azure.datalake.store.multithread.ADLUploader
- class: azure.datalake.store.multithread.ADLUploader
  fullName: azure.datalake.store.multithread.ADLUploader.active
  langs:
  - python
  module: azure.datalake.store.multithread
  name: active
  source:
    id: active
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 494
  summary: 'Return whether the uploader is active

    '
  syntax:
    content: active()
    parameters: []
  type: method
  uid: azure.datalake.store.multithread.ADLUploader.active
- class: azure.datalake.store.multithread.ADLUploader
  fullName: azure.datalake.store.multithread.ADLUploader.clear_saved
  langs:
  - python
  module: azure.datalake.store.multithread
  name: clear_saved
  source:
    id: clear_saved
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 430
  summary: 'Remove references to all persisted uploads.

    '
  syntax:
    content: clear_saved()
  type: method
  uid: azure.datalake.store.multithread.ADLUploader.clear_saved
- class: azure.datalake.store.multithread.ADLUploader
  fullName: azure.datalake.store.multithread.ADLUploader.load
  langs:
  - python
  module: azure.datalake.store.multithread
  name: load
  source:
    id: load
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 417
  summary: 'Load list of persisted transfers from disk, for possible resumption.

    '
  syntax:
    content: load()
    return:
      description: "* *A dictionary of upload instances. The hashes are auto-* \n\n\
        * *generated unique. The state of the chunks completed, errored, etc.,* \n\
        \n* *can be seen in the status attribute. Instances can be resumed with* \n\
        \n* `run()`. \n"
  type: method
  uid: azure.datalake.store.multithread.ADLUploader.load
- class: azure.datalake.store.multithread.ADLUploader
  fullName: azure.datalake.store.multithread.ADLUploader.run
  langs:
  - python
  module: azure.datalake.store.multithread
  name: run
  source:
    id: run
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 482
  summary: 'Populate transfer queue and execute downloads

    '
  syntax:
    content: run(nthreads=None, monitor=True)
    parameters:
    - defaultValue: None
      description: 'Override default nthreads, if given

        '
      id: nthreads
      type:
      - int [None]
    - defaultValue: 'True'
      description: 'To watch and wait (block) until completion.

        '
      id: monitor
      type:
      - bool [True]
  type: method
  uid: azure.datalake.store.multithread.ADLUploader.run
- class: azure.datalake.store.multithread.ADLUploader
  fullName: azure.datalake.store.multithread.ADLUploader.save
  langs:
  - python
  module: azure.datalake.store.multithread
  name: save
  source:
    id: save
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 396
  summary: 'Persist this upload


    Saves a copy of this transfer process in its current state to disk.

    This is done automatically for a running transfer, so that as a chunk

    is completed, this is reflected. Thus, if a transfer is interrupted,

    e.g., by user action, the transfer can be restarted at another time.

    All chunks that were not already completed will be restarted at that

    time.


    See methods `load` to retrieved saved transfers and `run` to

    resume a stopped transfer.

    '
  syntax:
    content: save(keep=True)
    parameters:
    - defaultValue: 'True'
      description: 'If True, transfer will be saved if some chunks remain to be

        completed; the transfer will be sure to be removed otherwise.

        '
      id: keep
      type:
      - bool (True)
  type: method
  uid: azure.datalake.store.multithread.ADLUploader.save
- class: azure.datalake.store.multithread.ADLUploader
  fullName: azure.datalake.store.multithread.ADLUploader.successful
  langs:
  - python
  module: azure.datalake.store.multithread
  name: successful
  source:
    id: successful
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 498
  summary: 'Return whether the uploader completed successfully.


    It will raise AssertionError if the uploader is active.

    '
  syntax:
    content: successful()
    parameters: []
  type: method
  uid: azure.datalake.store.multithread.ADLUploader.successful
references:
- fullName: azure.datalake.store.multithread.ADLUploader.active
  isExternal: false
  name: active
  parent: azure.datalake.store.multithread.ADLUploader
  uid: azure.datalake.store.multithread.ADLUploader.active
- fullName: azure.datalake.store.multithread.ADLUploader.clear_saved
  isExternal: false
  name: clear_saved
  parent: azure.datalake.store.multithread.ADLUploader
  uid: azure.datalake.store.multithread.ADLUploader.clear_saved
- fullName: azure.datalake.store.multithread.ADLUploader.load
  isExternal: false
  name: load
  parent: azure.datalake.store.multithread.ADLUploader
  uid: azure.datalake.store.multithread.ADLUploader.load
- fullName: azure.datalake.store.multithread.ADLUploader.run
  isExternal: false
  name: run
  parent: azure.datalake.store.multithread.ADLUploader
  uid: azure.datalake.store.multithread.ADLUploader.run
- fullName: azure.datalake.store.multithread.ADLUploader.save
  isExternal: false
  name: save
  parent: azure.datalake.store.multithread.ADLUploader
  uid: azure.datalake.store.multithread.ADLUploader.save
- fullName: azure.datalake.store.multithread.ADLUploader.successful
  isExternal: false
  name: successful
  parent: azure.datalake.store.multithread.ADLUploader
  uid: azure.datalake.store.multithread.ADLUploader.successful
- fullName: int [None]
  name: int [None]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: int [None]
- fullName: int [2**28]
  name: int [2**28]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**28
    name: 2**28
    uid: 2**28
  - fullName: ']'
    name: ']'
  uid: int [2**28]
- fullName: int [2**22]
  name: int [2**22]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**22
    name: 2**22
    uid: 2**22
  - fullName: ']'
    name: ']'
  uid: int [2**22]
- fullName: int [2**22]
  name: int [2**22]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**22
    name: 2**22
    uid: 2**22
  - fullName: ']'
    name: ']'
  uid: int [2**22]
- fullName: ADLTransferClient [None]
  name: ADLTransferClient [None]
  spec.python:
  - fullName: 'ADLTransferClient '
    name: 'ADLTransferClient '
    uid: 'ADLTransferClient '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: ADLTransferClient [None]
- fullName: bool [True]
  name: bool [True]
  spec.python:
  - fullName: 'bool '
    name: 'bool '
    uid: 'bool '
  - fullName: '['
    name: '['
  - fullName: 'True'
    name: 'True'
    uid: 'True'
  - fullName: ']'
    name: ']'
  uid: bool [True]
- fullName: bool [False]
  name: bool [False]
  spec.python:
  - fullName: 'bool '
    name: 'bool '
    uid: 'bool '
  - fullName: '['
    name: '['
  - fullName: 'False'
    name: 'False'
    uid: 'False'
  - fullName: ']'
    name: ']'
  uid: bool [False]
- fullName: callable [None]
  name: callable [None]
  spec.python:
  - fullName: 'callable '
    name: 'callable '
    uid: 'callable '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: callable [None]
- fullName: bool (True)
  name: bool (True)
  spec.python:
  - fullName: 'bool '
    name: 'bool '
    uid: 'bool '
  - fullName: (
    name: (
  - fullName: 'True'
    name: 'True'
    uid: 'True'
  - fullName: )
    name: )
  uid: bool (True)
