### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.datalake.store.utils.CountUpDownLatch
  - azure.datalake.store.utils.clamp
  - azure.datalake.store.utils.commonprefix
  - azure.datalake.store.utils.ensure_writable
  - azure.datalake.store.utils.read_block
  - azure.datalake.store.utils.tokenize
  - azure.datalake.store.utils.write_stdout
  fullName: azure.datalake.store.utils
  langs:
  - python
  module: azure.datalake.store.utils
  name: utils
  source:
    id: utils
    path: azure\datalake\store\utils.py
    remote:
      branch: master
      path: azure\datalake\store\utils.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 0
  type: module
  uid: azure.datalake.store.utils
- example:
  - '

    ```


    >>> clamp(0, 1, 100)

    1

    ```



    ```


    >>> clamp(42, 2, 128)

    42

    ```



    ```


    >>> clamp(1024, 1, 32)

    32

    ```

    '
  fullName: azure.datalake.store.utils.clamp
  langs:
  - python
  module: azure.datalake.store.utils
  name: clamp(n, smallest, largest)
  source:
    id: clamp
    path: azure\datalake\store\utils.py
    remote:
      branch: master
      path: azure\datalake\store\utils.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 144
  summary: 'Limit a value to a given range


    This is equivalent to smallest <= n <= largest.

    '
  syntax:
    content: clamp(n, smallest, largest)
    parameters:
    - id: n
    - id: smallest
    - id: largest
  type: function
  uid: azure.datalake.store.utils.clamp
- example:
  - '

    ```


    >>> # os.path.commonprefix returns ''/disk1/foo''

    >>> commonprefix([''/disk1/foobar'', ''/disk1/foobaz''])

    ''/disk1''

    ```



    ```


    >>> commonprefix([''a/b/c'', ''a/b/d'', ''a/c/d''])

    ''a''

    ```



    ```


    >>> commonprefix([''a/b/c'', ''d/e/f'', ''g/h/i''])

    ''''

    ```

    '
  fullName: azure.datalake.store.utils.commonprefix
  langs:
  - python
  module: azure.datalake.store.utils
  name: commonprefix(paths)
  source:
    id: commonprefix
    path: azure\datalake\store\utils.py
    remote:
      branch: master
      path: azure\datalake\store\utils.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 122
  summary: 'Find common directory for all paths


    Python''s `os.path.commonprefix` will not return a valid directory path in

    some cases, so we wrote this convenience method.

    '
  syntax:
    content: commonprefix(paths)
    parameters:
    - id: paths
  type: function
  uid: azure.datalake.store.utils.commonprefix
- fullName: azure.datalake.store.utils.ensure_writable
  langs:
  - python
  module: azure.datalake.store.utils
  name: ensure_writable(b)
  source:
    id: ensure_writable
    path: azure\datalake\store\utils.py
    remote:
      branch: master
      path: azure\datalake\store\utils.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 30
  syntax:
    content: ensure_writable(b)
    parameters:
    - id: b
  type: function
  uid: azure.datalake.store.utils.ensure_writable
- example:
  - '

    ```


    >>> from io import BytesIO  # doctest: +SKIP

    >>> f = BytesIO(b''Alice, 100\nBob, 200\nCharlie, 300'')  # doctest: +SKIP

    >>> read_block(f, 0, 13)  # doctest: +SKIP

    b''Alice, 100\nBo''

    ```



    ```


    >>> read_block(f, 0, 13, delimiter=b''\n'')  # doctest: +SKIP

    b''Alice, 100\n''

    ```



    ```


    >>> read_block(f, 10, 10, delimiter=b''\n'')  # doctest: +SKIP

    b''\nCharlie, 300''

    >>> f  = BytesIO(bytearray(2**22))  # doctest: +SKIP

    >>> read_block(f,0,2**22, delimiter=b''\n'')  # doctest: +SKIP

    IndexError: No delimiter found within max record size of 4MB.

    Transfer without specifying a delimiter (as binary) instead.

    ```

    '
  fullName: azure.datalake.store.utils.read_block
  langs:
  - python
  module: azure.datalake.store.utils
  name: read_block(f, offset, length, delimiter=None)
  source:
    id: read_block
    path: azure\datalake\store\utils.py
    remote:
      branch: master
      path: azure\datalake\store\utils.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 45
  summary: 'Read a block of bytes from a file

    '
  syntax:
    content: read_block(f, offset, length, delimiter=None)
    parameters:
    - description: 'a file object that supports seek, tell and read.

        '
      id: fn
      type:
      - file object
    - description: 'Byte offset to start read

        '
      id: offset
      type:
      - int
    - description: 'Maximum number of bytes to read

        '
      id: length
      type:
      - int
    - defaultValue: None
      description: 'Ensure reading stops at delimiter bytestring

        '
      id: delimiter
      type:
      - bytes (optional)
    - description: ''
      id: using the delimiter= keyword argument we ensure that the read
      type:
      - If
    - description: ''
      id: at or before the delimiter boundaries that follow the location
      type:
      - stops
    - description: ''
      id: + length. For ADL, if no delimiter is found and the data
      type:
      - offset
    - description: ''
      id: is > 4MB an exception is raised, since a single record cannot
      type:
      - requested
    - description: ''
      id: 4MB and be guaranteed to land contiguously in ADL.
      type:
      - exceed
    - description: ''
      id: bytestring returned WILL include the
      type:
      - The
    - description: ''
      id: delimiter string.
      type:
      - terminating
  type: function
  uid: azure.datalake.store.utils.read_block
- fullName: azure.datalake.store.utils.tokenize
  langs:
  - python
  module: azure.datalake.store.utils
  name: tokenize(*args, **kwargs)
  source:
    id: tokenize
    path: azure\datalake\store\utils.py
    remote:
      branch: master
      path: azure\datalake\store\utils.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 111
  summary: 'Deterministic token



    ```


    >>> tokenize(''Hello'') == tokenize(''Hello'')

    True

    ```

    '
  syntax:
    content: tokenize(*args, **kwargs)
  type: function
  uid: azure.datalake.store.utils.tokenize
- fullName: azure.datalake.store.utils.write_stdout
  langs:
  - python
  module: azure.datalake.store.utils
  name: write_stdout(data)
  source:
    id: write_stdout
    path: azure\datalake\store\utils.py
    remote:
      branch: master
      path: azure\datalake\store\utils.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 36
  summary: 'Write bytes or strings to standard output

    '
  syntax:
    content: write_stdout(data)
    parameters:
    - id: data
  type: function
  uid: azure.datalake.store.utils.write_stdout
references:
- fullName: azure.datalake.store.utils.CountUpDownLatch
  isExternal: false
  name: CountUpDownLatch
  parent: azure.datalake.store.utils
  uid: azure.datalake.store.utils.CountUpDownLatch
- fullName: azure.datalake.store.utils.clamp
  isExternal: false
  name: clamp(n, smallest, largest)
  parent: azure.datalake.store.utils
  uid: azure.datalake.store.utils.clamp
- fullName: azure.datalake.store.utils.commonprefix
  isExternal: false
  name: commonprefix(paths)
  parent: azure.datalake.store.utils
  uid: azure.datalake.store.utils.commonprefix
- fullName: azure.datalake.store.utils.ensure_writable
  isExternal: false
  name: ensure_writable(b)
  parent: azure.datalake.store.utils
  uid: azure.datalake.store.utils.ensure_writable
- fullName: azure.datalake.store.utils.read_block
  isExternal: false
  name: read_block(f, offset, length, delimiter=None)
  parent: azure.datalake.store.utils
  uid: azure.datalake.store.utils.read_block
- fullName: azure.datalake.store.utils.tokenize
  isExternal: false
  name: tokenize(*args, **kwargs)
  parent: azure.datalake.store.utils
  uid: azure.datalake.store.utils.tokenize
- fullName: azure.datalake.store.utils.write_stdout
  isExternal: false
  name: write_stdout(data)
  parent: azure.datalake.store.utils
  uid: azure.datalake.store.utils.write_stdout
- fullName: bytes (optional)
  name: bytes (optional)
  spec.python:
  - fullName: 'bytes '
    name: 'bytes '
    uid: 'bytes '
  - fullName: (
    name: (
  - fullName: optional
    name: optional
    uid: optional
  - fullName: )
    name: )
  uid: bytes (optional)
