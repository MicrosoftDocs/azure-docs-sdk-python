### YamlMime:PythonModule
uid: azure.datalake.store.utils
name: utils
fullName: azure.datalake.store.utils
functions:
- uid: azure.datalake.store.utils.clamp
  name: clamp
  summary: 'Limit a value to a given range


    This is equivalent to smallest <= n <= largest.'
  signature: clamp(n, smallest, largest)
  parameters:
  - name: n
    isRequired: true
  - name: smallest
    isRequired: true
  - name: largest
    isRequired: true
  examples:
  - '

    ```


    >>> clamp(0, 1, 100)

    1

    ```



    ```


    >>> clamp(42, 2, 128)

    42

    ```



    ```


    >>> clamp(1024, 1, 32)

    32

    ```

    '
- uid: azure.datalake.store.utils.commonprefix
  name: commonprefix
  summary: 'Find common directory for all paths


    Python''s `os.path.commonprefix` will not return a valid directory path in

    some cases, so we wrote this convenience method.'
  signature: commonprefix(paths)
  parameters:
  - name: paths
    isRequired: true
  examples:
  - '

    ```


    >>> # os.path.commonprefix returns ''/disk1/foo''

    >>> commonprefix([''/disk1/foobar'', ''/disk1/foobaz''])

    ''/disk1''

    ```



    ```


    >>> commonprefix([''a/b/c'', ''a/b/d'', ''a/c/d''])

    ''a''

    ```



    ```


    >>> commonprefix([''a/b/c'', ''d/e/f'', ''g/h/i''])

    ''''

    ```

    '
- uid: azure.datalake.store.utils.ensure_writable
  name: ensure_writable
  signature: ensure_writable(b)
  parameters:
  - name: b
    isRequired: true
- uid: azure.datalake.store.utils.read_block
  name: read_block
  summary: Read a block of bytes from a file
  signature: read_block(f, offset, length, delimiter=None)
  parameters:
  - name: fn
    description: a file object that supports seek, tell and read.
    isRequired: true
    types:
    - <xref:<xref:azure.datalake.store.utils.file object>>
  - name: offset
    description: Byte offset to start read
    isRequired: true
    types:
    - <xref:int>
  - name: length
    description: Maximum number of bytes to read
    isRequired: true
    types:
    - <xref:int>
  - name: delimiter
    description: Ensure reading stops at delimiter bytestring
    defaultValue: None
    types:
    - <xref:bytes >(<xref:optional>)
  - name: read
    isRequired: true
    types:
    - <xref:<xref:If using the delimiter= keyword argument we ensure that the>>
  - name: location
    isRequired: true
    types:
    - <xref:<xref:stops at>>
    - <xref:<xref:before the delimiter boundaries that follow the>>
  - name: ADL
    isRequired: true
    types:
    - <xref:<xref:offset + length. For>>
  - name: data
    isRequired: true
    types:
    - <xref:<xref:if no delimiter is found and the>>
  - name: raised
    isRequired: true
    types:
    - <xref:<xref:requested is > 4MB an exception is>>
  - name: cannot
    isRequired: true
    types:
    - <xref:<xref:since a single record>>
  - name: ADL.
    isRequired: true
    types:
    - <xref:<xref:exceed 4MB and be guaranteed to land contiguously in>>
  - name: the
    isRequired: true
    types:
    - <xref:<xref:The bytestring returned WILL include>>
  - name: string.
    isRequired: true
    types:
    - <xref:<xref:terminating delimiter>>
  - name: f
    isRequired: true
  examples:
  - "\n```\n\n>>> from io import BytesIO  \n>>> f = BytesIO(b'Alice, 100\\nBob, 200\\\
    nCharlie, 300')  \n>>> read_block(f, 0, 13)  \nb'Alice, 100\\nBo'\n```\n\n\n```\n\
    \n>>> read_block(f, 0, 13, delimiter=b'\\n')  \nb'Alice, 100\\n'\n```\n\n\n```\n\
    \n>>> read_block(f, 10, 10, delimiter=b'\\n')  \nb'\\nCharlie, 300'\n>>> f  =\
    \ BytesIO(bytearray(2**22))  \n>>> read_block(f,0,2**22, delimiter=b'\\n')  \n\
    IndexError: No delimiter found within max record size of 4MB.\nTransfer without\
    \ specifying a delimiter (as binary) instead.\n```\n"
- uid: azure.datalake.store.utils.tokenize
  name: tokenize
  summary: 'Deterministic token


    ```


    >>> tokenize(''Hello'') == tokenize(''Hello'')

    True

    ```'
  signature: tokenize(*args, **kwargs)
- uid: azure.datalake.store.utils.write_stdout
  name: write_stdout
  summary: Write bytes or strings to standard output
  signature: write_stdout(data)
  parameters:
  - name: data
    isRequired: true
classes:
- azure.datalake.store.utils.CountUpDownLatch
