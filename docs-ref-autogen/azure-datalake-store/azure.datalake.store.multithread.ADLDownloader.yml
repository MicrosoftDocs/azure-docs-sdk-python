### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.datalake.store.multithread.ADLDownloader.active
  - azure.datalake.store.multithread.ADLDownloader.clear_saved
  - azure.datalake.store.multithread.ADLDownloader.load
  - azure.datalake.store.multithread.ADLDownloader.run
  - azure.datalake.store.multithread.ADLDownloader.save
  - azure.datalake.store.multithread.ADLDownloader.successful
  class: azure.datalake.store.multithread.ADLDownloader
  fullName: azure.datalake.store.multithread.ADLDownloader
  inheritance:
  - type: builtins.object
  langs:
  - python
  module: azure.datalake.store.multithread
  name: ADLDownloader
  seealsoContent: '  azure.datalake.store.transfer.ADLTransferClient

    '
  source:
    id: ADLDownloader
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 60
  summary: 'Download remote file(s) using chunks and threads


    Launches multiple threads for efficient downloading, with *chunksize*

    assigned to each. The remote path can be a single file, a directory

    of files or a glob pattern.'
  syntax:
    content: ADLDownloader(adlfs, rpath, lpath, nthreads=None, chunksize=268435456,
      buffersize=4194304, blocksize=4194304, client=None, run=True, overwrite=False,
      verbose=False, progress_callback=None, timeout=0)
    parameters:
    - description: ''
      id: adlfs
      type:
      - ADL filesystem instance
    - description: 'remote path/globstring to use to find remote files. Recursive
        glob

        patterns using **** are not supported.'
      id: rpath
      type:
      - str
    - description: 'local path. If downloading a single file, will write to this specific

        file, unless it is an existing directory, in which case a file is

        created within it. If downloading multiple files, this is the root

        directory to write within. Will create directories as required.'
      id: lpath
      type:
      - str
    - description: Number of threads to use. If None, uses the number of cores.
      id: nthreads
      type:
      - int [None]
    - description: 'Number of bytes for a chunk. Large files are split into chunks.
        Files

        smaller than this number will always be transferred in a single thread.'
      id: chunksize
      type:
      - int [2**28]
    - description: 'Ignored in curret implementation.

        Number of bytes for internal buffer. This block cannot be bigger than

        a chunk and cannot be smaller than a block.'
      id: buffersize
      type:
      - int [2**22]
    - description: 'Number of bytes for a block. Within each chunk, we write a smaller

        block for each API call. This block cannot be bigger than a chunk.'
      id: blocksize
      type:
      - int [2**22]
    - description: 'Set an instance of ADLTransferClient when finer-grained control
        over

        transfer parameters is needed. Ignores *nthreads* and *chunksize* set

        by constructor.'
      id: client
      type:
      - ADLTransferClient [None]
    - description: Whether to begin executing immediately.
      id: run
      type:
      - bool [True]
    - description: 'Whether to forcibly overwrite existing files/directories. If False
        and

        local path is a directory, will quit regardless if any files would be

        overwritten or not. If True, only matching filenames are actually

        overwritten.'
      id: overwrite
      type:
      - bool [False]
    - description: 'Callback for progress with signature function(current, total)
        where

        current is the number of bytes transfered so far, and total is the

        size of the blob, or None if the total size is unknown.'
      id: progress_callback
      type:
      - callable [None]
    - description: 'Default value 0 means infinite timeout. Otherwise time in seconds
        before the

        process will stop and raise an exception if  transfer is still in progress'
      id: timeout
      type:
      - int (0)
  type: class
  uid: azure.datalake.store.multithread.ADLDownloader
- class: azure.datalake.store.multithread.ADLDownloader
  fullName: azure.datalake.store.multithread.ADLDownloader.active
  langs:
  - python
  module: azure.datalake.store.multithread
  name: active()
  namewithoutparameters: active
  source:
    id: active
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 285
  summary: Return whether the downloader is active
  syntax:
    content: active()
    parameters: []
  type: method
  uid: azure.datalake.store.multithread.ADLDownloader.active
- class: azure.datalake.store.multithread.ADLDownloader
  fullName: azure.datalake.store.multithread.ADLDownloader.clear_saved
  langs:
  - python
  module: azure.datalake.store.multithread
  name: clear_saved()
  namewithoutparameters: clear_saved
  source:
    id: clear_saved
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 187
  summary: Remove references to all persisted downloads.
  syntax:
    content: clear_saved()
  type: method
  uid: azure.datalake.store.multithread.ADLDownloader.clear_saved
- class: azure.datalake.store.multithread.ADLDownloader
  fullName: azure.datalake.store.multithread.ADLDownloader.load
  langs:
  - python
  module: azure.datalake.store.multithread
  name: load()
  namewithoutparameters: load
  source:
    id: load
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 174
  summary: Load list of persisted transfers from disk, for possible resumption.
  syntax:
    content: load()
    return:
      description: "* *A dictionary of download instances. The hashes are auto-* \n\
        \n* *generated unique. The state of the chunks completed, errored, etc.,*\
        \ \n\n* *can be seen in the status attribute. Instances can be resumed with*\
        \ \n\n* `run()`."
  type: method
  uid: azure.datalake.store.multithread.ADLDownloader.load
- class: azure.datalake.store.multithread.ADLDownloader
  fullName: azure.datalake.store.multithread.ADLDownloader.run
  langs:
  - python
  module: azure.datalake.store.multithread
  name: run(nthreads=None, monitor=True)
  namewithoutparameters: run
  source:
    id: run
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 251
  summary: Populate transfer queue and execute downloads
  syntax:
    content: run(nthreads=None, monitor=True)
    parameters:
    - defaultValue: None
      description: Override default nthreads, if given
      id: nthreads
      type:
      - int [None]
    - defaultValue: 'True'
      description: To watch and wait (block) until completion.
      id: monitor
      type:
      - bool [True]
  type: method
  uid: azure.datalake.store.multithread.ADLDownloader.run
- class: azure.datalake.store.multithread.ADLDownloader
  fullName: azure.datalake.store.multithread.ADLDownloader.save
  langs:
  - python
  module: azure.datalake.store.multithread
  name: save(keep=True)
  namewithoutparameters: save
  source:
    id: save
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 153
  summary: 'Persist this download


    Saves a copy of this transfer process in its current state to disk.

    This is done automatically for a running transfer, so that as a chunk

    is completed, this is reflected. Thus, if a transfer is interrupted,

    e.g., by user action, the transfer can be restarted at another time.

    All chunks that were not already completed will be restarted at that

    time.


    See methods `load` to retrieved saved transfers and `run` to

    resume a stopped transfer.'
  syntax:
    content: save(keep=True)
    parameters:
    - defaultValue: 'True'
      description: 'If True, transfer will be saved if some chunks remain to be

        completed; the transfer will be sure to be removed otherwise.'
      id: keep
      type:
      - bool (True)
  type: method
  uid: azure.datalake.store.multithread.ADLDownloader.save
- class: azure.datalake.store.multithread.ADLDownloader
  fullName: azure.datalake.store.multithread.ADLDownloader.successful
  langs:
  - python
  module: azure.datalake.store.multithread
  name: successful()
  namewithoutparameters: successful
  source:
    id: successful
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 289
  summary: 'Return whether the downloader completed successfully.


    It will raise AssertionError if the downloader is active.'
  syntax:
    content: successful()
    parameters: []
  type: method
  uid: azure.datalake.store.multithread.ADLDownloader.successful
references:
- fullName: azure.datalake.store.multithread.ADLDownloader.active
  isExternal: false
  name: active()
  parent: azure.datalake.store.multithread.ADLDownloader
  uid: azure.datalake.store.multithread.ADLDownloader.active
- fullName: azure.datalake.store.multithread.ADLDownloader.clear_saved
  isExternal: false
  name: clear_saved()
  parent: azure.datalake.store.multithread.ADLDownloader
  uid: azure.datalake.store.multithread.ADLDownloader.clear_saved
- fullName: azure.datalake.store.multithread.ADLDownloader.load
  isExternal: false
  name: load()
  parent: azure.datalake.store.multithread.ADLDownloader
  uid: azure.datalake.store.multithread.ADLDownloader.load
- fullName: azure.datalake.store.multithread.ADLDownloader.run
  isExternal: false
  name: run(nthreads=None, monitor=True)
  parent: azure.datalake.store.multithread.ADLDownloader
  uid: azure.datalake.store.multithread.ADLDownloader.run
- fullName: azure.datalake.store.multithread.ADLDownloader.save
  isExternal: false
  name: save(keep=True)
  parent: azure.datalake.store.multithread.ADLDownloader
  uid: azure.datalake.store.multithread.ADLDownloader.save
- fullName: azure.datalake.store.multithread.ADLDownloader.successful
  isExternal: false
  name: successful()
  parent: azure.datalake.store.multithread.ADLDownloader
  uid: azure.datalake.store.multithread.ADLDownloader.successful
- fullName: int [None]
  name: int [None]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: int [None]
- fullName: int [2**28]
  name: int [2**28]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**28
    name: 2**28
    uid: 2**28
  - fullName: ']'
    name: ']'
  uid: int [2**28]
- fullName: int [2**22]
  name: int [2**22]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**22
    name: 2**22
    uid: 2**22
  - fullName: ']'
    name: ']'
  uid: int [2**22]
- fullName: int [2**22]
  name: int [2**22]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**22
    name: 2**22
    uid: 2**22
  - fullName: ']'
    name: ']'
  uid: int [2**22]
- fullName: ADLTransferClient [None]
  name: ADLTransferClient [None]
  spec.python:
  - fullName: 'ADLTransferClient '
    name: 'ADLTransferClient '
    uid: 'ADLTransferClient '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: ADLTransferClient [None]
- fullName: bool [True]
  name: bool [True]
  spec.python:
  - fullName: 'bool '
    name: 'bool '
    uid: 'bool '
  - fullName: '['
    name: '['
  - fullName: 'True'
    name: 'True'
    uid: 'True'
  - fullName: ']'
    name: ']'
  uid: bool [True]
- fullName: bool [False]
  name: bool [False]
  spec.python:
  - fullName: 'bool '
    name: 'bool '
    uid: 'bool '
  - fullName: '['
    name: '['
  - fullName: 'False'
    name: 'False'
    uid: 'False'
  - fullName: ']'
    name: ']'
  uid: bool [False]
- fullName: callable [None]
  name: callable [None]
  spec.python:
  - fullName: 'callable '
    name: 'callable '
    uid: 'callable '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: callable [None]
- fullName: int (0)
  name: int (0)
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: (
    name: (
  - fullName: '0'
    name: '0'
    uid: '0'
  - fullName: )
    name: )
  uid: int (0)
- fullName: bool (True)
  name: bool (True)
  spec.python:
  - fullName: 'bool '
    name: 'bool '
    uid: 'bool '
  - fullName: (
    name: (
  - fullName: 'True'
    name: 'True'
    uid: 'True'
  - fullName: )
    name: )
  uid: bool (True)
