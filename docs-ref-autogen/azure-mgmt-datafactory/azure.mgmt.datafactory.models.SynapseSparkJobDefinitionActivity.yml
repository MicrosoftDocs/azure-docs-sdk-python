### YamlMime:PythonClass
uid: azure.mgmt.datafactory.models.SynapseSparkJobDefinitionActivity
name: SynapseSparkJobDefinitionActivity
fullName: azure.mgmt.datafactory.models.SynapseSparkJobDefinitionActivity
module: azure.mgmt.datafactory.models
inheritances:
- azure.mgmt.datafactory.models._models_py3.ExecutionActivity
summary: 'Execute spark job activity.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'SynapseSparkJobDefinitionActivity(*, name: str, spark_job: _models.SynapseSparkJobReference,
    additional_properties: Dict[str, MutableMapping[str, Any]] | None = None, description:
    str | None = None, depends_on: List[_models.ActivityDependency] | None = None,
    user_properties: List[_models.UserProperty] | None = None, linked_service_name:
    _models.LinkedServiceReference | None = None, policy: _models.ActivityPolicy |
    None = None, arguments: List[Any] | None = None, file: MutableMapping[str, Any]
    | None = None, scan_folder: MutableMapping[str, Any] | None = None, class_name:
    MutableMapping[str, Any] | None = None, files: List[MutableMapping[str, Any]]
    | None = None, python_code_reference: List[MutableMapping[str, Any]] | None =
    None, files_v2: List[MutableMapping[str, Any]] | None = None, target_big_data_pool:
    _models.BigDataPoolParametrizationReference | None = None, executor_size: MutableMapping[str,
    Any] | None = None, conf: MutableMapping[str, Any] | None = None, driver_size:
    MutableMapping[str, Any] | None = None, num_executors: MutableMapping[str, Any]
    | None = None, configuration_type: str | _models.ConfigurationType | None = None,
    target_spark_configuration: _models.SparkConfigurationParametrizationReference
    | None = None, spark_config: Dict[str, MutableMapping[str, Any]] | None = None,
    **kwargs: Any)'
  parameters:
  - name: additional_properties
    description: 'Unmatched properties from the message are deserialized to this

      collection.'
    types:
    - <xref:dict>[<xref:str>, <xref:JSON>]
  - name: name
    description: Activity name. Required.
    types:
    - <xref:str>
  - name: description
    description: Activity description.
    types:
    - <xref:str>
  - name: depends_on
    description: Activity depends on condition.
    types:
    - <xref:azure.mgmt.datafactory.aio.operations.FactoriesOperations.list>[<xref:azure.mgmt.datafactory.models.ActivityDependency>]
  - name: user_properties
    description: Activity user properties.
    types:
    - <xref:azure.mgmt.datafactory.aio.operations.FactoriesOperations.list>[<xref:azure.mgmt.datafactory.models.UserProperty>]
  - name: linked_service_name
    description: Linked service reference.
    types:
    - <xref:azure.mgmt.datafactory.models.LinkedServiceReference>
  - name: policy
    description: Activity policy.
    types:
    - <xref:azure.mgmt.datafactory.models.ActivityPolicy>
  - name: spark_job
    description: Synapse spark job reference. Required.
    types:
    - <xref:azure.mgmt.datafactory.models.SynapseSparkJobReference>
  - name: arguments
    description: User specified arguments to SynapseSparkJobDefinitionActivity.
    types:
    - <xref:azure.mgmt.datafactory.aio.operations.FactoriesOperations.list>[<xref:any>]
  - name: file
    description: 'The main file used for the job, which will override the ''file''
      of the spark job

      definition you provide. Type: string (or Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: scan_folder
    description: 'Scanning subfolders from the root folder of the main definition
      file,

      these files will be added as reference files. The folders named ''jars'', ''pyFiles'',
      ''files'' or

      ''archives'' will be scanned, and the folders name are case sensitive. Type:
      boolean (or

      Expression with resultType boolean).'
    types:
    - <xref:JSON>
  - name: class_name
    description: 'The fully-qualified identifier or the main class that is in the
      main

      definition file, which will override the ''className'' of the spark job definition
      you provide.

      Type: string (or Expression with resultType string).'
    types:
    - <xref:JSON>
  - name: files
    description: '(Deprecated. Please use pythonCodeReference and filesV2) Additional
      files used

      for reference in the main definition file, which will override the ''files''
      of the spark job

      definition you provide.'
    types:
    - <xref:azure.mgmt.datafactory.aio.operations.FactoriesOperations.list>[<xref:JSON>]
  - name: python_code_reference
    description: 'Additional python code files used for reference in the main

      definition file, which will override the ''pyFiles'' of the spark job definition
      you provide.'
    types:
    - <xref:azure.mgmt.datafactory.aio.operations.FactoriesOperations.list>[<xref:JSON>]
  - name: files_v2
    description: 'Additional files used for reference in the main definition file,
      which will

      override the ''jars'' and ''files'' of the spark job definition you provide.'
    types:
    - <xref:azure.mgmt.datafactory.aio.operations.FactoriesOperations.list>[<xref:JSON>]
  - name: target_big_data_pool
    description: 'The name of the big data pool which will be used to execute the

      spark batch job, which will override the ''targetBigDataPool'' of the spark
      job definition you

      provide.'
    types:
    - <xref:azure.mgmt.datafactory.models.BigDataPoolParametrizationReference>
  - name: executor_size
    description: 'Number of core and memory to be used for executors allocated in
      the

      specified Spark pool for the job, which will be used for overriding ''executorCores''
      and

      ''executorMemory'' of the spark job definition you provide. Type: string (or
      Expression with

      resultType string).'
    types:
    - <xref:JSON>
  - name: conf
    description: 'Spark configuration properties, which will override the ''conf''
      of the spark job

      definition you provide.'
    types:
    - <xref:JSON>
  - name: driver_size
    description: 'Number of core and memory to be used for driver allocated in the

      specified Spark pool for the job, which will be used for overriding ''driverCores''
      and

      ''driverMemory'' of the spark job definition you provide. Type: string (or Expression
      with

      resultType string).'
    types:
    - <xref:JSON>
  - name: num_executors
    description: 'Number of executors to launch for this job, which will override
      the

      ''numExecutors'' of the spark job definition you provide. Type: integer (or
      Expression with

      resultType integer).'
    types:
    - <xref:JSON>
  - name: configuration_type
    description: 'The type of the spark config. Known values are: "Default",

      "Customized", and "Artifact".'
    types:
    - <xref:str>
    - <xref:azure.mgmt.datafactory.models.ConfigurationType>
  - name: target_spark_configuration
    description: The spark configuration of the spark job.
    types:
    - <xref:azure.mgmt.datafactory.models.SparkConfigurationParametrizationReference>
  - name: spark_config
    description: Spark configuration property.
    types:
    - <xref:dict>[<xref:str>, <xref:JSON>]
variables:
- description: 'Unmatched properties from the message are deserialized to this

    collection.'
  name: additional_properties
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: Activity name. Required.
  name: name
  types:
  - <xref:str>
- description: Type of activity. Required.
  name: type
  types:
  - <xref:str>
- description: Activity description.
  name: description
  types:
  - <xref:str>
- description: Activity depends on condition.
  name: depends_on
  types:
  - <xref:list>[<xref:azure.mgmt.datafactory.models.ActivityDependency>]
- description: Activity user properties.
  name: user_properties
  types:
  - <xref:list>[<xref:azure.mgmt.datafactory.models.UserProperty>]
- description: Linked service reference.
  name: linked_service_name
  types:
  - <xref:azure.mgmt.datafactory.models.LinkedServiceReference>
- description: Activity policy.
  name: policy
  types:
  - <xref:azure.mgmt.datafactory.models.ActivityPolicy>
- description: Synapse spark job reference. Required.
  name: spark_job
  types:
  - <xref:azure.mgmt.datafactory.models.SynapseSparkJobReference>
- description: User specified arguments to SynapseSparkJobDefinitionActivity.
  name: arguments
  types:
  - <xref:list>[<xref:any>]
- description: 'The main file used for the job, which will override the ''file'' of
    the spark job

    definition you provide. Type: string (or Expression with resultType string).'
  name: file
  types:
  - <xref:JSON>
- description: 'Scanning subfolders from the root folder of the main definition file,
    these

    files will be added as reference files. The folders named ''jars'', ''pyFiles'',
    ''files'' or

    ''archives'' will be scanned, and the folders name are case sensitive. Type: boolean
    (or

    Expression with resultType boolean).'
  name: scan_folder
  types:
  - <xref:JSON>
- description: 'The fully-qualified identifier or the main class that is in the main

    definition file, which will override the ''className'' of the spark job definition
    you provide.

    Type: string (or Expression with resultType string).'
  name: class_name
  types:
  - <xref:JSON>
- description: '(Deprecated. Please use pythonCodeReference and filesV2) Additional
    files used for

    reference in the main definition file, which will override the ''files'' of the
    spark job

    definition you provide.'
  name: files
  types:
  - <xref:list>[<xref:JSON>]
- description: 'Additional python code files used for reference in the main

    definition file, which will override the ''pyFiles'' of the spark job definition
    you provide.'
  name: python_code_reference
  types:
  - <xref:list>[<xref:JSON>]
- description: 'Additional files used for reference in the main definition file, which
    will

    override the ''jars'' and ''files'' of the spark job definition you provide.'
  name: files_v2
  types:
  - <xref:list>[<xref:JSON>]
- description: 'The name of the big data pool which will be used to execute the

    spark batch job, which will override the ''targetBigDataPool'' of the spark job
    definition you

    provide.'
  name: target_big_data_pool
  types:
  - <xref:azure.mgmt.datafactory.models.BigDataPoolParametrizationReference>
- description: 'Number of core and memory to be used for executors allocated in the

    specified Spark pool for the job, which will be used for overriding ''executorCores''
    and

    ''executorMemory'' of the spark job definition you provide. Type: string (or Expression
    with

    resultType string).'
  name: executor_size
  types:
  - <xref:JSON>
- description: 'Spark configuration properties, which will override the ''conf'' of
    the spark job

    definition you provide.'
  name: conf
  types:
  - <xref:JSON>
- description: 'Number of core and memory to be used for driver allocated in the specified

    Spark pool for the job, which will be used for overriding ''driverCores'' and
    ''driverMemory'' of

    the spark job definition you provide. Type: string (or Expression with resultType
    string).'
  name: driver_size
  types:
  - <xref:JSON>
- description: 'Number of executors to launch for this job, which will override the

    ''numExecutors'' of the spark job definition you provide. Type: integer (or Expression
    with

    resultType integer).'
  name: num_executors
  types:
  - <xref:JSON>
- description: 'The type of the spark config. Known values are: "Default",

    "Customized", and "Artifact".'
  name: configuration_type
  types:
  - <xref:str>
  - <xref:azure.mgmt.datafactory.models.ConfigurationType>
- description: The spark configuration of the spark job.
  name: target_spark_configuration
  types:
  - <xref:azure.mgmt.datafactory.models.SparkConfigurationParametrizationReference>
- description: Spark configuration property.
  name: spark_config
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
