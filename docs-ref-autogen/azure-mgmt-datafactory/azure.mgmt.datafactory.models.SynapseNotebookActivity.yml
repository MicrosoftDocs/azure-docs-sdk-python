### YamlMime:PythonClass
uid: azure.mgmt.datafactory.models.SynapseNotebookActivity
name: SynapseNotebookActivity
fullName: azure.mgmt.datafactory.models.SynapseNotebookActivity
module: azure.mgmt.datafactory.models
inheritances:
- azure.mgmt.datafactory.models._models_py3.ExecutionActivity
summary: 'Execute Synapse notebook activity.


  All required parameters must be populated in order to send to server.'
constructor:
  syntax: 'SynapseNotebookActivity(*, name: str, notebook: _models.SynapseNotebookReference,
    additional_properties: Dict[str, MutableMapping[str, Any]] | None = None, description:
    str | None = None, state: str | _models.ActivityState | None = None, on_inactive_mark_as:
    str | _models.ActivityOnInactiveMarkAs | None = None, depends_on: List[_models.ActivityDependency]
    | None = None, user_properties: List[_models.UserProperty] | None = None, linked_service_name:
    _models.LinkedServiceReference | None = None, policy: _models.ActivityPolicy |
    None = None, spark_pool: _models.BigDataPoolParametrizationReference | None =
    None, parameters: Dict[str, _models.NotebookParameter] | None = None, executor_size:
    MutableMapping[str, Any] | None = None, conf: MutableMapping[str, Any] | None
    = None, driver_size: MutableMapping[str, Any] | None = None, num_executors: MutableMapping[str,
    Any] | None = None, configuration_type: str | _models.ConfigurationType | None
    = None, target_spark_configuration: _models.SparkConfigurationParametrizationReference
    | None = None, spark_config: Dict[str, MutableMapping[str, Any]] | None = None,
    **kwargs: Any)'
  keywordOnlyParameters:
  - name: additional_properties
    description: 'Unmatched properties from the message are deserialized to this

      collection.'
    types:
    - <xref:dict>[<xref:str>, <xref:JSON>]
  - name: name
    description: Activity name. Required.
    types:
    - <xref:str>
  - name: description
    description: Activity description.
    types:
    - <xref:str>
  - name: state
    description: 'Activity state. This is an optional property and if not provided,
      the state

      will be Active by default. Known values are: "Active" and "Inactive".'
    types:
    - <xref:str>
    - <xref:azure.mgmt.datafactory.models.ActivityState>
  - name: on_inactive_mark_as
    description: 'Status result of the activity when the state is set to Inactive.

      This is an optional property and if not provided when the activity is inactive,
      the status will

      be Succeeded by default. Known values are: "Succeeded", "Failed", and "Skipped".'
    types:
    - <xref:str>
    - <xref:azure.mgmt.datafactory.models.ActivityOnInactiveMarkAs>
  - name: depends_on
    description: Activity depends on condition.
    types:
    - <xref:list>[<xref:azure.mgmt.datafactory.models.ActivityDependency>]
  - name: user_properties
    description: Activity user properties.
    types:
    - <xref:list>[<xref:azure.mgmt.datafactory.models.UserProperty>]
  - name: linked_service_name
    description: Linked service reference.
    types:
    - <xref:azure.mgmt.datafactory.models.LinkedServiceReference>
  - name: policy
    description: Activity policy.
    types:
    - <xref:azure.mgmt.datafactory.models.ActivityPolicy>
  - name: notebook
    description: Synapse notebook reference. Required.
    types:
    - <xref:azure.mgmt.datafactory.models.SynapseNotebookReference>
  - name: spark_pool
    description: The name of the big data pool which will be used to execute the notebook.
    types:
    - <xref:azure.mgmt.datafactory.models.BigDataPoolParametrizationReference>
  - name: parameters
    description: Notebook parameters.
    types:
    - <xref:dict>[<xref:str>, <xref:azure.mgmt.datafactory.models.NotebookParameter>]
  - name: executor_size
    description: 'Number of core and memory to be used for executors allocated in
      the

      specified Spark pool for the session, which will be used for overriding ''executorCores''
      and

      ''executorMemory'' of the notebook you provide. Type: string (or Expression
      with resultType

      string).'
    types:
    - <xref:JSON>
  - name: conf
    description: 'Spark configuration properties, which will override the ''conf''
      of the notebook

      you provide.'
    types:
    - <xref:JSON>
  - name: driver_size
    description: 'Number of core and memory to be used for driver allocated in the

      specified Spark pool for the session, which will be used for overriding ''driverCores''
      and

      ''driverMemory'' of the notebook you provide. Type: string (or Expression with
      resultType

      string).'
    types:
    - <xref:JSON>
  - name: num_executors
    description: 'Number of executors to launch for this session, which will override
      the

      ''numExecutors'' of the notebook you provide. Type: integer (or Expression with
      resultType

      integer).'
    types:
    - <xref:JSON>
  - name: configuration_type
    description: 'The type of the spark config. Known values are: "Default",

      "Customized", and "Artifact".'
    types:
    - <xref:str>
    - <xref:azure.mgmt.datafactory.models.ConfigurationType>
  - name: target_spark_configuration
    description: The spark configuration of the spark job.
    types:
    - <xref:azure.mgmt.datafactory.models.SparkConfigurationParametrizationReference>
  - name: spark_config
    description: Spark configuration property.
    types:
    - <xref:dict>[<xref:str>, <xref:JSON>]
variables:
- description: 'Unmatched properties from the message are deserialized to this

    collection.'
  name: additional_properties
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
- description: Activity name. Required.
  name: name
  types:
  - <xref:str>
- description: Type of activity. Required.
  name: type
  types:
  - <xref:str>
- description: Activity description.
  name: description
  types:
  - <xref:str>
- description: 'Activity state. This is an optional property and if not provided,
    the state will

    be Active by default. Known values are: "Active" and "Inactive".'
  name: state
  types:
  - <xref:str>
  - <xref:azure.mgmt.datafactory.models.ActivityState>
- description: 'Status result of the activity when the state is set to Inactive.

    This is an optional property and if not provided when the activity is inactive,
    the status will

    be Succeeded by default. Known values are: "Succeeded", "Failed", and "Skipped".'
  name: on_inactive_mark_as
  types:
  - <xref:str>
  - <xref:azure.mgmt.datafactory.models.ActivityOnInactiveMarkAs>
- description: Activity depends on condition.
  name: depends_on
  types:
  - <xref:list>[<xref:azure.mgmt.datafactory.models.ActivityDependency>]
- description: Activity user properties.
  name: user_properties
  types:
  - <xref:list>[<xref:azure.mgmt.datafactory.models.UserProperty>]
- description: Linked service reference.
  name: linked_service_name
  types:
  - <xref:azure.mgmt.datafactory.models.LinkedServiceReference>
- description: Activity policy.
  name: policy
  types:
  - <xref:azure.mgmt.datafactory.models.ActivityPolicy>
- description: Synapse notebook reference. Required.
  name: notebook
  types:
  - <xref:azure.mgmt.datafactory.models.SynapseNotebookReference>
- description: The name of the big data pool which will be used to execute the notebook.
  name: spark_pool
  types:
  - <xref:azure.mgmt.datafactory.models.BigDataPoolParametrizationReference>
- description: Notebook parameters.
  name: parameters
  types:
  - <xref:dict>[<xref:str>, <xref:azure.mgmt.datafactory.models.NotebookParameter>]
- description: 'Number of core and memory to be used for executors allocated in the

    specified Spark pool for the session, which will be used for overriding ''executorCores''
    and

    ''executorMemory'' of the notebook you provide. Type: string (or Expression with
    resultType

    string).'
  name: executor_size
  types:
  - <xref:JSON>
- description: 'Spark configuration properties, which will override the ''conf'' of
    the notebook you

    provide.'
  name: conf
  types:
  - <xref:JSON>
- description: 'Number of core and memory to be used for driver allocated in the specified

    Spark pool for the session, which will be used for overriding ''driverCores''
    and ''driverMemory''

    of the notebook you provide. Type: string (or Expression with resultType string).'
  name: driver_size
  types:
  - <xref:JSON>
- description: 'Number of executors to launch for this session, which will override
    the

    ''numExecutors'' of the notebook you provide. Type: integer (or Expression with
    resultType

    integer).'
  name: num_executors
  types:
  - <xref:JSON>
- description: 'The type of the spark config. Known values are: "Default",

    "Customized", and "Artifact".'
  name: configuration_type
  types:
  - <xref:str>
  - <xref:azure.mgmt.datafactory.models.ConfigurationType>
- description: The spark configuration of the spark job.
  name: target_spark_configuration
  types:
  - <xref:azure.mgmt.datafactory.models.SparkConfigurationParametrizationReference>
- description: Spark configuration property.
  name: spark_config
  types:
  - <xref:dict>[<xref:str>, <xref:JSON>]
