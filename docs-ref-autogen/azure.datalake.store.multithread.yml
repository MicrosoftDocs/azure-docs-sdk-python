### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.datalake.store.multithread.ADLDownloader
  - azure.datalake.store.multithread.ADLUploader
  - azure.datalake.store.multithread.get_chunk
  - azure.datalake.store.multithread.load
  - azure.datalake.store.multithread.merge_chunks
  - azure.datalake.store.multithread.put_chunk
  - azure.datalake.store.multithread.save
  fullName: azure.datalake.store.multithread
  langs:
  - python
  module: azure.datalake.store.multithread
  name: multithread
  source:
    id: multithread
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 0
  summary: 'High performance multi-threaded module to up/download



    Calls method in `core` with thread pool executor to ensure the network

    is used to its maximum throughput.



    Only implements upload and download of (massive) files and directory trees.

    '
  type: module
  uid: azure.datalake.store.multithread
- fullName: azure.datalake.store.multithread.get_chunk
  langs:
  - python
  module: azure.datalake.store.multithread
  name: get_chunk
  source:
    id: get_chunk
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 265
  summary: 'Download a piece of a remote file and write locally


    Internal function used by *download*.

    '
  syntax:
    content: get_chunk(adlfs, src, dst, offset, size, buffersize, blocksize, shutdown_event=None,
      retries=10, delay=0.01, backoff=3)
    parameters:
    - id: adlfs
    - id: src
    - id: dst
    - id: offset
    - id: size
    - id: buffersize
    - id: blocksize
    - defaultValue: None
      id: shutdown_event
    - defaultValue: '10'
      id: retries
    - defaultValue: '0.01'
      id: delay
    - defaultValue: '3'
      id: backoff
  type: function
  uid: azure.datalake.store.multithread.get_chunk
- fullName: azure.datalake.store.multithread.load
  langs:
  - python
  module: azure.datalake.store.multithread
  name: load
  source:
    id: load
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 50
  syntax:
    content: load(filename)
    parameters:
    - id: filename
  type: function
  uid: azure.datalake.store.multithread.load
- fullName: azure.datalake.store.multithread.merge_chunks
  langs:
  - python
  module: azure.datalake.store.multithread
  name: merge_chunks
  source:
    id: merge_chunks
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 541
  syntax:
    content: merge_chunks(adlfs, outfile, files, shutdown_event=None, overwrite=False)
    parameters:
    - id: adlfs
    - id: outfile
    - id: files
    - defaultValue: None
      id: shutdown_event
    - defaultValue: 'False'
      id: overwrite
  type: function
  uid: azure.datalake.store.multithread.merge_chunks
- fullName: azure.datalake.store.multithread.put_chunk
  langs:
  - python
  module: azure.datalake.store.multithread
  name: put_chunk
  source:
    id: put_chunk
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 513
  summary: 'Upload a piece of a local file


    Internal function used by *upload*.

    '
  syntax:
    content: put_chunk(adlfs, src, dst, offset, size, buffersize, blocksize, delimiter=None,
      shutdown_event=None)
    parameters:
    - id: adlfs
    - id: src
    - id: dst
    - id: offset
    - id: size
    - id: buffersize
    - id: blocksize
    - defaultValue: None
      id: delimiter
    - defaultValue: None
      id: shutdown_event
  type: function
  uid: azure.datalake.store.multithread.put_chunk
- fullName: azure.datalake.store.multithread.save
  langs:
  - python
  module: azure.datalake.store.multithread
  name: save
  source:
    id: save
    path: azure\datalake\store\multithread.py
    remote:
      branch: master
      path: azure\datalake\store\multithread.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 33
  syntax:
    content: save(instance, filename, keep=True)
    parameters:
    - id: instance
    - id: filename
    - defaultValue: 'True'
      id: keep
  type: function
  uid: azure.datalake.store.multithread.save
references:
- fullName: azure.datalake.store.multithread.ADLDownloader
  isExternal: false
  name: ADLDownloader
  parent: azure.datalake.store.multithread
  uid: azure.datalake.store.multithread.ADLDownloader
- fullName: azure.datalake.store.multithread.ADLUploader
  isExternal: false
  name: ADLUploader
  parent: azure.datalake.store.multithread
  uid: azure.datalake.store.multithread.ADLUploader
- fullName: azure.datalake.store.multithread.get_chunk
  isExternal: false
  name: get_chunk
  parent: azure.datalake.store.multithread
  uid: azure.datalake.store.multithread.get_chunk
- fullName: azure.datalake.store.multithread.load
  isExternal: false
  name: load
  parent: azure.datalake.store.multithread
  uid: azure.datalake.store.multithread.load
- fullName: azure.datalake.store.multithread.merge_chunks
  isExternal: false
  name: merge_chunks
  parent: azure.datalake.store.multithread
  uid: azure.datalake.store.multithread.merge_chunks
- fullName: azure.datalake.store.multithread.put_chunk
  isExternal: false
  name: put_chunk
  parent: azure.datalake.store.multithread
  uid: azure.datalake.store.multithread.put_chunk
- fullName: azure.datalake.store.multithread.save
  isExternal: false
  name: save
  parent: azure.datalake.store.multithread
  uid: azure.datalake.store.multithread.save
