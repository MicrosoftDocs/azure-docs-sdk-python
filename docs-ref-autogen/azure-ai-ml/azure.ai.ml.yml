### YamlMime:PythonPackage
uid: azure.ai.ml
name: ml
fullName: azure.ai.ml
type: rootImport
functions:
- uid: azure.ai.ml.command
  name: command
  summary: Creates a Command object which can be used inside a dsl.pipeline function
    or used as a standalone Command job.
  signature: 'command(*, name: str | None = None, description: str | None = None,
    tags: Dict | None = None, properties: Dict | None = None, display_name: str |
    None = None, command: str | None = None, experiment_name: str | None = None, environment:
    str | Environment | None = None, environment_variables: Dict | None = None, distribution:
    Dict | MpiDistribution | TensorFlowDistribution | PyTorchDistribution | RayDistribution
    | DistributionConfiguration | None = None, compute: str | None = None, inputs:
    Dict | None = None, outputs: Dict | None = None, instance_count: int | None =
    None, instance_type: str | None = None, locations: List[str] | None = None, docker_args:
    str | None = None, shm_size: str | None = None, timeout: int | None = None, code:
    PathLike | str | None = None, identity: ManagedIdentityConfiguration | AmlTokenConfiguration
    | UserIdentityConfiguration | None = None, is_deterministic: bool = True, services:
    Dict[str, JobService | JupyterLabJobService | SshJobService | TensorBoardJobService
    | VsCodeJobService] | None = None, job_tier: str | None = None, priority: str
    | None = None, **kwargs: Any) -> Command'
  keywordOnlyParameters:
  - name: name
    description: The name of the Command job or component.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: description
    description: The description of the Command. Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: tags
    description: Tag dictionary. Tags can be added, removed, and updated. Defaults
      to None.
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:str>]]
  - name: properties
    description: The job property dictionary. Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:str>]]
  - name: display_name
    description: The display name of the job. Defaults to a randomly generated name.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: command
    description: The command to be executed. Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: experiment_name
    description: 'The name of the experiment that the job will be created under. Defaults
      to current

      directory name.'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: environment
    description: The environment that the job will run in.
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[<xref:str>, <xref:azure.ai.ml.entities.Environment>]]
  - name: environment_variables
    description: 'A dictionary of environment variable names and values.

      These environment variables are set on the process where user script is being
      executed.

      Defaults to None.'
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:str>]]
  - name: distribution
    description: The configuration for distributed jobs. Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[<xref:dict>, <xref:azure.ai.ml.PyTorchDistribution>,
      <xref:azure.ai.ml.MpiDistribution>, <xref:azure.ai.ml.TensorFlowDistribution>,
      <xref:azure.ai.ml.RayDistribution>]]
  - name: compute
    description: The compute target the job will run on. Defaults to default compute.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: inputs
    description: A mapping of input names to input data sources used in the job. Defaults
      to None.
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:typing.Union>[<xref:azure.ai.ml.Input>,
      <xref:str>, <xref:bool>, <xref:int>, <xref:float>, <xref:Enum>]]]
  - name: outputs
    description: A mapping of output names to output data sources used in the job.
      Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:typing.Union>[<xref:str>,
      <xref:azure.ai.ml.Output>]]]
  - name: instance_count
    description: The number of instances or nodes to be used by the compute target.
      Defaults to 1.
    types:
    - <xref:typing.Optional>[<xref:int>]
  - name: instance_type
    description: The type of VM to be used by the compute target.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: locations
    description: The list of locations where the job will run.
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:str>]]
  - name: docker_args
    description: 'Extra arguments to pass to the Docker run command. This would override
      any

      parameters that have already been set by the system, or in this section. This
      parameter is only

      supported for Azure ML compute types. Defaults to None.'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: shm_size
    description: 'The size of the Docker container''s shared memory block. This should
      be in the

      format of (number)(unit) where the number has to be greater than 0 and the unit
      can be one of

      b(bytes), k(kilobytes), m(megabytes), or g(gigabytes).'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: timeout
    description: The number, in seconds, after which the job will be cancelled.
    types:
    - <xref:typing.Optional>[<xref:int>]
  - name: code
    description: 'The source code to run the job. Can be a local path or "http:",
      "https:", or "azureml:" url

      pointing to a remote location.'
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[<xref:str>, <xref:os.PathLike>]]
  - name: identity
    description: The identity that the command job will use while running on compute.
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[ <xref:azure.ai.ml.entities.ManagedIdentityConfiguration>,
      <xref:azure.ai.ml.entities.AmlTokenConfiguration>, <xref:azure.ai.ml.entities.UserIdentityConfiguration>]]
  - name: is_deterministic
    description: 'Specifies whether the Command will return the same output given
      the same input.

      Defaults to True. When True, if a Command Component is deterministic and has
      been run before in the

      current workspace with the same input and settings, it will reuse results from
      a previously submitted

      job when used as a node or step in a pipeline. In that scenario, no compute
      resources will be used.'
    defaultValue: 'True'
    types:
    - <xref:bool>
  - name: services
    description: 'The interactive services for the node. Defaults to None. This is
      an experimental parameter,

      and may change at any time. Please see [https://aka.ms/azuremlexperimental](https://aka.ms/azuremlexperimental)
      for more information.'
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:typing.Union>[<xref:azure.ai.ml.entities.JobService>,
      <xref:azure.ai.ml.entities.JupyterLabJobService>, <xref:azure.ai.ml.entities.SshJobService>,
      <xref:azure.ai.ml.entities.TensorBoardJobService>, <xref:azure.ai.ml.entities.VsCodeJobService>]]]
  - name: job_tier
    description: The job tier. Accepted values are "Spot", "Basic", "Standard", or
      "Premium".
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: priority
    description: 'The priority of the job on the compute. Accepted values are "low",
      "medium", and "high".

      Defaults to "medium".'
    types:
    - <xref:typing.Optional>[<xref:str>]
  return:
    description: A Command object.
    types:
    - <xref:azure.ai.ml.entities.Command>
  examples:
  - "Creating a Command Job using the command() builder method.<!--[!code-python[Main](les\\\
    ml_samples_command_configurations.py )]-->\n\n<!-- literal_block {\"ids\": [],\
    \ \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\"\
    : \"C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\
    \\py2docfx\\\\dist_temp\\\\10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_command_configurations.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   from azure.ai.ml import Input, Output, command\n\n   train_func = command(\n\
    \       environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:33\",\n       command='echo\
    \ \"hello world\"',\n       distribution={\"type\": \"Pytorch\", \"process_count_per_instance\"\
    : 2},\n       inputs={\n           \"training_data\": Input(type=\"uri_folder\"\
    ),\n           \"max_epochs\": 20,\n           \"learning_rate\": 1.8,\n     \
    \      \"learning_rate_schedule\": \"time-based\",\n       },\n       outputs={\"\
    model_output\": Output(type=\"uri_folder\")},\n   )\n\n   ````\n"
- uid: azure.ai.ml.load_batch_deployment
  name: load_batch_deployment
  summary: Construct a batch deployment object from yaml file.
  signature: 'load_batch_deployment(source: str | PathLike | IO, *, relative_origin:
    str | None = None, **kwargs: Any) -> BatchDeployment'
  parameters:
  - name: source
    description: 'The local yaml source of a batch deployment object. Must be either
      a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed batch deployment object.
    types:
    - <xref:azure.ai.ml.entities.BatchDeployment>
- uid: azure.ai.ml.load_batch_endpoint
  name: load_batch_endpoint
  summary: Construct a batch endpoint object from yaml file.
  signature: 'load_batch_endpoint(source: str | PathLike | IO, relative_origin: str
    | None = None, **kwargs: Any) -> BatchEndpoint'
  parameters:
  - name: source
    description: 'The local yaml source of a batch endpoint object. Must be either
      a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    defaultValue: None
    types:
    - <xref:str>
  keywordOnlyParameters:
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed batch endpoint object.
    types:
    - <xref:azure.ai.ml.entities.BatchEndpoint>
- uid: azure.ai.ml.load_component
  name: load_component
  summary: Load component from local or remote to a component function.
  signature: 'load_component(source: PathLike | str | IO | None = None, *, relative_origin:
    str | None = None, **kwargs: Any) -> CommandComponent | ParallelComponent | PipelineComponent'
  parameters:
  - name: source
    description: 'The local yaml source of a component. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    defaultValue: None
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: A Component object
    types:
    - <xref:typing.Union>[<xref:azure.ai.ml.entities.CommandComponent>, <xref:azure.ai.ml.entities.ParallelComponent>,
      <xref:azure.ai.ml.entities.PipelineComponent>]
  examples:
  - "Loading a Component object from a YAML file, overriding its version to \"1.0.2\"\
    , and\nregistering it remotely.<!--[!code-python[Main](les\\ml_samples_component_configurations.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_component_configurations.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   from azure.ai.ml import load_component\n\n   component = load_component(\n\
    \       source=\"./sdk/ml/azure-ai-ml/tests/test_configs/components/helloworld_component.yml\"\
    ,\n       params_override=[{\"version\": \"1.0.2\"}],\n   )\n   registered_component\
    \ = ml_client.components.create_or_update(component)\n\n   ````\n"
- uid: azure.ai.ml.load_compute
  name: load_compute
  summary: Construct a compute object from a yaml file.
  signature: 'load_compute(source: str | PathLike | IO, *, relative_origin: str |
    None = None, params_override: List[Dict[str, str]] | None = None, **kwargs: Any)
    -> Compute'
  parameters:
  - name: source
    description: 'The local yaml source of a compute. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:typing.Dict>]]
  return:
    description: Loaded compute object.
    types:
    - <xref:azure.ai.ml.entities.Compute>
  examples:
  - "Loading a Compute object from a YAML file and overriding its description.<!--[!code-python[Main](les\\\
    ml_samples_compute.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [],\
    \ \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\
    \\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\
    \\10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_compute.py\", \"xml:space\"\
    : \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
    : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.ml\
    \ import load_compute\n\n   compute = load_compute(\n       \"../tests/test_configs/compute/compute-vm.yaml\"\
    ,\n       params_override=[{\"description\": \"loaded from compute-vm.yaml\"}],\n\
    \   )\n\n   ````\n"
- uid: azure.ai.ml.load_data
  name: load_data
  summary: Construct a data object from yaml file.
  signature: 'load_data(source: str | PathLike | IO, *, relative_origin: str | None
    = None, **kwargs: Any) -> Data'
  parameters:
  - name: source
    description: 'The local yaml source of a data object. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed Data or DataImport object.
    types:
    - <xref:azure.ai.ml.entities.Data>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if Data cannot be successfully validated.

      Details will be provided in the error message.'
- uid: azure.ai.ml.load_datastore
  name: load_datastore
  summary: Construct a datastore object from a yaml file.
  signature: 'load_datastore(source: str | PathLike | IO, *, relative_origin: str
    | None = None, **kwargs: Any) -> Datastore'
  parameters:
  - name: source
    description: 'The local yaml source of a datastore. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Loaded datastore object.
    types:
    - <xref:azure.ai.ml.entities.Datastore>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if Datastore cannot be successfully validated.

      Details will be provided in the error message.'
- uid: azure.ai.ml.load_environment
  name: load_environment
  summary: Construct a environment object from yaml file.
  signature: 'load_environment(source: str | PathLike | IO, *, relative_origin: str
    | None = None, **kwargs: Any) -> Environment'
  parameters:
  - name: source
    description: 'The local yaml source of an environment. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed environment object.
    types:
    - <xref:azure.ai.ml.entities.Environment>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if Environment cannot be successfully validated.

      Details will be provided in the error message.'
- uid: azure.ai.ml.load_feature_set
  name: load_feature_set
  summary: Construct a FeatureSet object from yaml file.
  signature: 'load_feature_set(source: str | PathLike | IO, *, relative_origin: str
    | None = None, **kwargs: Any) -> FeatureSet'
  parameters:
  - name: source
    description: 'The local yaml source of a FeatureSet object. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed FeatureSet object.
    types:
    - <xref:azure.ai.ml.entities.FeatureSet>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if FeatureSet cannot be successfully validated.

      Details will be provided in the error message.'
- uid: azure.ai.ml.load_feature_store
  name: load_feature_store
  summary: Load a feature store object from a yaml file.
  signature: 'load_feature_store(source: str | PathLike | IO, *, relative_origin:
    str | None = None, **kwargs: Any) -> FeatureStore'
  parameters:
  - name: source
    description: 'The local yaml source of a feature store. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Loaded feature store object.
    types:
    - <xref:azure.ai.ml.entities.FeatureStore>
- uid: azure.ai.ml.load_feature_store_entity
  name: load_feature_store_entity
  summary: Construct a FeatureStoreEntity object from yaml file.
  signature: 'load_feature_store_entity(source: str | PathLike | IO, *, relative_origin:
    str | None = None, **kwargs: Any) -> FeatureStoreEntity'
  parameters:
  - name: source
    description: 'The local yaml source of a FeatureStoreEntity object. Must be either
      a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed FeatureStoreEntity object.
    types:
    - <xref:azure.ai.ml.entities.FeatureStoreEntity>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if FeatureStoreEntity cannot be successfully validated.

      Details will be provided in the error message.'
- uid: azure.ai.ml.load_job
  name: load_job
  summary: Constructs a Job object from a YAML file.
  signature: 'load_job(source: str | PathLike | IO, *, relative_origin: str | None
    = None, **kwargs: Any) -> Job'
  parameters:
  - name: source
    description: 'A path to a local YAML file or an already-open file object containing
      a job configuration.

      If the source is a path, it will be opened and read. If the source is an open
      file, the file will be read

      directly.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The root directory for the YAML. This directory will be used as
      the origin for deducing

      the relative locations of files referenced in the parsed YAML. Defaults to the
      same directory as source if

      source is a file or file path input. Defaults to "./" if the source is a stream
      input with no name value.'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: params_override
    description: Parameter fields to overwrite values in the YAML file.
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:dict>]]
  return:
    description: A loaded Job object.
    types:
    - <xref:azure.ai.ml.entities.Job>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if Job cannot be successfully validated.

      Details will be provided in the error message.'
  examples:
  - "Loading a Job from a YAML config file.<!--[!code-python[Main](les\\ml_samples_misc.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_misc.py\", \"xml:space\": \"\
    preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\": {\"\
    linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.ml\
    \ import load_job\n\n   job = load_job(source=\"./sdk/ml/azure-ai-ml/tests/test_configs/command_job/command_job_test_local_env.yml\"\
    )\n\n   ````\n"
- uid: azure.ai.ml.load_model
  name: load_model
  summary: Constructs a Model object from a YAML file.
  signature: 'load_model(source: str | PathLike | IO, *, relative_origin: str | None
    = None, **kwargs: Any) -> Model'
  parameters:
  - name: source
    description: 'A path to a local YAML file or an already-open file object containing
      a job configuration.

      If the source is a path, it will be opened and read. If the source is an open
      file, the file will be read

      directly.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The root directory for the YAML. This directory will be used as
      the origin for deducing

      the relative locations of files referenced in the parsed YAML. Defaults to the
      same directory as source if

      source is a file or file path input. Defaults to "./" if the source is a stream
      input with no name value.'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: params_override
    description: Parameter fields to overwrite values in the YAML file.
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:dict>]]
  return:
    description: A loaded Model object.
    types:
    - <xref:azure.ai.ml.entities.Model>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if Job cannot be successfully validated.

      Details will be provided in the error message.'
  examples:
  - "Loading a Model from a YAML config file, overriding the name and version parameters.<!--[!code-python[Main](les\\\
    ml_samples_misc.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [],\
    \ \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\
    \\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\
    \\10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_misc.py\", \"xml:space\":\
    \ \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
    : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.ml\
    \ import load_model\n\n   model = load_model(\n       source=\"./sdk/ml/azure-ai-ml/tests/test_configs/model/model_with_stage.yml\"\
    ,\n       params_override=[{\"name\": \"new_model_name\"}, {\"version\": \"1\"\
    }],\n   )\n\n   ````\n"
- uid: azure.ai.ml.load_model_package
  name: load_model_package
  summary: '> [!NOTE]

    > This is an experimental method, and may change at any time. Please see [https://aka.ms/azuremlexperimental](https://aka.ms/azuremlexperimental)
    for more information.

    >


    Constructs a ModelPackage object from a YAML file.'
  signature: 'load_model_package(source: str | PathLike | IO, *, relative_origin:
    str | None = None, **kwargs: Any) -> ModelPackage'
  parameters:
  - name: source
    description: 'A path to a local YAML file or an already-open file object containing
      a job configuration.

      If the source is a path, it will be opened and read. If the source is an open
      file, the file will be read

      directly.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The root directory for the YAML. This directory will be used as
      the origin for deducing

      the relative locations of files referenced in the parsed YAML. Defaults to the
      same directory as source if

      source is a file or file path input. Defaults to "./" if the source is a stream
      input with no name value.'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: params_override
    description: Parameter fields to overwrite values in the YAML file.
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:dict>]]
  return:
    description: A loaded ModelPackage object.
    types:
    - <xref:azure.ai.ml.entities.ModelPackage>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if Job cannot be successfully validated.

      Details will be provided in the error message.'
  examples:
  - "Loading a ModelPackage from a YAML config file.<!--[!code-python[Main](les\\\
    ml_samples_misc.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [],\
    \ \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\
    \\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\
    \\10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_misc.py\", \"xml:space\":\
    \ \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
    : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.ml\
    \ import load_model_package\n\n   model_package = load_model_package(\n      \
    \ \"./sdk/ml/azure-ai-ml/tests/test_configs/model_package/model_package_simple.yml\"\
    \n   )\n\n   ````\n"
- uid: azure.ai.ml.load_online_deployment
  name: load_online_deployment
  summary: Construct a online deployment object from yaml file.
  signature: 'load_online_deployment(source: str | PathLike | IO, *, relative_origin:
    str | None = None, **kwargs: Any) -> OnlineDeployment'
  parameters:
  - name: source
    description: 'The local yaml source of an online deployment object. Must be either
      a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed online deployment object.
    types:
    - <xref:azure.ai.ml.entities.OnlineDeployment>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if Online Deployment cannot be successfully validated.

      Details will be provided in the error message.'
- uid: azure.ai.ml.load_online_endpoint
  name: load_online_endpoint
  summary: Construct a online endpoint object from yaml file.
  signature: 'load_online_endpoint(source: str | PathLike | IO, *, relative_origin:
    str | None = None, **kwargs: Any) -> OnlineEndpoint'
  parameters:
  - name: source
    description: 'The local yaml source of an online endpoint object. Must be either
      a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed online endpoint object.
    types:
    - <xref:azure.ai.ml.entities.OnlineEndpoint>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: 'Raised if Online Endpoint cannot be successfully validated.

      Details will be provided in the error message.'
- uid: azure.ai.ml.load_registry
  name: load_registry
  summary: Load a registry object from a yaml file.
  signature: 'load_registry(source: str | PathLike | IO, *, relative_origin: str |
    None = None, **kwargs: Any) -> Registry'
  parameters:
  - name: source
    description: 'The local yaml source of a registry. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Loaded registry object.
    types:
    - <xref:azure.ai.ml.entities.Registry>
- uid: azure.ai.ml.load_workspace
  name: load_workspace
  summary: Load a workspace object from a yaml file.
  signature: 'load_workspace(source: str | PathLike | IO, *, relative_origin: str
    | None = None, **kwargs: Any) -> Workspace'
  parameters:
  - name: source
    description: 'The local yaml source of a workspace. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Loaded workspace object.
    types:
    - <xref:azure.ai.ml.entities.Workspace>
  examples:
  - "Loading a Workspace from a YAML config file.<!--[!code-python[Main](les\\ml_samples_workspace.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\
    \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
    10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_workspace.py\", \"xml:space\"\
    : \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
    : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.ml\
    \ import load_workspace\n\n   ws = load_workspace(\n       \"../tests/test_configs/workspace/workspace_min.yaml\"\
    ,\n       params_override=[{\"description\": \"loaded from workspace_min.yaml\"\
    }],\n   )\n\n   ````\n"
- uid: azure.ai.ml.load_workspace_connection
  name: load_workspace_connection
  summary: Construct a workspace connection object from yaml file.
  signature: 'load_workspace_connection(source: str | PathLike | IO, *, relative_origin:
    str | None = None, **kwargs: Any) -> WorkspaceConnection'
  parameters:
  - name: source
    description: 'The local yaml source of a workspace connection object. Must be
      either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Constructed workspace connection object.
    types:
    - <xref:azure.ai.ml.entities.WorkspaceConnection>
  examples:
  - "Loading a Workspace Connection from a YAML config file.<!--[!code-python[Main](les\\\
    ml_samples_workspace.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
    : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\\
    hostedtoolcache\\\\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\
    \\py2docfx\\\\dist_temp\\\\10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_workspace.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   from azure.ai.ml import load_workspace_connection\n\n   wps_connection =\
    \ load_workspace_connection(\n       source=\"../tests/test_configs/workspace_connection/snowflake_user_pwd.yaml\"\
    \n   )\n\n   ````\n"
- uid: azure.ai.ml.load_workspace_hub
  name: load_workspace_hub
  summary: '> [!NOTE]

    > This is an experimental method, and may change at any time. Please see [https://aka.ms/azuremlexperimental](https://aka.ms/azuremlexperimental)
    for more information.

    >


    Load a WorkspaceHub object from a yaml file.'
  signature: 'load_workspace_hub(source: str | PathLike | IO, *, relative_origin:
    str | None = None, **kwargs: Any) -> WorkspaceHub'
  parameters:
  - name: source
    description: 'The local yaml source of a WorkspaceHub. Must be either a

      path to a local file, or an already-open file.

      If the source is a path, it will be open and read.

      An exception is raised if the file does not exist.

      If the source is an open file, the file will be read directly,

      and an exception is raised if the file is not readable.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:io.TextIOWrapper>]
  keywordOnlyParameters:
  - name: relative_origin
    description: 'The origin to be used when deducing

      the relative locations of files referenced in the parsed yaml.

      Defaults to the inputted source''s directory if it is a file or file path input.

      Defaults to "./" if the source is a stream input with no name value.'
    types:
    - <xref:str>
  - name: params_override
    description: 'Fields to overwrite on top of the yaml file.

      Format is [{"field1": "value1"}, {"field2": "value2"}]'
    types:
    - <xref:typing.List>[<xref:typing.Dict>]
  return:
    description: Loaded WorkspaceHub object.
    types:
    - <xref:azure.ai.ml.entities.WorkspaceHub>
  examples:
  - "Loading a Workspace Hub from a YAML config file.<!--[!code-python[Main](les\\\
    ml_samples_workspace.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
    : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\\
    hostedtoolcache\\\\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\
    \\py2docfx\\\\dist_temp\\\\10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_workspace.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   from azure.ai.ml import load_workspace_hub\n\n   hub = load_workspace_hub(\n\
    \       \"../tests/test_configs/workspace/workspacehub_min.yaml\",\n       params_override=[{\"\
    description\": \"loaded from workspacehub_min.yaml\"}],\n   )\n\n   ````\n"
- uid: azure.ai.ml.spark
  name: spark
  summary: Creates a Spark object which can be used inside a dsl.pipeline function
    or used as a standalone Spark job.
  signature: 'spark(*, experiment_name: str | None = None, name: str | None = None,
    display_name: str | None = None, description: str | None = None, tags: Dict |
    None = None, code: PathLike | str | None = None, entry: Dict[str, str] | SparkJobEntry
    | None = None, py_files: List[str] | None = None, jars: List[str] | None = None,
    files: List[str] | None = None, archives: List[str] | None = None, identity: Dict[str,
    str] | ManagedIdentity | AmlToken | UserIdentity | None = None, driver_cores:
    int | None = None, driver_memory: str | None = None, executor_cores: int | None
    = None, executor_memory: str | None = None, executor_instances: int | None = None,
    dynamic_allocation_enabled: bool | None = None, dynamic_allocation_min_executors:
    int | None = None, dynamic_allocation_max_executors: int | None = None, conf:
    Dict[str, str] | None = None, environment: str | Environment | None = None, inputs:
    Dict | None = None, outputs: Dict | None = None, args: str | None = None, compute:
    str | None = None, resources: Dict | SparkResourceConfiguration | None = None,
    **kwargs: Any) -> Spark'
  keywordOnlyParameters:
  - name: experiment_name
    description: The name of the experiment the job will be created under.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: name
    description: The name of the job.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: display_name
    description: The job display name.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: description
    description: The description of the job. Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: tags
    description: The dictionary of tags for the job. Tags can be added, removed, and
      updated. Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:str>]]
  - name: code
    description: 'The source code to run the job. Can be a local path or "http:",
      "https:", or "azureml:" url

      pointing to a remote location.'
  - name: entry
    description: The file or class entry point.
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[<xref:dict>[<xref:str>, <xref:str>],
      <xref:azure.ai.ml.entities.SparkJobEntry>]]
  - name: py_files
    description: 'The list of .zip, .egg or .py files to place on the PYTHONPATH for
      Python apps.

      Defaults to None.'
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:str>]]
  - name: jars
    description: The list of .JAR files to include on the driver and executor classpaths.
      Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:str>]]
  - name: files
    description: The list of files to be placed in the working directory of each executor.
      Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:str>]]
  - name: archives
    description: 'The list of archives to be extracted into the working directory
      of each executor.

      Defaults to None.'
    types:
    - <xref:typing.Optional>[<xref:typing.List>[<xref:str>]]
  - name: identity
    description: The identity that the Spark job will use while running on compute.
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[ <xref:dict>[<xref:str>, <xref:str>],
      <xref:azure.ai.ml.entities.ManagedIdentityConfiguration>, <xref:azure.ai.ml.entities.AmlTokenConfiguration>,
      <xref:azure.ai.ml.entities.UserIdentityConfiguration>]]
  - name: driver_cores
    description: The number of cores to use for the driver process, only in cluster
      mode.
    types:
    - <xref:typing.Optional>[<xref:int>]
  - name: driver_memory
    description: 'The amount of memory to use for the driver process, formatted as
      strings with a size unit

      suffix ("k", "m", "g" or "t") (e.g. "512m", "2g").'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: executor_cores
    description: The number of cores to use on each executor.
    types:
    - <xref:typing.Optional>[<xref:int>]
  - name: executor_memory
    description: 'The amount of memory to use per executor process, formatted as strings
      with a size unit

      suffix ("k", "m", "g" or "t") (e.g. "512m", "2g").'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: executor_instances
    description: The initial number of executors.
    types:
    - <xref:typing.Optional>[<xref:int>]
  - name: dynamic_allocation_enabled
    description: 'Whether to use dynamic resource allocation, which scales the number
      of

      executors registered with this application up and down based on the workload.'
    types:
    - <xref:typing.Optional>[<xref:bool>]
  - name: dynamic_allocation_min_executors
    description: 'The lower bound for the number of executors if dynamic allocation
      is

      enabled.'
    types:
    - <xref:typing.Optional>[<xref:int>]
  - name: dynamic_allocation_max_executors
    description: 'The upper bound for the number of executors if dynamic allocation
      is

      enabled.'
    types:
    - <xref:typing.Optional>[<xref:int>]
  - name: conf
    description: A dictionary with pre-defined Spark configurations key and values.
      Defaults to None.
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:str>]]
  - name: environment
    description: The Azure ML environment to run the job in.
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[<xref:str>, <xref:azure.ai.ml.entities.Environment>]]
  - name: inputs
    description: A mapping of input names to input data used in the job. Defaults
      to None.
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:azure.ai.ml.Input>]]
  - name: outputs
    description: A mapping of output names to output data used in the job. Defaults
      to None.
    types:
    - <xref:typing.Optional>[<xref:dict>[<xref:str>, <xref:azure.ai.ml.Output>]]
  - name: args
    description: The arguments for the job.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: compute
    description: The compute resource the job runs on.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: resources
    description: The compute resource configuration for the job.
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[<xref:dict>, <xref:azure.ai.ml.entities.SparkResourceConfiguration>]]
  return:
    description: A Spark object.
    types:
    - <xref:azure.ai.ml.entities.Spark>
  examples:
  - "Building a Spark pipeline using the DSL pipeline decorator<!--[!code-python[Main](les\\\
    ml_samples_spark_configurations.py )]-->\n\n<!-- literal_block {\"ids\": [], \"\
    classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\":\
    \ \"C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\
    \\py2docfx\\\\dist_temp\\\\10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_spark_configurations.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   from azure.ai.ml import Input, Output, dsl, spark\n   from azure.ai.ml.constants\
    \ import AssetTypes, InputOutputModes\n\n   # define the spark task\n   first_step\
    \ = spark(\n       code=\"/src\",\n       entry={\"file\": \"add_greeting_column.py\"\
    },\n       py_files=[\"utils.zip\"],\n       files=[\"my_files.txt\"],\n     \
    \  driver_cores=2,\n       driver_memory=\"1g\",\n       executor_cores=1,\n \
    \      executor_memory=\"1g\",\n       executor_instances=1,\n       inputs=dict(\n\
    \           file_input=Input(path=\"/dataset/iris.csv\", type=AssetTypes.URI_FILE,\
    \ mode=InputOutputModes.DIRECT)\n       ),\n       args=\"--file_input ${{inputs.file_input}}\"\
    ,\n       resources={\"instance_type\": \"standard_e4s_v3\", \"runtime_version\"\
    : \"3.2.0\"},\n   )\n\n   second_step = spark(\n       code=\"/src\",\n      \
    \ entry={\"file\": \"count_by_row.py\"},\n       jars=[\"scala_project.jar\"],\n\
    \       files=[\"my_files.txt\"],\n       driver_cores=2,\n       driver_memory=\"\
    1g\",\n       executor_cores=1,\n       executor_memory=\"1g\",\n       executor_instances=1,\n\
    \       inputs=dict(\n           file_input=Input(path=\"/dataset/iris.csv\",\
    \ type=AssetTypes.URI_FILE, mode=InputOutputModes.DIRECT)\n       ),\n       outputs=dict(output=Output(type=\"\
    uri_folder\", mode=InputOutputModes.DIRECT)),\n       args=\"--file_input ${{inputs.file_input}}\
    \ --output ${{outputs.output}}\",\n       resources={\"instance_type\": \"standard_e4s_v3\"\
    , \"runtime_version\": \"3.2.0\"},\n   )\n\n   # Define pipeline\n   @dsl.pipeline(description=\"\
    submit a pipeline with spark job\")\n   def spark_pipeline_from_builder(data):\n\
    \       add_greeting_column = first_step(file_input=data)\n       count_by_row\
    \ = second_step(file_input=data)\n       return {\"output\": count_by_row.outputs.output}\n\
    \n   pipeline = spark_pipeline_from_builder(\n       data=Input(path=\"/dataset/iris.csv\"\
    , type=AssetTypes.URI_FILE, mode=InputOutputModes.DIRECT),\n   )\n\n   ````\n"
classes:
- azure.ai.ml.Input
- azure.ai.ml.MLClient
- azure.ai.ml.MpiDistribution
- azure.ai.ml.Output
- azure.ai.ml.PyTorchDistribution
- azure.ai.ml.RayDistribution
- azure.ai.ml.TensorFlowDistribution
packages:
- azure.ai.ml.automl
- azure.ai.ml.constants
- azure.ai.ml.data_transfer
- azure.ai.ml.dsl
- azure.ai.ml.entities
- azure.ai.ml.identity
- azure.ai.ml.operations
- azure.ai.ml.parallel
- azure.ai.ml.sweep
modules:
- azure.ai.ml.exceptions
