### YamlMime:PythonClass
uid: azure.ai.ml.entities.Spark
name: Spark
fullName: azure.ai.ml.entities.Spark
module: azure.ai.ml.entities
inheritances:
- azure.ai.ml.entities._builders.base_node.BaseNode
- azure.ai.ml.entities._job.spark_job_entry_mixin.SparkJobEntryMixin
summary: 'Base class for spark node, used for spark component version consumption.


  You should not instantiate this class directly. Instead, you should

  create it from the builder function: spark.


  ]


  ]]'
constructor:
  syntax: 'Spark(*, component: str | SparkComponent, identity: Dict | ManagedIdentityConfiguration
    | AmlTokenConfiguration | UserIdentityConfiguration | None = None, driver_cores:
    int | str | None = None, driver_memory: str | None = None, executor_cores: int
    | str | None = None, executor_memory: str | None = None, executor_instances: int
    | str | None = None, dynamic_allocation_enabled: bool | str | None = None, dynamic_allocation_min_executors:
    int | str | None = None, dynamic_allocation_max_executors: int | str | None =
    None, conf: Dict[str, str] | None = None, inputs: Dict[str, NodeOutput | Input
    | str | bool | int | float | Enum] | None = None, outputs: Dict[str, str | Output]
    | None = None, compute: str | None = None, resources: Dict | SparkResourceConfiguration
    | None = None, entry: Dict[str, str] | SparkJobEntry | None = None, py_files:
    List[str] | None = None, jars: List[str] | None = None, files: List[str] | None
    = None, archives: List[str] | None = None, args: str | None = None, **kwargs:
    Any)'
  parameters:
  - name: component
    description: The ID or instance of the Spark component or job to be run during
      the step.
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:str>, <xref:azure.ai.ml.entities.SparkComponent>]
  - name: identity
    description: The identity that the Spark job will use while running on compute.
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:typing.Dict>[<xref:str>, <xref:str>], <xref:azure.ai.ml.entities.ManagedIdentityConfiguration>,
      <xref:azure.ai.ml.entities.AmlTokenConfiguration>, <xref:azure.ai.ml.entities.UserIdentityConfiguration>
  - name: driver_cores
    description: The number of cores to use for the driver process, only in cluster
      mode.
    isRequired: true
    types:
    - <xref:int>
  - name: driver_memory
    description: 'The amount of memory to use for the driver process, formatted as
      strings with a size unit

      suffix ("k", "m", "g" or "t") (e.g. "512m", "2g").'
    isRequired: true
    types:
    - <xref:str>
  - name: executor_cores
    description: The number of cores to use on each executor.
    isRequired: true
    types:
    - <xref:int>
  - name: executor_memory
    description: 'The amount of memory to use per executor process, formatted as strings
      with a size unit

      suffix ("k", "m", "g" or "t") (e.g. "512m", "2g").'
    isRequired: true
    types:
    - <xref:str>
  - name: executor_instances
    description: The initial number of executors.
    isRequired: true
    types:
    - <xref:int>
  - name: dynamic_allocation_enabled
    description: 'Whether to use dynamic resource allocation, which scales the number
      of

      executors registered with this application up and down based on the workload.'
    isRequired: true
    types:
    - <xref:bool>
  - name: dynamic_allocation_min_executors
    description: 'The lower bound for the number of executors if dynamic allocation

      is enabled.'
    isRequired: true
    types:
    - <xref:int>
  - name: dynamic_allocation_max_executors
    description: 'The upper bound for the number of executors if dynamic allocation

      is enabled.'
    isRequired: true
    types:
    - <xref:int>
  - name: conf
    description: A dictionary with pre-defined Spark configurations key and values.
    isRequired: true
    types:
    - <xref:typing.Dict>[<xref:str>, <xref:str>]
  - name: inputs
    description: A mapping of input names to input data sources used in the job.
    isRequired: true
    types:
    - <xref:typing.Dict>[<xref:str>, <xref:typing.Union>[ <xref:str>, <xref:bool>,
      <xref:int>, <xref:float>, <xref:Enum>, <xref:azure.ai.ml.entities._job.pipeline._io.NodeOutput>,
      <xref:azure.ai.ml.Input>
  - name: outputs
    description: A mapping of output names to output data sources used in the job.
    isRequired: true
    types:
    - <xref:typing.Dict>[<xref:str>, <xref:typing.Union>[<xref:str>, <xref:azure.ai.ml.Output>]]
  - name: args
    description: The arguments for the job.
    isRequired: true
    types:
    - <xref:str>
  - name: compute
    description: The compute resource the job runs on.
    isRequired: true
    types:
    - <xref:str>
  - name: resources
    description: The compute resource configuration for the job.
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:typing.Dict>, <xref:azure.ai.ml.entities.SparkResourceConfiguration>]
  - name: entry
    description: The file or class entry point.
    isRequired: true
    types:
    - <xref:typing.Dict>[<xref:str>, <xref:str>]
  - name: py_files
    description: The list of .zip, .egg or .py files to place on the PYTHONPATH for
      Python apps.
    isRequired: true
    types:
    - <xref:typing.List>[<xref:str>]
  - name: jars
    description: The list of .JAR files to include on the driver and executor classpaths.
    isRequired: true
    types:
    - <xref:typing.List>[<xref:str>]
  - name: files
    description: The list of files to be placed in the working directory of each executor.
    isRequired: true
    types:
    - <xref:typing.List>[<xref:str>]
  - name: archives
    description: The list of archives to be extracted into the working directory of
      each executor.
    isRequired: true
    types:
    - <xref:typing.List>[<xref:str>]
  keywordOnlyParameters:
  - name: component
    isRequired: true
  - name: identity
    isRequired: true
  - name: driver_cores
    isRequired: true
  - name: driver_memory
    isRequired: true
  - name: executor_cores
    isRequired: true
  - name: executor_memory
    isRequired: true
  - name: executor_instances
    isRequired: true
  - name: dynamic_allocation_enabled
    isRequired: true
  - name: dynamic_allocation_min_executors
    isRequired: true
  - name: dynamic_allocation_max_executors
    isRequired: true
  - name: conf
    isRequired: true
  - name: inputs
    isRequired: true
  - name: outputs
    isRequired: true
  - name: compute
    isRequired: true
  - name: resources
    isRequired: true
  - name: entry
    isRequired: true
  - name: py_files
    isRequired: true
  - name: jars
    isRequired: true
  - name: files
    isRequired: true
  - name: archives
    isRequired: true
  - name: args
    isRequired: true
methods:
- uid: azure.ai.ml.entities.Spark.clear
  name: clear
  signature: clear() -> None.  Remove all items from D.
- uid: azure.ai.ml.entities.Spark.copy
  name: copy
  signature: copy() -> a shallow copy of D
- uid: azure.ai.ml.entities.Spark.dump
  name: dump
  summary: Dumps the job content into a file in YAML format.
  signature: 'dump(dest: str | PathLike | IO, **kwargs: Any) -> None'
  parameters:
  - name: dest
    description: 'The local path or file stream to write the YAML content to.

      If dest is a file path, a new file will be created.

      If dest is an open file, the file will be written to directly.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:PathLike>, <xref:str>, <xref:typing.IO>[<xref:typing.AnyStr>]]
  keywordOnlyParameters:
  - name: kwargs
    description: Additional arguments to pass to the YAML serializer.
    types:
    - <xref:dict>
  exceptions:
  - type: FileExistsError
    description: Raised if dest is a file path and the file already exists.
  - type: IOError
    description: Raised if dest is an open file and the file is not writable.
- uid: azure.ai.ml.entities.Spark.fromkeys
  name: fromkeys
  summary: Create a new dictionary with keys from iterable and values set to value.
  signature: fromkeys(value=None, /)
  positionalOnlyParameters:
  - name: iterable
    isRequired: true
  - name: value
    defaultValue: None
  parameters:
  - name: type
    isRequired: true
- uid: azure.ai.ml.entities.Spark.get
  name: get
  summary: Return the value for key if key is in the dictionary, else default.
  signature: get(key, default=None, /)
  positionalOnlyParameters:
  - name: key
    isRequired: true
  - name: default
    defaultValue: None
- uid: azure.ai.ml.entities.Spark.items
  name: items
  signature: items() -> a set-like object providing a view on D's items
- uid: azure.ai.ml.entities.Spark.keys
  name: keys
  signature: keys() -> a set-like object providing a view on D's keys
- uid: azure.ai.ml.entities.Spark.pop
  name: pop
  summary: 'If the key is not found, return the default if given; otherwise,

    raise a KeyError.'
  signature: pop(k, [d]) -> v, remove specified key and return the corresponding value.
- uid: azure.ai.ml.entities.Spark.popitem
  name: popitem
  summary: 'Remove and return a (key, value) pair as a 2-tuple.


    Pairs are returned in LIFO (last-in, first-out) order.

    Raises KeyError if the dict is empty.'
  signature: popitem()
- uid: azure.ai.ml.entities.Spark.setdefault
  name: setdefault
  summary: 'Insert key with a value of default if key is not in the dictionary.


    Return the value for key if key is in the dictionary, else default.'
  signature: setdefault(key, default=None, /)
  positionalOnlyParameters:
  - name: key
    isRequired: true
  - name: default
    defaultValue: None
- uid: azure.ai.ml.entities.Spark.update
  name: update
  summary: 'If E is present and has a .keys() method, then does:  for k in E: D[k]
    = E[k]

    If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] =
    v

    In either case, this is followed by: for k in F:  D[k] = F[k]'
  signature: update([E], **F) -> None.  Update D from dict/iterable E and F.
- uid: azure.ai.ml.entities.Spark.values
  name: values
  signature: values() -> an object providing a view on D's values
attributes:
- uid: azure.ai.ml.entities.Spark.base_path
  name: base_path
  summary: The base path of the resource.
  return:
    description: The base path of the resource.
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.Spark.code
  name: code
  summary: The local or remote path pointing at source code.
  return:
    types:
    - <xref:typing.Union>[<xref:str>, <xref:PathLike>]
- uid: azure.ai.ml.entities.Spark.component
  name: component
  summary: The ID or instance of the Spark component or job to be run during the step.
  return:
    types:
    - <xref:azure.ai.ml.entities.SparkComponent>
- uid: azure.ai.ml.entities.Spark.creation_context
  name: creation_context
  summary: The creation context of the resource.
  return:
    description: The creation metadata for the resource.
    types:
    - <xref:typing.Optional>[<xref:azure.ai.ml.entities.SystemData>]
- uid: azure.ai.ml.entities.Spark.entry
  name: entry
- uid: azure.ai.ml.entities.Spark.id
  name: id
  summary: The resource ID.
  return:
    description: The global ID of the resource, an Azure Resource Manager (ARM) ID.
    types:
    - <xref:typing.Optional>[<xref:str>]
- uid: azure.ai.ml.entities.Spark.identity
  name: identity
  summary: The identity that the Spark job will use while running on compute.
  return:
    types:
    - <xref:typing.Union>[<xref:azure.ai.ml.entities.ManagedIdentityConfiguration>,
      <xref:azure.ai.ml.entities.AmlTokenConfiguration>, <xref:azure.ai.ml.entities.UserIdentityConfiguration>]
- uid: azure.ai.ml.entities.Spark.inputs
  name: inputs
  summary: Get the inputs for the object.
  return:
    description: A dictionary containing the inputs for the object.
    types:
    - <xref:typing.Dict>[<xref:str>, <xref:typing.Union>[<xref:azure.ai.ml.Input>,
      <xref:str>, <xref:bool>, <xref:int>, <xref:float>]]
- uid: azure.ai.ml.entities.Spark.log_files
  name: log_files
  summary: Job output files.
  return:
    description: The dictionary of log names and URLs.
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>[<xref:str>, <xref:str>]]
- uid: azure.ai.ml.entities.Spark.name
  name: name
  summary: Get the name of the node.
  return:
    description: The name of the node.
    types:
    - <xref:str>
- uid: azure.ai.ml.entities.Spark.outputs
  name: outputs
  summary: Get the outputs of the object.
  return:
    description: A dictionary containing the outputs for the object.
    types:
    - <xref:typing.Dict>[<xref:str>, <xref:typing.Union>[<xref:str>, <xref:azure.ai.ml.Output>]]
- uid: azure.ai.ml.entities.Spark.resources
  name: resources
  summary: The compute resource configuration for the job.
  return:
    types:
    - <xref:azure.ai.ml.entities.SparkResourceConfiguration>
- uid: azure.ai.ml.entities.Spark.status
  name: status
  summary: "The status of the job.\n\nCommon values returned include \"Running\",\
    \ \"Completed\", and \"Failed\". All possible values are:\n\n   * NotStarted -\
    \ This is a temporary state that client-side Run objects are in before cloud submission.\
    \ \n\n   * Starting - The Run has started being processed in the cloud. The caller\
    \ has a run ID at this point. \n\n   * Provisioning - On-demand compute is being\
    \ created for a given job submission. \n\n   * Preparing - The run environment\
    \ is being prepared and is in one of two stages:\n\n        * Docker image build\
    \ \n\n        * conda environment setup \n\n   * Queued - The job is queued on\
    \ the compute target. For example, in BatchAI, the job is in a queued state\n\n\
    \        while waiting for all the requested nodes to be ready.\n\n   * Running\
    \ - The job has started to run on the compute target. \n\n   * Finalizing - User\
    \ code execution has completed, and the run is in post-processing stages. \n\n\
    \   * CancelRequested - Cancellation has been requested for the job. \n\n   *\
    \ Completed - The run has completed successfully. This includes both the user\
    \ code execution and run\n\n        post-processing stages.\n\n   * Failed - The\
    \ run failed. Usually the Error property on a run will provide details as to why.\
    \ \n\n   * Canceled - Follows a cancellation request and indicates that the run\
    \ is now successfully cancelled. \n\n   * NotResponding - For runs that have Heartbeats\
    \ enabled, no heartbeat has been recently sent."
  return:
    description: Status of the job.
    types:
    - <xref:typing.Optional>[<xref:str>]
- uid: azure.ai.ml.entities.Spark.studio_url
  name: studio_url
  summary: Azure ML studio endpoint.
  return:
    description: The URL to the job details page.
    types:
    - <xref:typing.Optional>[<xref:str>]
- uid: azure.ai.ml.entities.Spark.type
  name: type
  summary: The type of the job.
  return:
    description: The type of the job.
    types:
    - <xref:typing.Optional>[<xref:str>]
- uid: azure.ai.ml.entities.Spark.CODE_ID_RE_PATTERN
  name: CODE_ID_RE_PATTERN
  signature: CODE_ID_RE_PATTERN = re.compile('\\/subscriptions\\/(?P<subscription>[\\w,-]+)\\/resourceGroups\\/(?P<resource_group>[\\w,-]+)\\/providers\\/Microsoft\\.MachineLearningServices\\/workspaces\\/(?P<workspace>[\\w,-]+)\\/codes\\/(?P<co)
