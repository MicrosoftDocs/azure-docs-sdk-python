### YamlMime:PythonClass
uid: azure.ai.ml.entities.SparkResourceConfiguration
name: SparkResourceConfiguration
fullName: azure.ai.ml.entities.SparkResourceConfiguration
module: azure.ai.ml.entities
inheritances:
- azure.ai.ml.entities._mixins.RestTranslatableMixin
- azure.ai.ml.entities._mixins.DictMixin
summary: Compute resource configuration for Spark component or job.
constructor:
  syntax: 'SparkResourceConfiguration(*, instance_type: str | None = None, runtime_version:
    str | None = None)'
  keywordOnlyParameters:
  - name: instance_type
    description: The type of VM to be used by the compute target.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: runtime_version
    description: The Spark runtime version.
    types:
    - <xref:typing.Optional>[<xref:str>]
examples:
- "Configuring a SparkJob with SparkResourceConfiguration.<!--[!code-python[Main](les\\\
  ml_samples_spark_configurations.py )]-->\n\n<!-- literal_block {\"ids\": [], \"\
  classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"\
  C:\\\\hostedtoolcache\\\\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\
  \\py2docfx\\\\dist_temp\\\\10\\\\azure-ai-ml-1.15.0\\\\samples\\\\ml_samples_spark_configurations.py\"\
  , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.ml\
  \ import Input, Output\n   from azure.ai.ml.entities._credentials import AmlTokenConfiguration,\
  \ SparkJob, SparkResourceConfiguration\n\n   spark_job = SparkJob(\n       code=\"\
  ./tests/test_configs/spark_job/basic_spark_job/src\",\n       entry={\"file\": \"\
  ./main.py\"},\n       jars=[\"simple-1.1.1.jar\"],\n       identity=AmlTokenConfiguration(),\n\
  \       driver_cores=1,\n       driver_memory=\"2g\",\n       executor_cores=2,\n\
  \       executor_memory=\"2g\",\n       executor_instances=2,\n       dynamic_allocation_enabled=True,\n\
  \       dynamic_allocation_min_executors=1,\n       dynamic_allocation_max_executors=3,\n\
  \       name=\"builder-spark-job\",\n       experiment_name=\"builder-spark-experiment-name\"\
  ,\n       environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:33\",\n       inputs={\n\
  \           \"input1\": Input(\n               type=\"uri_file\", path=\"azureml://datastores/workspaceblobstore/paths/python/data.csv\"\
  , mode=\"direct\"\n           )\n       },\n       outputs={\n           \"output1\"\
  : Output(\n               type=\"uri_file\",\n               path=\"azureml://datastores/workspaceblobstore/spark_titanic_output/titanic.parquet\"\
  ,\n               mode=\"direct\",\n           )\n       },\n       resources=SparkResourceConfiguration(instance_type=\"\
  Standard_E8S_V3\", runtime_version=\"3.2.0\"),\n   )\n\n   ````\n"
methods:
- uid: azure.ai.ml.entities.SparkResourceConfiguration.get
  name: get
  signature: 'get(key: Any, default: Any | None = None) -> Any'
  parameters:
  - name: key
    isRequired: true
  - name: default
    defaultValue: None
- uid: azure.ai.ml.entities.SparkResourceConfiguration.has_key
  name: has_key
  signature: 'has_key(k: Any) -> bool'
  parameters:
  - name: k
    isRequired: true
- uid: azure.ai.ml.entities.SparkResourceConfiguration.items
  name: items
  signature: items() -> list
- uid: azure.ai.ml.entities.SparkResourceConfiguration.keys
  name: keys
  signature: keys() -> list
- uid: azure.ai.ml.entities.SparkResourceConfiguration.update
  name: update
  signature: 'update(*args: Any, **kwargs: Any) -> None'
- uid: azure.ai.ml.entities.SparkResourceConfiguration.values
  name: values
  signature: values() -> list
attributes:
- uid: azure.ai.ml.entities.SparkResourceConfiguration.instance_type_list
  name: instance_type_list
  signature: instance_type_list = ['standard_e4s_v3', 'standard_e8s_v3', 'standard_e16s_v3',
    'standard_e32s_v3', 'standard_e64s_v3']
