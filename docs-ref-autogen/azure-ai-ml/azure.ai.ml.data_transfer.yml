### YamlMime:PythonPackage
uid: azure.ai.ml.data_transfer
name: data_transfer
fullName: azure.ai.ml.data_transfer
type: import
functions:
- uid: azure.ai.ml.data_transfer.copy_data
  name: copy_data
  summary: '> [!NOTE]

    > This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental
    for more information.

    >


    Create a DataTransferCopy object which can be used inside dsl.pipeline as a function.'
  signature: 'copy_data(*, name: str | None = None, description: str | None = None,
    tags: Dict | None = None, display_name: str | None = None, experiment_name: str
    | None = None, compute: str | None = None, inputs: Dict | None = None, outputs:
    Dict | None = None, is_deterministic: bool = True, data_copy_mode: str | None
    = None, **kwargs) -> DataTransferCopy'
  parameters:
  - name: name
    description: The name of the job.
    types:
    - <xref:str>
  - name: description
    description: Description of the job.
    types:
    - <xref:str>
  - name: tags
    description: Tag dictionary. Tags can be added, removed, and updated.
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: display_name
    description: Display name of the job.
    types:
    - <xref:str>
  - name: experiment_name
    description: Name of the experiment the job will be created under.
    types:
    - <xref:str>
  - name: compute
    description: The compute resource the job runs on.
    types:
    - <xref:str>
  - name: inputs
    description: Mapping of inputs data bindings used in the job.
    types:
    - <xref:dict>
  - name: outputs
    description: Mapping of outputs data bindings used in the job.
    types:
    - <xref:dict>
  - name: is_deterministic
    description: 'Specify whether the command will return same output given same input.

      If a command (component) is deterministic, when use it as a node/step in a pipeline,

      it will reuse results from a previous submitted job in current workspace which
      has same inputs and settings.

      In this case, this step will not use any compute resource.

      Default to be True, specify is_deterministic=False if you would like to avoid
      such reuse behavior.'
    types:
    - <xref:bool>
  - name: data_copy_mode
    description: data copy mode in copy task, possible value is "merge_with_overwrite",
      "fail_if_conflict".
    types:
    - <xref:str>
  return:
    description: A DataTransferCopy object.
    types:
    - <xref:azure.ai.ml.entities._component.datatransfer_component.DataTransferCopyComponent>
- uid: azure.ai.ml.data_transfer.export_data
  name: export_data
  summary: '> [!NOTE]

    > This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental
    for more information.

    >


    Create a DataTransferExport object which can be used inside dsl.pipeline.'
  signature: 'export_data(*, name: str | None = None, description: str | None = None,
    tags: Dict | None = None, display_name: str | None = None, experiment_name: str
    | None = None, compute: str | None = None, sink: Dict | Database | FileSystem
    | None = None, inputs: Dict | None = None, **kwargs) -> DataTransferExport'
  parameters:
  - name: name
    description: The name of the job.
    types:
    - <xref:str>
  - name: description
    description: Description of the job.
    types:
    - <xref:str>
  - name: tags
    description: Tag dictionary. Tags can be added, removed, and updated.
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: display_name
    description: Display name of the job.
    types:
    - <xref:str>
  - name: experiment_name
    description: Name of the experiment the job will be created under.
    types:
    - <xref:str>
  - name: compute
    description: The compute resource the job runs on.
    types:
    - <xref:str>
  - name: sink
    description: The sink of external data and databases.
    types:
    - <xref:typing.Union>[ <xref:typing.Dict>, <xref:azure.ai.ml.data_transfer.Database>,
      <xref:azure.ai.ml.data_transfer.FileSystem>]
  - name: inputs
    description: Mapping of inputs data bindings used in the job.
    types:
    - <xref:dict>
  return:
    description: A DataTransferExport object.
    types:
    - <xref:azure.ai.ml.entities._job.pipeline._component_translatable.DataTransferExport>
  exceptions:
  - type: azure.ai.ml.exceptions.ValidationException
    description: If sink is not provided or exporting file system is not supported.
- uid: azure.ai.ml.data_transfer.import_data
  name: import_data
  summary: '> [!NOTE]

    > This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental
    for more information.

    >


    Create a DataTransferImport object which can be used inside dsl.pipeline.'
  signature: 'import_data(*, name: str | None = None, description: str | None = None,
    tags: Dict | None = None, display_name: str | None = None, experiment_name: str
    | None = None, compute: str | None = None, source: Dict | Database | FileSystem
    | None = None, outputs: Dict | None = None, **kwargs) -> DataTransferImport'
  parameters:
  - name: name
    description: The name of the job.
    types:
    - <xref:str>
  - name: description
    description: Description of the job.
    types:
    - <xref:str>
  - name: tags
    description: Tag dictionary. Tags can be added, removed, and updated.
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: display_name
    description: Display name of the job.
    types:
    - <xref:str>
  - name: experiment_name
    description: Name of the experiment the job will be created under.
    types:
    - <xref:str>
  - name: compute
    description: The compute resource the job runs on.
    types:
    - <xref:str>
  - name: source
    description: The data source of file system or database.
    types:
    - <xref:typing.Union>[<xref:typing.Dict>, <xref:azure.ai.ml.data_transfer.Database>,
      <xref:azure.ai.ml.data_transfer.FileSystem>]
  - name: outputs
    description: 'Mapping of outputs data bindings used in the job.

      The default will be an output port with the key "sink" and type "mltable".'
    types:
    - <xref:dict>
  return:
    description: A DataTransferImport object.
    types:
    - <xref:azure.ai.ml.entities._job.pipeline._component_translatable.DataTransferImport>
classes:
- azure.ai.ml.data_transfer.DataTransferCopy
- azure.ai.ml.data_transfer.DataTransferCopyComponent
- azure.ai.ml.data_transfer.DataTransferExport
- azure.ai.ml.data_transfer.DataTransferExportComponent
- azure.ai.ml.data_transfer.DataTransferImport
- azure.ai.ml.data_transfer.DataTransferImportComponent
- azure.ai.ml.data_transfer.Database
- azure.ai.ml.data_transfer.FileSystem
