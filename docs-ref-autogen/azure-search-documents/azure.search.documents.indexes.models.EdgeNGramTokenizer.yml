### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: azure.search.documents.indexes.models.EdgeNGramTokenizer
  fullName: azure.search.documents.indexes.models.EdgeNGramTokenizer
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: msrest.serialization.Model
    type: azure.search.documents.indexes._internal._generated.models._models_py3.LexicalTokenizer
  langs:
  - python
  module: azure.search.documents.indexes.models
  name: EdgeNGramTokenizer
  summary: 'Tokenizes the input from an edge into n-grams of the given size(s). This
    tokenizer is implemented using Apache Lucene.


    All required parameters must be populated in order to send to Azure.'
  syntax:
    content: 'EdgeNGramTokenizer(*, name: str, min_gram: typing.Union[int, NoneType]
      = 1, max_gram: typing.Union[int, NoneType] = 2, token_chars: typing.Union[typing.List[typing.Union[str,
      _ForwardRef(''TokenCharacterKind'')]], NoneType] = None, **kwargs)'
    parameters:
    - description: 'Required. Identifies the concrete type of the tokenizer.Constant
        filled by

        server.'
      id: odata_type
      type:
      - str
    - description: 'Required. The name of the tokenizer. It must only contain letters,
        digits, spaces,

        dashes or underscores, can only start and end with alphanumeric characters,
        and is limited to

        128 characters.'
      id: name
      type:
      - str
    - description: 'The minimum n-gram length. Default is 1. Maximum is 300. Must
        be less than the

        value of maxGram.'
      id: min_gram
      type:
      - int
    - description: The maximum n-gram length. Default is 2. Maximum is 300.
      id: max_gram
      type:
      - int
    - description: Character classes to keep in the tokens.
      id: token_chars
      type:
      - list[str
      - azure.search.documents.indexes.models.TokenCharacterKind]
  type: class
  uid: azure.search.documents.indexes.models.EdgeNGramTokenizer
references:
- fullName: list[str
  name: list[str
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  uid: list[str
- fullName: azure.search.documents.indexes.models.TokenCharacterKind]
  name: TokenCharacterKind]
  spec.python:
  - fullName: azure.search.documents.indexes.models.TokenCharacterKind
    name: TokenCharacterKind
    uid: azure.search.documents.indexes.models.TokenCharacterKind
  - fullName: ']'
    name: ']'
  uid: azure.search.documents.indexes.models.TokenCharacterKind]
