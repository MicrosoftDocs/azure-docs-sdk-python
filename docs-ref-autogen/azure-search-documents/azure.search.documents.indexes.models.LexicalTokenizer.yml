### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.LexicalTokenizer
name: LexicalTokenizer
fullName: azure.search.documents.indexes.models.LexicalTokenizer
module: azure.search.documents.indexes.models
inheritances:
- msrest.serialization.Model
summary: 'Base type for tokenizers.


  You probably want to use the sub-classes and not this class directly. Known

  sub-classes are: ClassicTokenizer, EdgeNGramTokenizer, KeywordTokenizer, KeywordTokenizerV2,
  MicrosoftLanguageStemmingTokenizer, MicrosoftLanguageTokenizer, NGramTokenizer,
  PathHierarchyTokenizerV2, PatternTokenizer, LuceneStandardTokenizer, LuceneStandardTokenizerV2,
  UaxUrlEmailTokenizer.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'LexicalTokenizer(*, name: str, **kwargs)'
  parameters:
  - name: odata_type
    description: 'Required. Identifies the concrete type of the tokenizer.Constant
      filled by

      server.'
    isRequired: true
    types:
    - <xref:str>
  - name: name
    description: 'Required. The name of the tokenizer. It must only contain letters,
      digits, spaces,

      dashes or underscores, can only start and end with alphanumeric characters,
      and is limited to

      128 characters.'
    isRequired: true
    types:
    - <xref:str>
