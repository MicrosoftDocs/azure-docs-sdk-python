### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: azure.search.documents.indexes.models.LexicalTokenizer
  fullName: azure.search.documents.indexes.models.LexicalTokenizer
  inheritance:
  - inheritance:
    - type: builtins.object
    type: msrest.serialization.Model
  langs:
  - python
  module: azure.search.documents.indexes.models
  name: LexicalTokenizer
  summary: 'Base type for tokenizers.


    You probably want to use the sub-classes and not this class directly. Known

    sub-classes are: ClassicTokenizer, EdgeNGramTokenizer, KeywordTokenizer, KeywordTokenizerV2,
    MicrosoftLanguageStemmingTokenizer, MicrosoftLanguageTokenizer, NGramTokenizer,
    PathHierarchyTokenizerV2, PatternTokenizer, LuceneStandardTokenizer, LuceneStandardTokenizerV2,
    UaxUrlEmailTokenizer.


    All required parameters must be populated in order to send to Azure.'
  syntax:
    content: 'LexicalTokenizer(*, name: str, **kwargs)'
    parameters:
    - description: 'Required. Identifies the concrete type of the tokenizer.Constant
        filled by

        server.'
      id: odata_type
      type:
      - str
    - description: 'Required. The name of the tokenizer. It must only contain letters,
        digits, spaces,

        dashes or underscores, can only start and end with alphanumeric characters,
        and is limited to

        128 characters.'
      id: name
      type:
      - str
  type: class
  uid: azure.search.documents.indexes.models.LexicalTokenizer
references: []
