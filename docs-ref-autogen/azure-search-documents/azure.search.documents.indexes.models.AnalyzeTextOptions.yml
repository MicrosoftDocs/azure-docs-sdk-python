### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.AnalyzeTextOptions
name: AnalyzeTextOptions
fullName: azure.search.documents.indexes.models.AnalyzeTextOptions
module: azure.search.documents.indexes.models
inheritances:
- msrest.serialization.Model
summary: 'Specifies some text and analysis components used to break that text into
  tokens.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: AnalyzeTextOptions(**kwargs)
  parameters:
  - name: text
    description: Required. The text to break into tokens.
    isRequired: true
    types:
    - <xref:str>
  - name: analyzer_name
    description: 'The name of the analyzer to use to break the given text. If this
      parameter is

      not specified, you must specify a tokenizer instead. The tokenizer and analyzer
      parameters are

      mutually exclusive. Possible values include: "ar.microsoft", "ar.lucene", "hy.lucene",

      "bn.microsoft", "eu.lucene", "bg.microsoft", "bg.lucene", "ca.microsoft", "ca.lucene",
      "zh-

      Hans.microsoft", "zh-Hans.lucene", "zh-Hant.microsoft", "zh-Hant.lucene", "hr.microsoft",

      "cs.microsoft", "cs.lucene", "da.microsoft", "da.lucene", "nl.microsoft", "nl.lucene",

      "en.microsoft", "en.lucene", "et.microsoft", "fi.microsoft", "fi.lucene", "fr.microsoft",

      "fr.lucene", "gl.lucene", "de.microsoft", "de.lucene", "el.microsoft", "el.lucene",

      "gu.microsoft", "he.microsoft", "hi.microsoft", "hi.lucene", "hu.microsoft",
      "hu.lucene",

      "is.microsoft", "id.microsoft", "id.lucene", "ga.lucene", "it.microsoft", "it.lucene",

      "ja.microsoft", "ja.lucene", "kn.microsoft", "ko.microsoft", "ko.lucene", "lv.microsoft",

      "lv.lucene", "lt.microsoft", "ml.microsoft", "ms.microsoft", "mr.microsoft",
      "nb.microsoft",

      "no.lucene", "fa.lucene", "pl.microsoft", "pl.lucene", "pt-BR.microsoft", "pt-BR.lucene",
      "pt-

      PT.microsoft", "pt-PT.lucene", "pa.microsoft", "ro.microsoft", "ro.lucene",
      "ru.microsoft",

      "ru.lucene", "sr-cyrillic.microsoft", "sr-latin.microsoft", "sk.microsoft",
      "sl.microsoft",

      "es.microsoft", "es.lucene", "sv.microsoft", "sv.lucene", "ta.microsoft", "te.microsoft",

      "th.microsoft", "th.lucene", "tr.microsoft", "tr.lucene", "uk.microsoft", "ur.microsoft",

      "vi.microsoft", "standard.lucene", "standardasciifolding.lucene", "keyword",
      "pattern",

      "simple", "stop", "whitespace".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.search.documents.indexes.models.LexicalAnalyzerName>
  - name: tokenizer_name
    description: 'The name of the tokenizer to use to break the given text. If this
      parameter

      is not specified, you must specify an analyzer instead. The tokenizer and analyzer
      parameters

      are mutually exclusive. Possible values include: "classic", "edgeNGram", "keyword_v2",

      "letter", "lowercase", "microsoft_language_tokenizer", "microsoft_language_stemming_tokenizer",

      "nGram", "path_hierarchy_v2", "pattern", "standard_v2", "uax_url_email", "whitespace".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.search.documents.indexes.models.LexicalTokenizerName>
  - name: token_filters
    description: 'An optional list of token filters to use when breaking the given
      text.

      This parameter can only be set when using the tokenizer parameter.'
    isRequired: true
    types:
    - <xref:list>[<xref:str>
    - <xref:azure.search.documents.indexes.models.TokenFilterName>]
  - name: char_filters
    description: 'An optional list of character filters to use when breaking the given
      text.

      This parameter can only be set when using the tokenizer parameter.'
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
