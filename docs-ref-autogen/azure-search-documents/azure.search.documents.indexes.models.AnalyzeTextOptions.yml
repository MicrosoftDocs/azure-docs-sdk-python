### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.AnalyzeTextOptions
name: AnalyzeTextOptions
fullName: azure.search.documents.indexes.models.AnalyzeTextOptions
module: azure.search.documents.indexes.models
inheritances:
- msrest.serialization.Model
summary: 'Specifies some text and analysis components used to break that text into
  tokens.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: AnalyzeTextOptions(**kwargs)
  parameters:
  - name: text
    description: Required. The text to break into tokens.
    isRequired: true
    types:
    - <xref:str>
  - name: analyzer_name
    description: 'The name of the analyzer to use to break the given text. If this
      parameter is

      not specified, you must specify a tokenizer instead. The tokenizer and analyzer
      parameters are

      mutually exclusive. Possible values include: "ar.microsoft", "ar.lucene", "hy.lucene",

      "bn.microsoft", "eu.lucene", "bg.microsoft", "bg.lucene", "ca.microsoft", "ca.lucene",
      "zh-

      Hans.microsoft", "zh-Hans.lucene", "zh-Hant.microsoft", "zh-Hant.lucene", "hr.microsoft",

      "cs.microsoft", "cs.lucene", "da.microsoft", "da.lucene", "nl.microsoft", "nl.lucene",

      "en.microsoft", "en.lucene", "et.microsoft", "fi.microsoft", "fi.lucene", "fr.microsoft",

      "fr.lucene", "gl.lucene", "de.microsoft", "de.lucene", "el.microsoft", "el.lucene",

      "gu.microsoft", "he.microsoft", "hi.microsoft", "hi.lucene", "hu.microsoft",
      "hu.lucene",

      "is.microsoft", "id.microsoft", "id.lucene", "ga.lucene", "it.microsoft", "it.lucene",

      "ja.microsoft", "ja.lucene", "kn.microsoft", "ko.microsoft", "ko.lucene", "lv.microsoft",

      "lv.lucene", "lt.microsoft", "ml.microsoft", "ms.microsoft", "mr.microsoft",
      "nb.microsoft",

      "no.lucene", "fa.lucene", "pl.microsoft", "pl.lucene", "pt-BR.microsoft", "pt-BR.lucene",
      "pt-

      PT.microsoft", "pt-PT.lucene", "pa.microsoft", "ro.microsoft", "ro.lucene",
      "ru.microsoft",

      "ru.lucene", "sr-cyrillic.microsoft", "sr-latin.microsoft", "sk.microsoft",
      "sl.microsoft",

      "es.microsoft", "es.lucene", "sv.microsoft", "sv.lucene", "ta.microsoft", "te.microsoft",

      "th.microsoft", "th.lucene", "tr.microsoft", "tr.lucene", "uk.microsoft", "ur.microsoft",

      "vi.microsoft", "standard.lucene", "standardasciifolding.lucene", "keyword",
      "pattern",

      "simple", "stop", "whitespace".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.search.documents.indexes.models.LexicalAnalyzerName>
  - name: tokenizer_name
    description: 'The name of the tokenizer to use to break the given text. If this
      parameter

      is not specified, you must specify an analyzer instead. The tokenizer and analyzer
      parameters

      are mutually exclusive. Possible values include: "classic", "edgeNGram", "keyword_v2",

      "letter", "lowercase", "microsoft_language_tokenizer", "microsoft_language_stemming_tokenizer",

      "nGram", "path_hierarchy_v2", "pattern", "standard_v2", "uax_url_email", "whitespace".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.search.documents.indexes.models.LexicalTokenizerName>
  - name: token_filters
    description: 'An optional list of token filters to use when breaking the given
      text.

      This parameter can only be set when using the tokenizer parameter.'
    isRequired: true
    types:
    - <xref:list>[<xref:str>
    - <xref:azure.search.documents.indexes.models.TokenFilterName>]
  - name: char_filters
    description: 'An optional list of character filters to use when breaking the given
      text.

      This parameter can only be set when using the tokenizer parameter.'
    isRequired: true
    types:
    - <xref:list>[<xref:str>]
methods:
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.as_dict
  name: as_dict
  summary: "Return a dict that can be JSONify using json.dump.\n\nAdvanced usage might\
    \ optionally use a callback as parameter:\n\nKey is the attribute name used in\
    \ Python. Attr_desc\nis a dict of metadata. Currently contains 'type' with the\n\
    msrest type and 'key' with the RestAPI encoded key.\nValue is the current value\
    \ in this object.\n\nThe string returned will be used to serialize the key.\n\
    If the return type is a list, this is considered hierarchical\nresult dict.\n\n\
    See the three examples in this file:\n\n* attribute_transformer \n\n* full_restapi_key_transformer\
    \ \n\n* last_restapi_key_transformer \n\nIf you want XML serialization, you can\
    \ pass the kwargs is_xml=True."
  signature: as_dict(keep_readonly=True, key_transformer=<function attribute_transformer>,
    **kwargs)
  parameters:
  - name: key_transformer
    description: A key transformer function.
    types:
    - <xref:function>
  - name: keep_readonly
    defaultValue: 'True'
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.deserialize
  name: deserialize
  summary: Parse a str using the RestAPI syntax and return a model.
  signature: deserialize(data, content_type=None)
  parameters:
  - name: data
    description: A str using RestAPI structure. JSON by default.
    isRequired: true
    types:
    - <xref:str>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  return:
    description: An instance of this model
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.enable_additional_properties_sending
  name: enable_additional_properties_sending
  signature: enable_additional_properties_sending()
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.from_dict
  name: from_dict
  summary: 'Parse a dict using given key extractor return a model.


    By default consider key

    extractors (rest_key_case_insensitive_extractor, attribute_key_case_insensitive_extractor

    and last_rest_key_case_insensitive_extractor)'
  signature: from_dict(data, key_extractors=None, content_type=None)
  parameters:
  - name: data
    description: A dict using RestAPI structure
    isRequired: true
    types:
    - <xref:dict>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  - name: key_extractors
    defaultValue: None
  return:
    description: An instance of this model
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.is_xml_model
  name: is_xml_model
  signature: is_xml_model()
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.serialize
  name: serialize
  summary: 'Return the JSON that would be sent to azure from this model.


    This is an alias to *as_dict(full_restapi_key_transformer, keep_readonly=False)*.


    If you want XML serialization, you can pass the kwargs is_xml=True.'
  signature: serialize(keep_readonly=False, **kwargs)
  parameters:
  - name: keep_readonly
    description: If you want to serialize the readonly attributes
    defaultValue: 'False'
    types:
    - <xref:bool>
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.validate
  name: validate
  summary: Validate this model recursively and return a list of ValidationError.
  signature: validate()
  return:
    description: A list of validation error
    types:
    - <xref:list>
