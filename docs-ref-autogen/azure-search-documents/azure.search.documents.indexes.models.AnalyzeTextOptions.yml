### YamlMime:PythonClass
uid: azure.search.documents.indexes.models.AnalyzeTextOptions
name: AnalyzeTextOptions
fullName: azure.search.documents.indexes.models.AnalyzeTextOptions
module: azure.search.documents.indexes.models
summary: 'Specifies some text and analysis components used to break that text into
  tokens.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'AnalyzeTextOptions(*, text: str, analyzer_name: str | None = None, tokenizer_name:
    str | None = None, normalizer_name: str | None = None, token_filters: List[str]
    | None = None, char_filters: List[str] | None = None, **kwargs)'
  keywordOnlyParameters:
  - name: text
    isRequired: true
  - name: analyzer_name
    defaultValue: None
  - name: tokenizer_name
    defaultValue: None
  - name: normalizer_name
    defaultValue: None
  - name: token_filters
    defaultValue: None
  - name: char_filters
    defaultValue: None
variables:
- description: Required. The text to break into tokens.
  name: text
  types:
  - <xref:str>
- description: 'The name of the analyzer to use to break the given text. If this parameter
    is

    not specified, you must specify a tokenizer instead. The tokenizer and analyzer
    parameters are

    mutually exclusive. Possible values include: "ar.microsoft", "ar.lucene", "hy.lucene",

    "bn.microsoft", "eu.lucene", "bg.microsoft", "bg.lucene", "ca.microsoft", "ca.lucene",
    "zh-

    Hans.microsoft", "zh-Hans.lucene", "zh-Hant.microsoft", "zh-Hant.lucene", "hr.microsoft",

    "cs.microsoft", "cs.lucene", "da.microsoft", "da.lucene", "nl.microsoft", "nl.lucene",

    "en.microsoft", "en.lucene", "et.microsoft", "fi.microsoft", "fi.lucene", "fr.microsoft",

    "fr.lucene", "gl.lucene", "de.microsoft", "de.lucene", "el.microsoft", "el.lucene",

    "gu.microsoft", "he.microsoft", "hi.microsoft", "hi.lucene", "hu.microsoft", "hu.lucene",

    "is.microsoft", "id.microsoft", "id.lucene", "ga.lucene", "it.microsoft", "it.lucene",

    "ja.microsoft", "ja.lucene", "kn.microsoft", "ko.microsoft", "ko.lucene", "lv.microsoft",

    "lv.lucene", "lt.microsoft", "ml.microsoft", "ms.microsoft", "mr.microsoft", "nb.microsoft",

    "no.lucene", "fa.lucene", "pl.microsoft", "pl.lucene", "pt-BR.microsoft", "pt-BR.lucene",
    "pt-

    PT.microsoft", "pt-PT.lucene", "pa.microsoft", "ro.microsoft", "ro.lucene", "ru.microsoft",

    "ru.lucene", "sr-cyrillic.microsoft", "sr-latin.microsoft", "sk.microsoft", "sl.microsoft",

    "es.microsoft", "es.lucene", "sv.microsoft", "sv.lucene", "ta.microsoft", "te.microsoft",

    "th.microsoft", "th.lucene", "tr.microsoft", "tr.lucene", "uk.microsoft", "ur.microsoft",

    "vi.microsoft", "standard.lucene", "standardasciifolding.lucene", "keyword", "pattern",

    "simple", "stop", "whitespace".'
  name: analyzer_name
  types:
  - <xref:str>
  - <xref:azure.search.documents.indexes.models.LexicalAnalyzerName>
- description: 'The name of the tokenizer to use to break the given text. If this
    parameter

    is not specified, you must specify an analyzer instead. The tokenizer and analyzer
    parameters

    are mutually exclusive. Possible values include: "classic", "edgeNGram", "keyword_v2",

    "letter", "lowercase", "microsoft_language_tokenizer", "microsoft_language_stemming_tokenizer",

    "nGram", "path_hierarchy_v2", "pattern", "standard_v2", "uax_url_email", "whitespace".'
  name: tokenizer_name
  types:
  - <xref:str>
  - <xref:azure.search.documents.indexes.models.LexicalTokenizerName>
- description: 'An optional list of token filters to use when breaking the given text.

    This parameter can only be set when using the tokenizer parameter.'
  name: token_filters
  types:
  - <xref:list>[<xref:str>
  - <xref:azure.search.documents.indexes.models.TokenFilterName>]
- description: 'An optional list of character filters to use when breaking the given
    text.

    This parameter can only be set when using the tokenizer parameter.'
  name: char_filters
  types:
  - <xref:list>[<xref:str>]
methods:
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.as_dict
  name: as_dict
  summary: Return a dict that can be serialized using json.dump.
  signature: 'as_dict(keep_readonly: bool = True, key_transformer: ~typing.Callable[[str,
    ~typing.Dict[str, ~typing.Any], ~typing.Any], ~typing.Any] = <function attribute_transformer>,
    **kwargs: ~typing.Any) -> MutableMapping[str, Any]'
  parameters:
  - name: keep_readonly
    description: If you want to serialize the readonly attributes
    defaultValue: 'True'
    types:
    - <xref:bool>
  - name: key_transformer
    description: A callable that will transform the key of the dict
    types:
    - <xref:typing.Callable>
  return:
    description: A dict JSON compatible object
    types:
    - <xref:dict>
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.deserialize
  name: deserialize
  summary: Parse a str using the RestAPI syntax and return a AnalyzeTextOptions instance.
  signature: 'deserialize(data: Any, content_type: str | None = None) -> Self | None'
  parameters:
  - name: data
    description: A str using RestAPI structure. JSON by default.
    isRequired: true
    types:
    - <xref:str>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  return:
    description: A AnalyzeTextOptions instance
    types:
    - <xref:azure.search.documents.indexes.models.AnalyzeTextOptions>
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.enable_additional_properties_sending
  name: enable_additional_properties_sending
  signature: enable_additional_properties_sending() -> None
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.from_dict
  name: from_dict
  summary: 'Parse a dict using given key extractor return a model.


    By default consider key

    extractors (rest_key_case_insensitive_extractor, attribute_key_case_insensitive_extractor

    and last_rest_key_case_insensitive_extractor)'
  signature: 'from_dict(data: Any, key_extractors: Callable[[str, Dict[str, Any],
    Any], Any] | None = None, content_type: str | None = None) -> Self | None'
  parameters:
  - name: data
    description: A dict using RestAPI structure
    isRequired: true
    types:
    - <xref:dict>
  - name: key_extractors
    description: A callable that will extract a key from a dict
    defaultValue: None
    types:
    - <xref:typing.Callable>
  - name: content_type
    description: JSON by default, set application/xml if XML.
    defaultValue: None
    types:
    - <xref:str>
  return:
    description: A AnalyzeTextOptions instance
    types:
    - <xref:azure.search.documents.indexes.models.AnalyzeTextOptions>
  exceptions:
  - type: DeserializationError if something went wrong
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.is_xml_model
  name: is_xml_model
  signature: is_xml_model() -> bool
- uid: azure.search.documents.indexes.models.AnalyzeTextOptions.serialize
  name: serialize
  summary: 'Return the JSON that would be sent to server from this model.

    :param bool keep_readonly: If you want to serialize the readonly attributes

    :returns: A dict JSON compatible object

    :rtype: dict'
  signature: 'serialize(keep_readonly: bool = False, **kwargs: Any) -> MutableMapping[str,
    Any]'
  parameters:
  - name: keep_readonly
    defaultValue: 'False'
