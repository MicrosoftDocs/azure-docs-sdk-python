### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: azure.search.documents.indexes.models.MicrosoftLanguageTokenizer
  fullName: azure.search.documents.indexes.models.MicrosoftLanguageTokenizer
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: msrest.serialization.Model
    type: azure.search.documents.indexes._internal._generated.models._models_py3.LexicalTokenizer
  langs:
  - python
  module: azure.search.documents.indexes.models
  name: MicrosoftLanguageTokenizer
  summary: 'Divides text using language-specific rules.


    All required parameters must be populated in order to send to Azure.'
  syntax:
    content: 'MicrosoftLanguageTokenizer(*, name: str, max_token_length: typing.Union[int,
      NoneType] = 255, is_search_tokenizer: typing.Union[bool, NoneType] = False,
      language: typing.Union[str, _ForwardRef(''MicrosoftTokenizerLanguage''), NoneType]
      = None, **kwargs)'
    parameters:
    - description: 'Required. Identifies the concrete type of the tokenizer.Constant
        filled by

        server.'
      id: odata_type
      type:
      - str
    - description: 'Required. The name of the tokenizer. It must only contain letters,
        digits, spaces,

        dashes or underscores, can only start and end with alphanumeric characters,
        and is limited to

        128 characters.'
      id: name
      type:
      - str
    - description: 'The maximum token length. Tokens longer than the maximum length
        are

        split. Maximum token length that can be used is 300 characters. Tokens longer
        than 300

        characters are first split into tokens of length 300 and then each of those
        tokens is split

        based on the max token length set. Default is 255.'
      id: max_token_length
      type:
      - int
    - description: 'A value indicating how the tokenizer is used. Set to true if used

        as the search tokenizer, set to false if used as the indexing tokenizer. Default
        is false.'
      id: is_search_tokenizer
      type:
      - bool
    - description: 'The language to use. The default is English. Possible values include:

        "bangla", "bulgarian", "catalan", "chineseSimplified", "chineseTraditional",
        "croatian",

        "czech", "danish", "dutch", "english", "french", "german", "greek", "gujarati",
        "hindi",

        "icelandic", "indonesian", "italian", "japanese", "kannada", "korean", "malay",
        "malayalam",

        "marathi", "norwegianBokmaal", "polish", "portuguese", "portugueseBrazilian",
        "punjabi",

        "romanian", "russian", "serbianCyrillic", "serbianLatin", "slovenian", "spanish",
        "swedish",

        "tamil", "telugu", "thai", "ukrainian", "urdu", "vietnamese".'
      id: language
      type:
      - str
      - azure.search.documents.indexes.models.MicrosoftTokenizerLanguage
  type: class
  uid: azure.search.documents.indexes.models.MicrosoftLanguageTokenizer
references: []
