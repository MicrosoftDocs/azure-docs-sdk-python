### YamlMime:PythonClass
uid: azure.mgmt.synapse.models.BigDataPoolResourceInfo
name: BigDataPoolResourceInfo
fullName: azure.mgmt.synapse.models.BigDataPoolResourceInfo
module: azure.mgmt.synapse.models
inheritances:
- azure.mgmt.synapse.models._models_py3.TrackedResource
summary: 'A Big Data pool.


  Variables are only populated by the server, and will be ignored when sending a request.


  All required parameters must be populated in order to send to Azure.'
constructor:
  syntax: 'BigDataPoolResourceInfo(*, location: str, tags: typing.Union[typing.Dict[str,
    str], NoneType] = None, provisioning_state: typing.Union[str, NoneType] = None,
    auto_scale: typing.Union[_ForwardRef(''AutoScaleProperties''), NoneType] = None,
    creation_date: typing.Union[datetime.datetime, NoneType] = None, auto_pause: typing.Union[_ForwardRef(''AutoPauseProperties''),
    NoneType] = None, is_compute_isolation_enabled: typing.Union[bool, NoneType] =
    None, session_level_packages_enabled: typing.Union[bool, NoneType] = None, cache_size:
    typing.Union[int, NoneType] = None, dynamic_executor_allocation: typing.Union[_ForwardRef(''DynamicExecutorAllocation''),
    NoneType] = None, spark_events_folder: typing.Union[str, NoneType] = None, node_count:
    typing.Union[int, NoneType] = None, library_requirements: typing.Union[_ForwardRef(''LibraryRequirements''),
    NoneType] = None, custom_libraries: typing.Union[typing.List[_ForwardRef(''LibraryInfo'')],
    NoneType] = None, spark_config_properties: typing.Union[_ForwardRef(''LibraryRequirements''),
    NoneType] = None, spark_version: typing.Union[str, NoneType] = None, default_spark_log_folder:
    typing.Union[str, NoneType] = None, node_size: typing.Union[str, _ForwardRef(''NodeSize''),
    NoneType] = None, node_size_family: typing.Union[str, _ForwardRef(''NodeSizeFamily''),
    NoneType] = None, **kwargs)'
  parameters:
  - name: tags
    description: A set of tags. Resource tags.
    isRequired: true
    types:
    - <xref:dict>[<xref:str>, <xref:str>]
  - name: location
    description: Required. The geo-location where the resource lives.
    isRequired: true
    types:
    - <xref:str>
  - name: provisioning_state
    description: The state of the Big Data pool.
    isRequired: true
    types:
    - <xref:str>
  - name: auto_scale
    description: Auto-scaling properties.
    isRequired: true
    types:
    - <xref:azure.mgmt.synapse.models.AutoScaleProperties>
  - name: creation_date
    description: The time when the Big Data pool was created.
    isRequired: true
    types:
    - <xref:datetime.datetime>
  - name: auto_pause
    description: Auto-pausing properties.
    isRequired: true
    types:
    - <xref:azure.mgmt.synapse.models.AutoPauseProperties>
  - name: is_compute_isolation_enabled
    description: Whether compute isolation is required or not.
    isRequired: true
    types:
    - <xref:bool>
  - name: session_level_packages_enabled
    description: Whether session level packages enabled.
    isRequired: true
    types:
    - <xref:bool>
  - name: cache_size
    description: The cache size.
    isRequired: true
    types:
    - <xref:int>
  - name: dynamic_executor_allocation
    description: Dynamic Executor Allocation.
    isRequired: true
    types:
    - <xref:azure.mgmt.synapse.models.DynamicExecutorAllocation>
  - name: spark_events_folder
    description: The Spark events folder.
    isRequired: true
    types:
    - <xref:str>
  - name: node_count
    description: The number of nodes in the Big Data pool.
    isRequired: true
    types:
    - <xref:int>
  - name: library_requirements
    description: Library version requirements.
    isRequired: true
    types:
    - <xref:azure.mgmt.synapse.models.LibraryRequirements>
  - name: custom_libraries
    description: List of custom libraries/packages associated with the spark pool.
    isRequired: true
    types:
    - <xref:list>[<xref:azure.mgmt.synapse.models.LibraryInfo>]
  - name: spark_config_properties
    description: Spark configuration file to specify additional properties.
    isRequired: true
    types:
    - <xref:azure.mgmt.synapse.models.LibraryRequirements>
  - name: spark_version
    description: The Apache Spark version.
    isRequired: true
    types:
    - <xref:str>
  - name: default_spark_log_folder
    description: The default folder where Spark logs will be written.
    isRequired: true
    types:
    - <xref:str>
  - name: node_size
    description: 'The level of compute power that each node in the Big Data pool has.
      Possible

      values include: "None", "Small", "Medium", "Large", "XLarge", "XXLarge", "XXXLarge".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.mgmt.synapse.models.NodeSize>
  - name: node_size_family
    description: 'The kind of nodes that the Big Data pool provides. Possible values

      include: "None", "MemoryOptimized".'
    isRequired: true
    types:
    - <xref:str>
    - <xref:azure.mgmt.synapse.models.NodeSizeFamily>
variables:
- description: 'Fully qualified resource ID for the resource. Ex -

    /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}.'
  name: id
  types:
  - <xref:str>
- description: The name of the resource.
  name: name
  types:
  - <xref:str>
- description: 'The type of the resource. E.g. "Microsoft.Compute/virtualMachines"
    or

    "Microsoft.Storage/storageAccounts".'
  name: type
  types:
  - <xref:str>
- description: The time when the Big Data pool was updated successfully.
  name: last_succeeded_timestamp
  types:
  - <xref:datetime.datetime>
