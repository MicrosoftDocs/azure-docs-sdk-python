### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: azure.mgmt.synapse.models.BigDataPoolResourceInfo
  fullName: azure.mgmt.synapse.models.BigDataPoolResourceInfo
  inheritance:
  - inheritance:
    - inheritance:
      - inheritance:
        - type: builtins.object
        type: msrest.serialization.Model
      type: azure.mgmt.synapse.models._models_py3.Resource
    type: azure.mgmt.synapse.models._models_py3.TrackedResource
  langs:
  - python
  module: azure.mgmt.synapse.models
  name: BigDataPoolResourceInfo
  summary: 'Big Data pool.


    A Big Data pool.


    Variables are only populated by the server, and will be ignored when

    sending a request.


    All required parameters must be populated in order to send to Azure.'
  syntax:
    content: 'BigDataPoolResourceInfo(*, location: str, tags=None, provisioning_state:
      str = None, auto_scale=None, creation_date=None, auto_pause=None, spark_events_folder:
      str = None, node_count: int = None, library_requirements=None, spark_version:
      str = None, default_spark_log_folder: str = None, node_size=None, node_size_family=None,
      **kwargs) -> None'
    parameters:
    - description: Resource tags.
      id: tags
      type:
      - dict[str, str]
    - description: Required. The geo-location where the resource lives
      id: location
      type:
      - str
    - description: The state of the Big Data pool.
      id: provisioning_state
      type:
      - str
    - description: Auto-scaling properties
      id: auto_scale
      type:
      - azure.mgmt.synapse.models.AutoScaleProperties
    - description: The time when the Big Data pool was created.
      id: creation_date
      type:
      - datetime
    - description: Auto-pausing properties
      id: auto_pause
      type:
      - azure.mgmt.synapse.models.AutoPauseProperties
    - description: The Spark events folder
      id: spark_events_folder
      type:
      - str
    - description: The number of nodes in the Big Data pool.
      id: node_count
      type:
      - int
    - description: Library version requirements
      id: library_requirements
      type:
      - azure.mgmt.synapse.models.LibraryRequirements
    - description: The Apache Spark version.
      id: spark_version
      type:
      - str
    - description: 'The default folder where Spark logs will

        be written.'
      id: default_spark_log_folder
      type:
      - str
    - description: 'The level of compute power that each node in the Big

        Data pool has. Possible values include: ''None'', ''Small'', ''Medium'', ''Large'''
      id: node_size
      type:
      - str
      - azure.mgmt.synapse.models.NodeSize
    - description: 'The kind of nodes that the Big Data pool

        provides. Possible values include: ''None'', ''MemoryOptimized'''
      id: node_size_family
      type:
      - str
      - azure.mgmt.synapse.models.NodeSizeFamily
    variables:
    - description: 'Fully qualified resource Id for the resource. Ex -

        /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}'
      id: id
      type:
      - str
    - description: The name of the resource
      id: name
      type:
      - str
    - description: 'The type of the resource. Ex-

        Microsoft.Compute/virtualMachines or Microsoft.Storage/storageAccounts.'
      id: type
      type:
      - str
  type: class
  uid: azure.mgmt.synapse.models.BigDataPoolResourceInfo
references:
- fullName: dict[str, str]
  name: dict[str, str]
  spec.python:
  - fullName: dict
    name: dict
    uid: dict
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  - fullName: ', '
    name: ', '
  - fullName: str
    name: str
    uid: str
  - fullName: ']'
    name: ']'
  uid: dict[str, str]
