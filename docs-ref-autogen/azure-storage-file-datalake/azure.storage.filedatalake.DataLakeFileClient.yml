### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.storage.filedatalake.DataLakeFileClient.append_data
  - azure.storage.filedatalake.DataLakeFileClient.create_file
  - azure.storage.filedatalake.DataLakeFileClient.delete_file
  - azure.storage.filedatalake.DataLakeFileClient.download_file
  - azure.storage.filedatalake.DataLakeFileClient.flush_data
  - azure.storage.filedatalake.DataLakeFileClient.from_connection_string
  - azure.storage.filedatalake.DataLakeFileClient.get_file_properties
  - azure.storage.filedatalake.DataLakeFileClient.query_file
  - azure.storage.filedatalake.DataLakeFileClient.rename_file
  - azure.storage.filedatalake.DataLakeFileClient.upload_data
  class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "Creating the DataLakeServiceClient from connection string.<!--[!code-python[Main](les\\\
    datalake_samples_instantiate_client.py )]-->\n\n<!-- literal_block {\"ids\": [],\
    \ \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\"\
    : \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\\113\\\\azure-storage-file-datalake-12.1.2\\\
    \\samples\\\\datalake_samples_instantiate_client.py\", \"xml:space\": \"preserve\"\
    , \"language\": \"python\", \"linenos\": false, \"highlight_args\": {\"linenostart\"\
    : 1}} -->\n\n````python\n\n   from azure.storage.filedatalake import DataLakeFileClient\n\
    \   DataLakeFileClient.from_connection_string(connection_string, \"myfilesystem\"\
    , \"mydirectory\", \"myfile\")\n\n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient
  inheritance:
  - inheritance:
    - inheritance:
      - type: builtins.object
      type: azure.storage.filedatalake._shared.base_client.StorageAccountHostsMixin
    type: azure.storage.filedatalake._path_client.PathClient
  langs:
  - python
  module: azure.storage.filedatalake
  name: DataLakeFileClient
  summary: A client to interact with the DataLake file, even if the file may not yet
    exist.
  syntax:
    content: DataLakeFileClient(account_url, file_system_name, file_path, credential=None,
      **kwargs)
    parameters:
    - description: The URI to the storage account.
      id: account_url
      type:
      - str
    - description: The file system for the directory or files.
      id: file_system_name
      type:
      - str
    - description: 'The whole file path, so that to interact with a specific file.

        eg. "{directory}/{subdirectory}/{file}"'
      id: file_path
      type:
      - str
    - description: 'The credentials with which to authenticate. This is optional if
        the

        account URL already has a SAS token. The value can be a SAS token string,
        and account

        shared access key, or an instance of a TokenCredentials class from azure.identity.

        If the URL already has a SAS token, specifying an explicit credential will
        take priority.'
      id: credential
    variables:
    - description: The full endpoint URL to the file system, including SAS token if
        used.
      id: url
      type:
      - str
    - description: The full primary endpoint URL.
      id: primary_endpoint
      type:
      - str
    - description: The hostname of the primary endpoint.
      id: primary_hostname
      type:
      - str
  type: class
  uid: azure.storage.filedatalake.DataLakeFileClient
- class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "Append data to the file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\113\\\\azure-storage-file-datalake-12.1.2\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"language\": \"python\", \"linenos\": false, \"\
    highlight_args\": {\"linenostart\": 1}} -->\n\n````python\n\n   file_client.append_data(data=file_content[2048:3072],\
    \ offset=2048, length=1024)\n\n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient.append_data
  langs:
  - python
  module: azure.storage.filedatalake
  name: append_data(data, offset, length=None, **kwargs)
  namewithoutparameters: append_data
  summary: Append data to the file.
  syntax:
    content: append_data(data, offset, length=None, **kwargs)
    parameters:
    - description: Content to be appended to file
      id: data
      isRequired: true
    - description: start position of the data to be appended to.
      id: offset
      isRequired: true
    - defaultValue: None
      description: Size of the data in bytes.
      id: length
    - description: 'If true, calculates an MD5 hash of the block content. The storage

        service checks the hash of the content that has arrived

        with the hash that was sent. This is primarily valuable for detecting

        bitflips on the wire if using http instead of https as https (the default)

        will already validate. Note that this MD5 hash is not stored with the

        file.'
      id: validate_content
      isRequired: true
      type:
      - bool
    - description: 'Required if the file has an active lease. Value can be a DataLakeLeaseClient
        object

        or the lease ID as a string.'
      id: lease
      isRequired: true
      type:
      - azure.storage.filedatalake.DataLakeLeaseClient
      - str
    return:
      description: dict of the response header
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.append_data
- class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "Create file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\113\\\\azure-storage-file-datalake-12.1.2\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"language\": \"python\", \"linenos\": false, \"\
    highlight_args\": {\"linenostart\": 1}} -->\n\n````python\n\n   file_client =\
    \ filesystem_client.get_file_client(file_name)\n   file_client.create_file()\n\
    \n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient.create_file
  langs:
  - python
  module: azure.storage.filedatalake
  name: create_file(content_settings=None, metadata=None, **kwargs)
  namewithoutparameters: create_file
  summary: Create a new file.
  syntax:
    content: create_file(content_settings=None, metadata=None, **kwargs)
    parameters:
    - defaultValue: None
      description: ContentSettings object used to set path properties.
      id: content_settings
      type:
      - azure.storage.filedatalake.ContentSettings
    - defaultValue: None
      description: Name-value pairs associated with the file as metadata.
      id: metadata
      type:
      - dict(str, str)
    - description: 'Required if the file has an active lease. Value can be a DataLakeLeaseClient
        object

        or the lease ID as a string.'
      id: lease
      isRequired: true
      type:
      - azure.storage.filedatalake.DataLakeLeaseClient
      - str
    - description: 'Optional and only valid if Hierarchical Namespace is enabled for
        the account.

        When creating a file or directory and the parent folder does not have a default
        ACL,

        the umask restricts the permissions of the file or directory to be created.

        The resulting permission is given by p & ^u, where p is the permission and
        u is the umask.

        For example, if p is 0777 and u is 0057, then the resulting permission is
        0720.

        The default permission is 0777 for a directory and 0666 for a file. The default
        umask is 0027.

        The umask must be specified in 4-digit octal notation (e.g. 0766).'
      id: umask
      isRequired: true
      type:
      - str
    - description: 'Optional and only valid if Hierarchical Namespace

        is enabled for the account. Sets POSIX access permissions for the file

        owner, the file owning group, and others. Each class may be granted

        read, write, or execute permission.  The sticky bit is also supported.

        Both symbolic (rwxrw-rw-) and 4-digit octal notation (e.g. 0766) are

        supported.'
      id: permissions
      isRequired: true
      type:
      - str
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'An ETag value, or the wildcard character (*). Used to check if
        the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: etag
      isRequired: true
      type:
      - str
    - description: The match condition to use upon the etag.
      id: match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    - description: The timeout parameter is expressed in seconds.
      id: timeout
      isRequired: true
      type:
      - int
    return:
      description: response dict (Etag and last modified).
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.create_file
- class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "Delete file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\113\\\\azure-storage-file-datalake-12.1.2\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"language\": \"python\", \"linenos\": false, \"\
    highlight_args\": {\"linenostart\": 1}} -->\n\n````python\n\n   new_client.delete_file()\n\
    \n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient.delete_file
  langs:
  - python
  module: azure.storage.filedatalake
  name: delete_file(**kwargs)
  namewithoutparameters: delete_file
  summary: Marks the specified file for deletion.
  syntax:
    content: delete_file(**kwargs)
    parameters:
    - description: 'Required if the file has an active lease. Value can be a LeaseClient
        object

        or the lease ID as a string.'
      id: lease
      isRequired: true
      type:
      - azure.storage.filedatalake.DataLakeLeaseClient
      - str
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'An ETag value, or the wildcard character (*). Used to check if
        the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: etag
      isRequired: true
      type:
      - str
    - description: The match condition to use upon the etag.
      id: match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    - description: The timeout parameter is expressed in seconds.
      id: timeout
      isRequired: true
      type:
      - int
    return:
      description: None
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.delete_file
- class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "Return the downloaded data.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\113\\\\azure-storage-file-datalake-12.1.2\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"language\": \"python\", \"linenos\": false, \"\
    highlight_args\": {\"linenostart\": 1}} -->\n\n````python\n\n   download = file_client.download_file()\n\
    \   downloaded_bytes = download.readall()\n\n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient.download_file
  langs:
  - python
  module: azure.storage.filedatalake
  name: download_file(offset=None, length=None, **kwargs)
  namewithoutparameters: download_file
  summary: 'Downloads a file to the StorageStreamDownloader. The readall() method
    must

    be used to read all the content, or readinto() must be used to download the file
    into

    a stream.'
  syntax:
    content: download_file(offset=None, length=None, **kwargs)
    parameters:
    - defaultValue: None
      description: 'Start of byte range to use for downloading a section of the file.

        Must be set if length is provided.'
      id: offset
      type:
      - int
    - defaultValue: None
      description: 'Number of bytes to read from the stream. This is optional, but

        should be supplied for optimal performance.'
      id: length
      type:
      - int
    - description: 'If specified, download only succeeds if the file''s lease is active

        and matches this ID. Required if the file has an active lease.'
      id: lease
      isRequired: true
      type:
      - azure.storage.filedatalake.DataLakeLeaseClient
      - str
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'An ETag value, or the wildcard character (*). Used to check if
        the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: etag
      isRequired: true
      type:
      - str
    - description: The match condition to use upon the etag.
      id: match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    - description: The number of parallel connections with which to download.
      id: max_concurrency
      isRequired: true
      type:
      - int
    - description: 'The timeout parameter is expressed in seconds. This method may
        make

        multiple calls to the Azure service and the timeout will apply to

        each call individually.'
      id: timeout
      isRequired: true
      type:
      - int
    return:
      description: A streaming object (StorageStreamDownloader)
      type:
      - azure.storage.filedatalake.StorageStreamDownloader
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.download_file
- class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "Commit the previous appended data.<!--[!code-python[Main](les\\datalake_samples_file_system.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\113\\\\azure-storage-file-datalake-12.1.2\\\\samples\\\\datalake_samples_file_system.py\"\
    , \"xml:space\": \"preserve\", \"language\": \"python\", \"linenos\": false, \"\
    highlight_args\": {\"linenostart\": 1}} -->\n\n````python\n\n   with open(SOURCE_FILE,\
    \ \"rb\") as data:\n       file_client = file_system_client.get_file_client(\"\
    myfile\")\n       file_client.create_file()\n       file_client.append_data(data,\
    \ 0)\n       file_client.flush_data(data.tell())\n\n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient.flush_data
  langs:
  - python
  module: azure.storage.filedatalake
  name: flush_data(offset, retain_uncommitted_data=False, **kwargs)
  namewithoutparameters: flush_data
  summary: Commit the previous appended data.
  syntax:
    content: flush_data(offset, retain_uncommitted_data=False, **kwargs)
    parameters:
    - description: 'offset is equal to the length of the file after commit the

        previous appended data.'
      id: offset
      isRequired: true
    - defaultValue: 'False'
      description: 'Valid only for flush operations.  If

        "true", uncommitted data is retained after the flush operation

        completes; otherwise, the uncommitted data is deleted after the flush

        operation.  The default is false.  Data at offsets less than the

        specified position are written to the file when flush succeeds, but

        this optional parameter allows data after the flush position to be

        retained for a future flush operation.'
      id: retain_uncommitted_data
      type:
      - bool
    - description: ContentSettings object used to set path properties.
      id: content_settings
      isRequired: true
      type:
      - azure.storage.filedatalake.ContentSettings
    - description: 'Azure Storage Events allow applications to receive

        notifications when files change. When Azure Storage Events are

        enabled, a file changed event is raised. This event has a property

        indicating whether this is the final change to distinguish the

        difference between an intermediate flush to a file stream and the

        final close of a file stream. The close query parameter is valid only

        when the action is "flush" and change notifications are enabled. If

        the value of close is "true" and the flush operation completes

        successfully, the service raises a file change notification with a

        property indicating that this is the final update (the file stream has

        been closed). If "false" a change notification is raised indicating

        the file has changed. The default is false. This query parameter is

        set to true by the Hadoop ABFS driver to indicate that the file stream

        has been closed."'
      id: close
      isRequired: true
      type:
      - bool
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'An ETag value, or the wildcard character (*). Used to check if
        the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: etag
      isRequired: true
      type:
      - str
    - description: The match condition to use upon the etag.
      id: match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    return:
      description: response header in dict
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.flush_data
- class: azure.storage.filedatalake.DataLakeFileClient
  fullName: azure.storage.filedatalake.DataLakeFileClient.from_connection_string
  langs:
  - python
  module: azure.storage.filedatalake
  name: from_connection_string(conn_str, file_system_name, file_path, credential=None,
    **kwargs)
  namewithoutparameters: from_connection_string
  summary: 'Create DataLakeFileClient from a Connection String.



    :return a DataLakeFileClient

    :rtype ~azure.storage.filedatalake.DataLakeFileClient'
  syntax:
    content: from_connection_string(conn_str, file_system_name, file_path, credential=None,
      **kwargs)
    parameters:
    - description: A connection string to an Azure Storage account.
      id: conn_str
      isRequired: true
      type:
      - str
    - description: The name of file system to interact with.
      id: file_system_name
      isRequired: true
      type:
      - str
    - description: The name of directory to interact with. The directory is under
        file system.
      id: directory_name
      isRequired: true
      type:
      - str
    - description: The name of file to interact with. The file is under directory.
      id: file_name
      isRequired: true
      type:
      - str
    - defaultValue: None
      description: 'The credentials with which to authenticate. This is optional if
        the

        account URL already has a SAS token, or the connection string already has
        shared

        access key values. The value can be a SAS token string, and account shared
        access

        key, or an instance of a TokenCredentials class from azure.identity.

        Credentials provided here will take precedence over those in the connection
        string.'
      id: credential
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.from_connection_string
- class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "Getting the properties for a file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\113\\\\azure-storage-file-datalake-12.1.2\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"language\": \"python\", \"linenos\": false, \"\
    highlight_args\": {\"linenostart\": 1}} -->\n\n````python\n\n   properties = file_client.get_file_properties()\n\
    \n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient.get_file_properties
  langs:
  - python
  module: azure.storage.filedatalake
  name: get_file_properties(**kwargs)
  namewithoutparameters: get_file_properties
  summary: 'Returns all user-defined metadata, standard HTTP properties, and

    system properties for the file. It does not return the content of the file.'
  syntax:
    content: get_file_properties(**kwargs)
    parameters:
    - description: 'Required if the directory or file has an active lease. Value can
        be a DataLakeLeaseClient object

        or the lease ID as a string.'
      id: lease
      isRequired: true
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'An ETag value, or the wildcard character (*). Used to check if
        the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: etag
      isRequired: true
      type:
      - str
    - description: The match condition to use upon the etag.
      id: match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    - description: The timeout parameter is expressed in seconds.
      id: timeout
      isRequired: true
      type:
      - int
    return:
      type:
      - FileProperties
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.get_file_properties
- class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "select/project on datalake file data by providing simple query expressions.<!--[!code-python[Main](les\\\
    datalake_samples_query.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
    : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\\
    a\\\\1\\\\s\\\\dist_temp\\\\113\\\\azure-storage-file-datalake-12.1.2\\\\samples\\\
    \\datalake_samples_query.py\", \"xml:space\": \"preserve\", \"language\": \"python\"\
    , \"linenos\": false, \"highlight_args\": {\"linenostart\": 1}} -->\n\n````python\n\
    \n   errors = []\n   def on_error(error):\n       errors.append(error)\n\n   #\
    \ upload the csv file\n   file_client = datalake_service_client.get_file_client(filesystem_name,\
    \ \"csvfile\")\n   file_client.upload_data(CSV_DATA, overwrite=True)\n\n   # select\
    \ the second column of the csv file\n   query_expression = \"SELECT _2 from DataLakeStorage\"\
    \n   input_format = DelimitedTextDialect(delimiter=',', quotechar='\"', lineterminator='\\\
    n', escapechar=\"\", has_header=False)\n   output_format = DelimitedJsonDialect(delimiter='\\\
    n')\n   reader = file_client.query_file(query_expression, on_error=on_error, file_format=input_format,\
    \ output_format=output_format)\n   content = reader.readall()\n\n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient.query_file
  langs:
  - python
  module: azure.storage.filedatalake
  name: query_file(query_expression, **kwargs)
  namewithoutparameters: query_file
  summary: 'Enables users to select/project on datalake file data by providing simple
    query expressions.

    This operations returns a DataLakeFileQueryReader, users need to use readall()
    or readinto() to get query data.'
  syntax:
    content: query_file(query_expression, **kwargs)
    parameters:
    - description: 'Required. a query statement.

        eg. Select * from DataLakeStorage'
      id: query_expression
      isRequired: true
      type:
      - str
    - description: A function to be called on any processing errors returned by the
        service.
      id: on_error
      isRequired: true
      type:
      - Callable[Exception]
    - description: 'Optional. Defines the serialization of the data currently stored
        in the file. The default is to

        treat the file data as CSV data formatted in the default dialect. This can
        be overridden with

        a custom DelimitedTextDialect, or alternatively a DelimitedJsonDialect.'
      id: file_format
      isRequired: true
      type:
      - azure.storage.filedatalake.DelimitedTextDialect
      - azure.storage.filedatalake.DelimitedJsonDialect
    - description: 'Optional. Defines the output serialization for the data stream.
        By default the data will be returned

        as it is represented in the file. By providing an output format, the file
        data will be reformatted

        according to that profile. This value can be a DelimitedTextDialect or a DelimitedJsonDialect.'
      id: output_format
      isRequired: true
      type:
      - azure.storage.filedatalake.DelimitedTextDialect
      - azure.storage.filedatalake.DelimitedJsonDialect
    - description: 'Required if the file has an active lease. Value can be a DataLakeLeaseClient
        object

        or the lease ID as a string.'
      id: lease
      isRequired: true
      type:
      - azure.storage.filedatalake.DataLakeLeaseClient
      - str
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'An ETag value, or the wildcard character (*). Used to check if
        the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: etag
      isRequired: true
      type:
      - str
    - description: The match condition to use upon the etag.
      id: match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    - description: The timeout parameter is expressed in seconds.
      id: timeout
      isRequired: true
      type:
      - int
    return:
      description: A streaming object (DataLakeFileQueryReader)
      type:
      - azure.storage.filedatalake.DataLakeFileQueryReader
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.query_file
- class: azure.storage.filedatalake.DataLakeFileClient
  example:
  - "Rename the source file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\113\\\\azure-storage-file-datalake-12.1.2\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"language\": \"python\", \"linenos\": false, \"\
    highlight_args\": {\"linenostart\": 1}} -->\n\n````python\n\n   new_client = file_client.rename_file(file_client.file_system_name\
    \ + '/' + 'newname')\n\n   ````\n"
  fullName: azure.storage.filedatalake.DataLakeFileClient.rename_file
  langs:
  - python
  module: azure.storage.filedatalake
  name: rename_file(new_name, **kwargs)
  namewithoutparameters: rename_file
  summary: Rename the source file.
  syntax:
    content: rename_file(new_name, **kwargs)
    parameters:
    - description: 'the new file name the user want to rename to.

        The value must have the following format: "{filesystem}/{directory}/{subdirectory}/{file}".'
      id: new_name
      isRequired: true
      type:
      - str
    - description: ContentSettings object used to set path properties.
      id: content_settings
      isRequired: true
      type:
      - azure.storage.filedatalake.ContentSettings
    - description: 'A lease ID for the source path. If specified,

        the source path must have an active lease and the leaase ID must

        match.'
      id: source_lease
      isRequired: true
      type:
      - azure.storage.filedatalake.DataLakeLeaseClient
      - str
    - description: 'Required if the file/directory has an active lease. Value can
        be a LeaseClient object

        or the lease ID as a string.'
      id: lease
      isRequired: true
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'An ETag value, or the wildcard character (*). Used to check if
        the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: etag
      isRequired: true
      type:
      - str
    - description: The match condition to use upon the etag.
      id: match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: source_if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: source_if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'The source ETag value, or the wildcard character (*). Used to
        check if the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: source_etag
      isRequired: true
      type:
      - str
    - description: The source match condition to use upon the etag.
      id: source_match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    - description: The timeout parameter is expressed in seconds.
      id: timeout
      isRequired: true
      type:
      - int
    return:
      description: the renamed file client
      type:
      - DataLakeFileClient
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.rename_file
- class: azure.storage.filedatalake.DataLakeFileClient
  fullName: azure.storage.filedatalake.DataLakeFileClient.upload_data
  langs:
  - python
  module: azure.storage.filedatalake
  name: upload_data(data, length=None, overwrite=False, **kwargs)
  namewithoutparameters: upload_data
  summary: Upload data to a file.
  syntax:
    content: upload_data(data, length=None, overwrite=False, **kwargs)
    parameters:
    - description: Content to be uploaded to file
      id: data
      isRequired: true
    - defaultValue: None
      description: Size of the data in bytes.
      id: length
      type:
      - int
    - defaultValue: 'False'
      description: to overwrite an existing file or not.
      id: overwrite
      type:
      - bool
    - description: ContentSettings object used to set path properties.
      id: content_settings
      isRequired: true
      type:
      - azure.storage.filedatalake.ContentSettings
    - description: Name-value pairs associated with the blob as metadata.
      id: metadata
      isRequired: true
      type:
      - dict(str, str)
    - description: 'Required if the blob has an active lease. Value can be a DataLakeLeaseClient
        object

        or the lease ID as a string.'
      id: or str lease
      isRequired: true
      type:
      - azure.storage.filedatalake.DataLakeLeaseClient
    - description: 'Optional and only valid if Hierarchical Namespace is enabled for
        the account.

        When creating a file or directory and the parent folder does not have a default
        ACL,

        the umask restricts the permissions of the file or directory to be created.

        The resulting permission is given by p & ^u, where p is the permission and
        u is the umask.

        For example, if p is 0777 and u is 0057, then the resulting permission is
        0720.

        The default permission is 0777 for a directory and 0666 for a file. The default
        umask is 0027.

        The umask must be specified in 4-digit octal notation (e.g. 0766).'
      id: umask
      isRequired: true
      type:
      - str
    - description: 'Optional and only valid if Hierarchical Namespace

        is enabled for the account. Sets POSIX access permissions for the file

        owner, the file owning group, and others. Each class may be granted

        read, write, or execute permission.  The sticky bit is also supported.

        Both symbolic (rwxrw-rw-) and 4-digit octal notation (e.g. 0766) are

        supported.'
      id: permissions
      isRequired: true
      type:
      - str
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only

        if the resource has been modified since the specified time.'
      id: if_modified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'A DateTime value. Azure expects the date value passed in to be
        UTC.

        If timezone is included, any non-UTC datetimes will be converted to UTC.

        If a date is passed in without timezone info, it is assumed to be UTC.

        Specify this header to perform the operation only if

        the resource has not been modified since the specified date/time.'
      id: if_unmodified_since
      isRequired: true
      type:
      - datetime.datetime
    - description: 'An ETag value, or the wildcard character (*). Used to check if
        the resource has changed,

        and act according to the condition specified by the *match_condition* parameter.'
      id: etag
      isRequired: true
      type:
      - str
    - description: The match condition to use upon the etag.
      id: match_condition
      isRequired: true
      type:
      - azure.core.MatchConditions
    - description: The timeout parameter is expressed in seconds.
      id: timeout
      isRequired: true
      type:
      - int
    - description: 'The maximum chunk size for uploading a file in chunks.

        Defaults to 100*1024*1024, or 100MB.'
      id: chunk_size
      isRequired: true
      type:
      - int
    return:
      description: response dict (Etag and last modified).
  type: method
  uid: azure.storage.filedatalake.DataLakeFileClient.upload_data
references:
- fullName: azure.storage.filedatalake.DataLakeFileClient.append_data
  isExternal: false
  name: append_data(data, offset, length=None, **kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.append_data
- fullName: azure.storage.filedatalake.DataLakeFileClient.create_file
  isExternal: false
  name: create_file(content_settings=None, metadata=None, **kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.create_file
- fullName: azure.storage.filedatalake.DataLakeFileClient.delete_file
  isExternal: false
  name: delete_file(**kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.delete_file
- fullName: azure.storage.filedatalake.DataLakeFileClient.download_file
  isExternal: false
  name: download_file(offset=None, length=None, **kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.download_file
- fullName: azure.storage.filedatalake.DataLakeFileClient.flush_data
  isExternal: false
  name: flush_data(offset, retain_uncommitted_data=False, **kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.flush_data
- fullName: azure.storage.filedatalake.DataLakeFileClient.from_connection_string
  isExternal: false
  name: from_connection_string(conn_str, file_system_name, file_path, credential=None,
    **kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.from_connection_string
- fullName: azure.storage.filedatalake.DataLakeFileClient.get_file_properties
  isExternal: false
  name: get_file_properties(**kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.get_file_properties
- fullName: azure.storage.filedatalake.DataLakeFileClient.query_file
  isExternal: false
  name: query_file(query_expression, **kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.query_file
- fullName: azure.storage.filedatalake.DataLakeFileClient.rename_file
  isExternal: false
  name: rename_file(new_name, **kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.rename_file
- fullName: azure.storage.filedatalake.DataLakeFileClient.upload_data
  isExternal: false
  name: upload_data(data, length=None, overwrite=False, **kwargs)
  parent: azure.storage.filedatalake.DataLakeFileClient
  uid: azure.storage.filedatalake.DataLakeFileClient.upload_data
- fullName: dict(str, str)
  name: dict(str, str)
  spec.python:
  - fullName: dict
    name: dict
    uid: dict
  - fullName: (
    name: (
  - fullName: str
    name: str
    uid: str
  - fullName: ', '
    name: ', '
  - fullName: str
    name: str
    uid: str
  - fullName: )
    name: )
  uid: dict(str, str)
- fullName: Callable[Exception]
  name: Callable[Exception]
  spec.python:
  - fullName: Callable
    name: Callable
    uid: Callable
  - fullName: '['
    name: '['
  - fullName: Exception
    name: Exception
    uid: Exception
  - fullName: ']'
    name: ']'
  uid: Callable[Exception]
