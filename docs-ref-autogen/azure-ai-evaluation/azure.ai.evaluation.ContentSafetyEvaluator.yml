### YamlMime:PythonClass
uid: azure.ai.evaluation.ContentSafetyEvaluator
name: ContentSafetyEvaluator
fullName: azure.ai.evaluation.ContentSafetyEvaluator
module: azure.ai.evaluation
summary: '> [!NOTE]

  > This is an experimental class, and may change at any time. Please see [https://aka.ms/azuremlexperimental](https://aka.ms/azuremlexperimental)
  for more information.

  >


  Initialize a content safety evaluator configured to evaluate content safety metrics
  for QA scenario.'
constructor:
  syntax: 'ContentSafetyEvaluator(credential, azure_ai_project, *, violence_threshold:
    int = 3, sexual_threshold: int = 3, self_harm_threshold: int = 3, hate_unfairness_threshold:
    int = 3, **kwargs)'
  parameters:
  - name: credential
    description: The credential for connecting to Azure AI project. Required
    isRequired: true
    types:
    - <xref:azure.core.credentials.TokenCredential>
  - name: azure_ai_project
    description: 'The Azure AI project, which can either be a string representing
      the project endpoint

      or an instance of AzureAIProject. It contains subscription id, resource group,
      and project name.'
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:str>, <xref:azure.ai.evaluation.AzureAIProject>]
  - name: violence_threshold
    description: The threshold for violence evaluation. Default is 3.
    isRequired: true
    types:
    - <xref:int>
  - name: sexual_threshold
    description: The threshold for sexual content evaluation. Default is 3.
    isRequired: true
    types:
    - <xref:int>
  - name: self_harm_threshold
    description: The threshold for self-harm evaluation. Default is 3.
    isRequired: true
    types:
    - <xref:int>
  - name: hate_unfairness_threshold
    description: The threshold for hate/unfairness evaluation. Default is 3.
    isRequired: true
    types:
    - <xref:int>
  - name: kwargs
    description: Additional arguments to pass to the evaluator.
    isRequired: true
    types:
    - <xref:typing.Any>
  keywordOnlyParameters:
  - name: violence_threshold
    defaultValue: '3'
  - name: sexual_threshold
    defaultValue: '3'
  - name: self_harm_threshold
    defaultValue: '3'
  - name: hate_unfairness_threshold
    defaultValue: '3'
examples:
- "Initialize with threshold and call a ContentSafetyEvaluator with a query and response.<!--[!code-python[Main](les\\\
  evaluation_samples_threshold.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
  : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\ToolCache\\\
  \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\15\\\
  \\azure_ai_evaluation-1.9.0\\\\samples\\\\evaluation_samples_threshold.py\", \"\
  xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   import os\n\
  \   from azure.identity import DefaultAzureCredential\n   from azure.ai.evaluation\
  \ import ContentSafetyEvaluator\n\n   azure_ai_project = {\n       \"subscription_id\"\
  : os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n       \"resource_group_name\": os.environ.get(\"\
  AZURE_RESOURCE_GROUP_NAME\"),\n       \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"\
  ),\n   }\n   credential = DefaultAzureCredential()\n\n   chat_eval = ContentSafetyEvaluator(azure_ai_project=azure_ai_project,\
  \ credential=credential, threshold=3)\n\n   chat_eval(\n       query=\"What is the\
  \ capital of France?\",\n       response=\"Paris\",\n   )\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.ContentSafetyEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'content_safety'
