### YamlMime:PythonClass
uid: azure.ai.evaluation.F1ScoreEvaluator
name: F1ScoreEvaluator
fullName: azure.ai.evaluation.F1ScoreEvaluator
module: azure.ai.evaluation
summary: 'Calculates the F1 score for a given response and ground truth or a multi-turn
  conversation.


  F1 Scores range from 0 to 1, with 1 being the best possible score.


  The F1-score computes the ratio of the number of shared words between the model
  generation and

  the ground truth. Ratio is computed over the individual words in the generated response
  against those in the ground

  truth answer. The number of shared words between the generation and the truth is
  the basis of the F1 score:

  precision is the ratio of the number of shared words to the total number of words
  in the generation, and recall

  is the ratio of the number of shared words to the total number of words in the ground
  truth.


  Use the F1 score when you want a single comprehensive metric that combines both
  recall and precision in your

  model''s responses. It provides a balanced evaluation of your model''s performance
  in terms of capturing accurate

  information in the response.'
constructor:
  syntax: F1ScoreEvaluator(*, threshold=0.5)
  parameters:
  - name: threshold
    description: The threshold for the F1 score evaluator. Default is 0.5.
    isRequired: true
    types:
    - <xref:float>
  keywordOnlyParameters:
  - name: threshold
    defaultValue: '0.5'
examples:
- "Initialize with threshold and call an F1ScoreEvaluator.<!--[!code-python[Main](les\\\
  evaluation_samples_threshold.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
  : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\ToolCache\\\
  \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\15\\\
  \\azure_ai_evaluation-1.11.1\\\\samples\\\\evaluation_samples_threshold.py\", \"\
  xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation\
  \ import F1ScoreEvaluator\n\n   f1_evaluator = F1ScoreEvaluator(threshold=0.6)\n\
  \   f1_evaluator(response=\"Lyon is the capital of France.\", ground_truth=\"Paris\
  \ is the capital of France.\")\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.F1ScoreEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'azureai://built-in/evaluators/f1_score'
