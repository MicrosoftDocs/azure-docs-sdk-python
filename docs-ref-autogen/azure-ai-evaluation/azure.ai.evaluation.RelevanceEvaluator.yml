### YamlMime:PythonClass
uid: azure.ai.evaluation.RelevanceEvaluator
name: RelevanceEvaluator
fullName: azure.ai.evaluation.RelevanceEvaluator
module: azure.ai.evaluation
inheritances:
- azure.ai.evaluation._evaluators._common._base_prompty_eval.PromptyEvaluatorBase
summary: 'Evaluates relevance score for a given query and response or a multi-turn
  conversation, including reasoning.


  The relevance measure assesses the ability of answers to capture the key points
  of the context.

  High relevance scores signify the AI system''s understanding of the input and its
  capability to produce coherent

  and contextually appropriate outputs. Conversely, low relevance scores indicate
  that generated responses might

  be off-topic, lacking in context, or insufficient in addressing the user''s intended
  queries. Use the relevance

  metric when evaluating the AI system''s performance in understanding the input and
  generating contextually

  appropriate responses.


  Relevance scores range from 1 to 5, with 1 being the worst and 5 being the best.


  > [!NOTE]

  > To align with our support of a diverse set of models, an output key without the
  gpt_ prefix has been added.

  >

  > To maintain backwards compatibility, the old key with the gpt_ prefix is still
  be present in the output;

  >

  > however, it is recommended to use the new key moving forward as the old key will
  be deprecated in the future.

  >'
constructor:
  syntax: RelevanceEvaluator(model_config)
  parameters:
  - name: model_config
    description: Configuration for the Azure OpenAI model.
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:azure.ai.evaluation.AzureOpenAIModelConfiguration>,
      <xref:azure.ai.evaluation.OpenAIModelConfiguration>]
examples:
- "Initialize and call a RelevanceEvaluator with a query, response, and context.<!--[!code-python[Main](les\\\
  evaluation_samples_evaluate.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
  : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\
  \\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\
  \\4\\\\azure_ai_evaluation-1.2.0\\\\samples\\\\evaluation_samples_evaluate.py\"\
  , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   import os\n\
  \   from azure.ai.evaluation import RelevanceEvaluator\n\n   model_config = {\n\
  \       \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n       \"\
  api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n       \"azure_deployment\": os.environ.get(\"\
  AZURE_OPENAI_DEPLOYMENT\"),\n   }\n\n   relevance_eval = RelevanceEvaluator(model_config=model_config)\n\
  \   relevance_eval(\n       query=\"What is the capital of Japan?\",\n       response=\"\
  The capital of Japan is Tokyo.\",\n   )\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.RelevanceEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'azureml://registries/azureml/models/Relevance-Evaluator/versions/4'
