### YamlMime:PythonPackage
uid: azure.ai.evaluation
name: evaluation
fullName: azure.ai.evaluation
type: rootImport
functions:
- uid: azure.ai.evaluation.evaluate
  name: evaluate
  summary: "Evaluates target or data with built-in or custom evaluators. If both target\
    \ and data are provided,\n   data will be run through target function and then\
    \ results will be evaluated."
  signature: 'evaluate(*, data: str | PathLike, evaluators: Dict[str, Callable | AzureOpenAIGrader],
    evaluation_name: str | None = None, target: Callable | None = None, evaluator_config:
    Dict[str, EvaluatorConfig] | None = None, azure_ai_project: str | AzureAIProject
    | None = None, output_path: str | PathLike | None = None, fail_on_evaluator_errors:
    bool = False, **kwargs) -> EvaluationResult'
  keywordOnlyParameters:
  - name: data
    description: 'Path to the data to be evaluated or passed to target if target is
      set.

      JSONL and CSV files are supported.  *target* and *data* both cannot be None.
      Required.'
    types:
    - <xref:str>
  - name: evaluators
    description: 'Evaluators to be used for evaluation. It should be a dictionary
      with key as alias for evaluator

      and value as the evaluator function. Also accepts AzureOpenAIGrader instances
      as values, which are processed separately.

      Required.'
    types:
    - <xref:typing.Dict>[<xref:str>, <xref:typing.Union>[<xref:typing.Callable>, <xref:azure.ai.evaluation.AzureOpenAIGrader>]]
  - name: evaluation_name
    description: Display name of the evaluation.
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: target
    description: Target to be evaluated. *target* and *data* both cannot be None
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:typing.Callable>]
  - name: evaluator_config
    description: 'Configuration for evaluators. The configuration should be a dictionary
      with evaluator

      names as keys and a values that are dictionaries containing the column mappings.
      The column mappings should

      be a dictionary with keys as the column names in the evaluator input and values
      as the column names in the

      input data or data generated by target.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>[<xref:str>, <xref:azure.ai.evaluation.EvaluatorConfig>]]
  - name: output_path
    description: 'The local folder or file path to save evaluation results to if set.
      If folder path is provided

      the results will be saved to a file named *evaluation_results.json* in the folder.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: azure_ai_project
    description: 'The Azure AI project, which can either be a string representing
      the project endpoint

      or an instance of AzureAIProject. It contains subscription id, resource group,
      and project name.'
    defaultValue: None
    types:
    - <xref:typing.Optional>[<xref:typing.Union>[<xref:str>, <xref:azure.ai.evaluation.AzureAIProject>]]
  - name: fail_on_evaluator_errors
    description: 'Whether or not the evaluation should cancel early with an EvaluationException

      if ANY evaluator fails during their evaluation.

      Defaults to false, which means that evaluations will continue regardless of
      failures.

      If such failures occur, metrics may be missing, and evidence of failures can
      be found in the evaluation''s logs.'
    defaultValue: 'False'
    types:
    - <xref:bool>
  - name: user_agent
    description: A string to append to the default user-agent sent with evaluation
      http requests
    types:
    - <xref:typing.Optional>[<xref:str>]
  return:
    description: Evaluation results.
    types:
    - <xref:azure.ai.evaluation.EvaluationResult>
  examples:
  - "Run an evaluation on local data with one or more evaluators using Azure AI Project\
    \ URL in following format\n[https:/](https:/)/{resource_name}.services.ai.azure.com/api/projects/{project_name}<!--[!code-python[Main](les\\\
    evaluation_samples_evaluate_fdp.py )]-->\n\n<!-- literal_block {\"ids\": [], \"\
    classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\":\
    \ \"C:\\\\ToolCache\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\
    \\dist_temp\\\\15\\\\azure_ai_evaluation-1.9.0\\\\samples\\\\evaluation_samples_evaluate_fdp.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   import os\n   from azure.ai.evaluation import evaluate, RelevanceEvaluator,\
    \ CoherenceEvaluator, IntentResolutionEvaluator\n\n   model_config = {\n     \
    \  \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),  # https://<account_name>.services.ai.azure.com\n\
    \       \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n       \"azure_deployment\"\
    : os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n   }\n\n   print(os.getcwd())\n\
    \   path = \"./sdk/evaluation/azure-ai-evaluation/samples/data/evaluate_test_data.jsonl\"\
    \n\n   evaluate(\n       data=path,\n       evaluators={\n           \"coherence\"\
    : CoherenceEvaluator(model_config=model_config),\n           \"relevance\": RelevanceEvaluator(model_config=model_config),\n\
    \           \"intent_resolution\": IntentResolutionEvaluator(model_config=model_config),\n\
    \       },\n       evaluator_config={\n           \"coherence\": {\n         \
    \      \"column_mapping\": {\n                   \"response\": \"${data.response}\"\
    ,\n                   \"query\": \"${data.query}\",\n               },\n     \
    \      },\n           \"relevance\": {\n               \"column_mapping\": {\n\
    \                   \"response\": \"${data.response}\",\n                   \"\
    context\": \"${data.context}\",\n                   \"query\": \"${data.query}\"\
    ,\n               },\n           },\n       },\n   )\n\n\n   ````\n"
classes:
- azure.ai.evaluation.AzureAIProject
- azure.ai.evaluation.AzureOpenAIGrader
- azure.ai.evaluation.AzureOpenAILabelGrader
- azure.ai.evaluation.AzureOpenAIModelConfiguration
- azure.ai.evaluation.AzureOpenAIScoreModelGrader
- azure.ai.evaluation.AzureOpenAIStringCheckGrader
- azure.ai.evaluation.AzureOpenAITextSimilarityGrader
- azure.ai.evaluation.BleuScoreEvaluator
- azure.ai.evaluation.CodeVulnerabilityEvaluator
- azure.ai.evaluation.CoherenceEvaluator
- azure.ai.evaluation.ContentSafetyEvaluator
- azure.ai.evaluation.Conversation
- azure.ai.evaluation.EvaluationResult
- azure.ai.evaluation.EvaluatorConfig
- azure.ai.evaluation.F1ScoreEvaluator
- azure.ai.evaluation.FluencyEvaluator
- azure.ai.evaluation.GleuScoreEvaluator
- azure.ai.evaluation.GroundednessEvaluator
- azure.ai.evaluation.GroundednessProEvaluator
- azure.ai.evaluation.HateUnfairnessEvaluator
- azure.ai.evaluation.IndirectAttackEvaluator
- azure.ai.evaluation.IntentResolutionEvaluator
- azure.ai.evaluation.Message
- azure.ai.evaluation.MeteorScoreEvaluator
- azure.ai.evaluation.OpenAIModelConfiguration
- azure.ai.evaluation.ProtectedMaterialEvaluator
- azure.ai.evaluation.QAEvaluator
- azure.ai.evaluation.RelevanceEvaluator
- azure.ai.evaluation.ResponseCompletenessEvaluator
- azure.ai.evaluation.RetrievalEvaluator
- azure.ai.evaluation.RougeScoreEvaluator
- azure.ai.evaluation.SelfHarmEvaluator
- azure.ai.evaluation.SexualEvaluator
- azure.ai.evaluation.SimilarityEvaluator
- azure.ai.evaluation.TaskAdherenceEvaluator
- azure.ai.evaluation.ToolCallAccuracyEvaluator
- azure.ai.evaluation.UngroundedAttributesEvaluator
- azure.ai.evaluation.ViolenceEvaluator
packages:
- azure.ai.evaluation.red_team
- azure.ai.evaluation.simulator
enums:
- azure.ai.evaluation.RougeType
