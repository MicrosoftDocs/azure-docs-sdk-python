### YamlMime:PythonClass
uid: azure.ai.evaluation.ResponseCompletenessEvaluator
name: ResponseCompletenessEvaluator
fullName: azure.ai.evaluation.ResponseCompletenessEvaluator
module: azure.ai.evaluation
summary: "> [!NOTE]\n> This is an experimental class, and may change at any time.\
  \ Please see [https://aka.ms/azuremlexperimental](https://aka.ms/azuremlexperimental)\
  \ for more information.\n>\n\nEvaluates the extent to which a given response contains\
  \ all necessary and relevant information with respect to the\n   provided ground\
  \ truth.\n\nThe completeness measure assesses how thoroughly an AI model's generated\
  \ response aligns with the key information,\nclaims, and statements established\
  \ in the ground truth. This evaluation considers the presence, accuracy,\nand relevance\
  \ of the content provided.\nThe assessment spans multiple levels, ranging from fully\
  \ incomplete to fully complete, ensuring a comprehensive\nevaluation of the response's\
  \ content quality.\nUse this metric when you need to evaluate an AI model's ability\
  \ to deliver comprehensive and accurate information,\nparticularly in text generation\
  \ tasks where conveying all essential details is crucial for clarity,\ncontext,\
  \ and correctness.\nCompleteness scores range from 1 to 5:\n1: Fully incomplete\
  \ \u2014 Contains none of the necessary information.\n2: Barely complete \u2014\
  \ Contains only a small portion of the required information.\n3: Moderately complete\
  \ \u2014 Covers about half of the required content.\n4: Mostly complete \u2014 Includes\
  \ most of the necessary details with minimal omissions.\n5: Fully complete \u2014\
  \ Contains all key information without any omissions.\n:param model_config: Configuration\
  \ for the Azure OpenAI model.\n:type model_config: Union[~azure.ai.evaluation.AzureOpenAIModelConfiguration,\n\
  \n   ~azure.ai.evaluation.OpenAIModelConfiguration]"
constructor:
  syntax: 'ResponseCompletenessEvaluator(model_config, *, threshold: float | None
    = 3, credential=None, **kwargs)'
  parameters:
  - name: model_config
    isRequired: true
  keywordOnlyParameters:
  - name: threshold
    defaultValue: '3'
  - name: credential
    defaultValue: None
examples:
- "Initialize and call CompletenessEvaluator using Azure AI Project URL in the following\
  \ format\n[https:/](https:/)/{resource_name}.services.ai.azure.com/api/projects/{project_name}<!--[!code-python[Main](les\\\
  evaluation_samples_evaluate_fdp.py )]-->\n\n<!-- literal_block {\"ids\": [], \"\
  classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"\
  C:\\\\ToolCache\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\
  \\dist_temp\\\\15\\\\azure_ai_evaluation-1.11.1\\\\samples\\\\evaluation_samples_evaluate_fdp.py\"\
  , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   import os\n\
  \   from azure.ai.evaluation import CompletenessEvaluator\n\n   model_config = {\n\
  \       \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),  # https://<account_name>.services.ai.azure.com\n\
  \       \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n       \"azure_deployment\"\
  : os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n   }\n\n   completeness_eval = CompletenessEvaluator(model_config=model_config)\n\
  \   completeness_eval(\n       response=\"The capital of Japan is Tokyo.\",\n  \
  \     ground_truth=\"Tokyo is Japan's capital.\",\n   )\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.ResponseCompletenessEvaluator.id
  name: id
  signature: id = 'azureai://built-in/evaluators/response_completeness'
