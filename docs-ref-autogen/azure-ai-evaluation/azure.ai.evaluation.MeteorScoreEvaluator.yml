### YamlMime:PythonClass
uid: azure.ai.evaluation.MeteorScoreEvaluator
name: MeteorScoreEvaluator
fullName: azure.ai.evaluation.MeteorScoreEvaluator
module: azure.ai.evaluation
inheritances:
- azure.ai.evaluation._evaluators._common._base_eval.EvaluatorBase
summary: 'Calculates the METEOR score for a given response and ground truth.


  The METEOR (Metric for Evaluation of Translation with Explicit Ordering) score grader
  evaluates generated text by

  comparing it to reference texts, focusing on precision, recall, and content alignment.
  It addresses limitations of

  other metrics like BLEU by considering synonyms, stemming, and paraphrasing. METEOR
  score considers synonyms and

  word stems to more accurately capture meaning and language variations. In addition
  to machine translation and

  text summarization, paraphrase detection is an optimal use case for the METEOR score.


  Use the METEOR score when you want a more linguistically informed evaluation metric
  that captures not only

  n-gram overlap but also accounts for synonyms, stemming, and word order. This is
  particularly useful for evaluating

  tasks like machine translation, text summarization, and text generation.


  The METEOR score ranges from 0 to 1, with 1 indicating a perfect match.'
constructor:
  syntax: 'MeteorScoreEvaluator(alpha: float = 0.9, beta: float = 3.0, gamma: float
    = 0.5)'
  parameters:
  - name: alpha
    description: The METEOR score alpha parameter. Default is 0.9.
    defaultValue: '0.9'
    types:
    - <xref:float>
  - name: beta
    description: The METEOR score beta parameter. Default is 3.0.
    defaultValue: '3.0'
    types:
    - <xref:float>
  - name: gamma
    description: The METEOR score gamma parameter. Default is 0.5.
    defaultValue: '0.5'
    types:
    - <xref:float>
examples:
- "Initialize and call a MeteorScoreEvaluator with alpha of 0.8.<!--[!code-python[Main](les\\\
  evaluation_samples_evaluate.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
  : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\
  \\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\
  \\4\\\\azure_ai_evaluation-1.2.0\\\\samples\\\\evaluation_samples_evaluate.py\"\
  , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation\
  \ import MeteorScoreEvaluator\n\n   meteor_evaluator = MeteorScoreEvaluator(alpha=0.8)\n\
  \   meteor_evaluator(response=\"Paris is the capital of France.\", ground_truth=\"\
  France's capital is Paris.\")\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.MeteorScoreEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'azureml://registries/azureml/models/Meteor-Score-Evaluator/versions/3'
