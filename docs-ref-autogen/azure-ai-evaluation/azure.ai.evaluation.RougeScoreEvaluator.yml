### YamlMime:PythonClass
uid: azure.ai.evaluation.RougeScoreEvaluator
name: RougeScoreEvaluator
fullName: azure.ai.evaluation.RougeScoreEvaluator
module: azure.ai.evaluation
summary: 'Calculates the ROUGE score for a given response and ground truth.


  The ROUGE score (Recall-Oriented Understudy for Gisting Evaluation) evaluates the
  similarity between the

  generated text and reference text based on n-gram overlap, including ROUGE-N (unigram,
  bigram, etc.), and

  ROUGE-L (longest common subsequence). It calculates precision, recall, and F1 scores
  to capture how well

  the generated text matches the reference text. Rouge type options are "rouge1" (Unigram
  overlap), "rouge2"

  (Bigram overlap), "rouge3" (Trigram overlap), "rouge4" (4-gram overlap), "rouge5"
  (5-gram overlap), "rougeL"

  (L-graph overlap)


  Use the ROUGE score when you need a robust evaluation metric for text summarization,
  machine translation, and

  other natural language processing tasks, especially when focusing on recall and
  the ability to capture relevant

  information from the reference text.


  ROUGE scores range from 0 to 1, with higher scores indicating better quality.'
constructor:
  syntax: 'RougeScoreEvaluator(rouge_type: RougeType)'
  parameters:
  - name: rouge_type
    isRequired: true
examples:
- "Initialize and call a RougeScoreEvaluator with a four-gram rouge type.<!--[!code-python[Main](les\\\
  evaluation_samples_evaluate.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
  : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\
  \\windows\\\\Python\\\\3.11.10\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\
  \\14\\\\azure_ai_evaluation-1.2.0\\\\samples\\\\evaluation_samples_evaluate.py\"\
  , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation\
  \ import RougeScoreEvaluator, RougeType\n\n   rouge_evaluator = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_4)\n\
  \   rouge_evaluator(response=\"Paris is the capital of France.\", ground_truth=\"\
  France's capital is Paris.\")\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.RougeScoreEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'azureml://registries/azureml/models/Rouge-Score-Evaluator/versions/3'
