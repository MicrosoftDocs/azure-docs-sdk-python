### YamlMime:PythonClass
uid: azure.ai.evaluation.RougeScoreEvaluator
name: RougeScoreEvaluator
fullName: azure.ai.evaluation.RougeScoreEvaluator
module: azure.ai.evaluation
summary: 'Calculates the ROUGE score for a given response and ground truth.


  The ROUGE score (Recall-Oriented Understudy for Gisting Evaluation) evaluates the
  similarity between the

  generated text and reference text based on n-gram overlap, including ROUGE-N (unigram,
  bigram, etc.), and

  ROUGE-L (longest common subsequence). It calculates precision, recall, and F1 scores
  to capture how well

  the generated text matches the reference text. Rouge type options are "rouge1" (Unigram
  overlap), "rouge2"

  (Bigram overlap), "rouge3" (Trigram overlap), "rouge4" (4-gram overlap), "rouge5"
  (5-gram overlap), "rougeL"

  (L-graph overlap)


  Use the ROUGE score when you need a robust evaluation metric for text summarization,
  machine translation, and

  other natural language processing tasks, especially when focusing on recall and
  the ability to capture relevant

  information from the reference text.


  ROUGE scores range from 0 to 1, with higher scores indicating better quality.

  :param rouge_type: The type of ROUGE score to calculate. Default is "rouge1".

  :type rouge_type: str

  :param precision_threshold: The threshold value to determine if the precision evaluation
  passes or fails. Default is 0.5.

  :type precision_threshold: float

  :param recall_threshold: The threshold value to determine if the recall evaluation
  passes or fails. Default is 0.5.

  :type recall_threshold: float

  :param f1_score_threshold: The threshold value to determine if the F1 score evaluation
  passes or fails. Default is 0.5.

  :type f1_score_threshold: float'
constructor:
  syntax: 'RougeScoreEvaluator(rouge_type: RougeType, *, precision_threshold: float
    = 0.5, recall_threshold: float = 0.5, f1_score_threshold: float = 0.5)'
  parameters:
  - name: rouge_type
    isRequired: true
  keywordOnlyParameters:
  - name: precision_threshold
    defaultValue: '0.5'
  - name: recall_threshold
    defaultValue: '0.5'
  - name: f1_score_threshold
    defaultValue: '0.5'
examples:
- "Initialize with a specified threshold and call a RougeScoreEvaluator with a four-gram\
  \ rouge type.<!--[!code-python[Main](les\\evaluation_samples_threshold.py )]-->\n\
  \n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
  : [], \"backrefs\": [], \"source\": \"C:\\\\ToolCache\\\\Python\\\\3.11.9\\\\x64\\\
  \\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\15\\\\azure_ai_evaluation-1.9.0\\\
  \\samples\\\\evaluation_samples_threshold.py\", \"xml:space\": \"preserve\", \"\
  force\": false, \"language\": \"python\", \"highlight_args\": {\"linenostart\":\
  \ 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation import\
  \ RougeScoreEvaluator, RougeType\n\n   rouge_evaluator = RougeScoreEvaluator(\n\
  \       rouge_type=RougeType.ROUGE_4, precision_threshold=0.5, recall_threshold=0.5,\
  \ f1_score_threshold=0.5\n   )\n   rouge_evaluator(response=\"Paris is the capital\
  \ of France.\", ground_truth=\"France's capital is Paris.\")\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.RougeScoreEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'azureml://registries/azureml/models/Rouge-Score-Evaluator/versions/3'
