### YamlMime:PythonClass
uid: azure.ai.evaluation.AzureOpenAIPythonGrader
name: AzureOpenAIPythonGrader
fullName: azure.ai.evaluation.AzureOpenAIPythonGrader
module: azure.ai.evaluation
summary: "> [!NOTE]\n> This is an experimental class, and may change at any time.\
  \ Please see [https://aka.ms/azuremlexperimental](https://aka.ms/azuremlexperimental)\
  \ for more information.\n>\n\nWrapper class for OpenAI's Python code graders.\n\n\
  Enables custom Python-based evaluation logic with flexible scoring and\npass/fail\
  \ thresholds. The grader executes user-provided Python code\nto evaluate outputs\
  \ against custom criteria.\n\nSupplying a PythonGrader to the *evaluate* method\
  \ will cause an\nasynchronous request to evaluate the grader via the OpenAI API.\
  \ The\nresults of the evaluation will then be merged into the standard\nevaluation\
  \ results.\n\n]\n:param name: The name of the grader.\n:type name: str\n:param image_tag:\
  \ The image tag for the Python execution environment.\n:type image_tag: str\n:param\
  \ pass_threshold: Score threshold for pass/fail classification.\n\n   Scores >=\
  \ threshold are considered passing."
constructor:
  syntax: 'AzureOpenAIPythonGrader(*, model_config: AzureOpenAIModelConfiguration
    | OpenAIModelConfiguration, name: str, image_tag: str, pass_threshold: float,
    source: str, **kwargs: Any)'
  parameters:
  - name: model_config
    description: The model configuration to use for the grader.
    isRequired: true
    types:
    - <xref:typing.Union>[ <xref:azure.ai.evaluation.AzureOpenAIModelConfiguration>,
      <xref:azure.ai.evaluation.OpenAIModelConfiguration>
  - name: source
    description: 'Python source code containing the grade function.

      Must define: def grade(sample: dict, item: dict) -> float'
    isRequired: true
    types:
    - <xref:str>
  - name: kwargs
    description: Additional keyword arguments to pass to the grader.
    isRequired: true
    types:
    - <xref:typing.Any>
  keywordOnlyParameters:
  - name: model_config
    isRequired: true
  - name: name
    isRequired: true
  - name: image_tag
    isRequired: true
  - name: pass_threshold
    isRequired: true
  - name: source
    isRequired: true
examples:
- "Using AzureOpenAIPythonGrader for custom evaluation logic.<!--[!code-python[Main](les\\\
  evaluation_samples_common.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
  : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\ToolCache\\\
  \\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\15\\\
  \\azure_ai_evaluation-1.11.1\\\\samples\\\\evaluation_samples_common.py\", \"xml:space\"\
  : \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\":\
  \ {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation\
  \ import AzureOpenAIPythonGrader, evaluate\n   from azure.ai.evaluation._model_configurations\
  \ import AzureOpenAIModelConfiguration\n   import os\n\n   # Configure your Azure\
  \ OpenAI connection\n   model_config = AzureOpenAIModelConfiguration(\n       azure_endpoint=os.environ[\"\
  AZURE_OPENAI_ENDPOINT\"],\n       api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n\
  \       api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n       azure_deployment=os.environ[\"\
  MODEL_DEPLOYMENT_NAME\"],\n   )\n\n   # Create a Python grader with custom evaluation\
  \ logic\n   python_grader = AzureOpenAIPythonGrader(\n       model_config=model_config,\n\
  \       name=\"custom_accuracy\",\n       image_tag=\"2025-05-08\",\n       pass_threshold=0.8,\
  \  # 80% threshold for passing\n       source=\"\"\"\n   def grade(sample: dict,\
  \ item: dict) -> float:\n       \\\"\\\"\\\"\n       Custom grading logic that compares\
  \ model output to expected label.\n       \n       Args:\n           sample: Dictionary\
  \ that is typically empty in Azure AI Evaluation\n           item: Dictionary containing\
  \ ALL the data including model output and ground truth\n       \n       Returns:\n\
  \           Float score between 0.0 and 1.0\n       \\\"\\\"\\\"\n       # Important:\
  \ In Azure AI Evaluation, all data is in 'item', not 'sample'\n       # The 'sample'\
  \ parameter is typically an empty dictionary\n       \n       # Get the model's\
  \ response/output from item\n       output = item.get(\"response\", \"\") or item.get(\"\
  output\", \"\") or item.get(\"output_text\", \"\")\n       output = output.lower()\n\
  \       \n       # Get the expected label/ground truth from item\n       label =\
  \ item.get(\"ground_truth\", \"\") or item.get(\"label\", \"\") or item.get(\"expected\"\
  , \"\")\n       label = label.lower()\n       \n       # Handle empty cases\n  \
  \     if not output or not label:\n           return 0.0\n       \n       # Exact\
  \ match gets full score\n       if output == label:\n           return 1.0\n   \
  \    \n       # Partial match logic (customize as needed)\n       if output in label\
  \ or label in output:\n           return 0.5\n       \n       return 0.0\n   \"\"\
  \",\n   )\n\n   # Run evaluation\n   evaluation_result = evaluate(\n       data=\"\
  evaluation_data.jsonl\",  # JSONL file with columns: query, response, ground_truth,\
  \ etc.\n       evaluators={\"custom_accuracy\": python_grader},\n   )\n\n   # Access\
  \ results\n   print(f\"Pass rate: {evaluation_result['metrics']['custom_accuracy.pass_rate']}\"\
  )\n\n   ````\n"
methods:
- uid: azure.ai.evaluation.AzureOpenAIPythonGrader.get_client
  name: get_client
  summary: 'Construct an appropriate OpenAI client using this grader''s model configuration.

    Returns a slightly different client depending on whether or not this grader''s
    model

    configuration is for Azure OpenAI or OpenAI.'
  signature: get_client() -> Any
  return:
    description: The OpenAI client.
    types:
    - '[<xref:openai.OpenAI>, <xref:openai.AzureOpenAI>]'
attributes:
- uid: azure.ai.evaluation.AzureOpenAIPythonGrader.id
  name: id
  signature: id = 'azureai://built-in/evaluators/azure-openai/python_grader'
