### YamlMime:PythonClass
uid: azure.ai.evaluation.AzureOpenAIPythonGrader
name: AzureOpenAIPythonGrader
fullName: azure.ai.evaluation.AzureOpenAIPythonGrader
module: azure.ai.evaluation
summary: '> [!NOTE]

  > This is an experimental class, and may change at any time. Please see [https://aka.ms/azuremlexperimental](https://aka.ms/azuremlexperimental)
  for more information.

  >


  Wrapper class for OpenAI''s Python code graders.


  Enables custom Python-based evaluation logic with flexible scoring and

  pass/fail thresholds. The grader executes user-provided Python code

  to evaluate outputs against custom criteria.


  Supplying a PythonGrader to the *evaluate* method will cause an

  asynchronous request to evaluate the grader via the OpenAI API. The

  results of the evaluation will then be merged into the standard

  evaluation results.'
constructor:
  syntax: 'AzureOpenAIPythonGrader(*, model_config: AzureOpenAIModelConfiguration
    | OpenAIModelConfiguration, name: str, pass_threshold: float, source: str, image_tag:
    str | None = None, credential: TokenCredential | None = None, **kwargs: Any)'
  parameters:
  - name: model_config
    description: The model configuration to use for the grader.
    isRequired: true
    types:
    - <xref:typing.Union>[<xref:azure.ai.evaluation.AzureOpenAIModelConfiguration>,
      <xref:azure.ai.evaluation.OpenAIModelConfiguration>]
  - name: name
    description: The name of the grader.
    isRequired: true
    types:
    - <xref:str>
  - name: image_tag
    description: The image tag for the Python execution environment.
    isRequired: true
    types:
    - <xref:str>
  - name: pass_threshold
    description: Score threshold for pass/fail classification. Scores >= threshold
      are considered passing.
    isRequired: true
    types:
    - <xref:float>
  - name: source
    description: 'Python source code containing the grade function.

      Must define: def grade(sample: dict, item: dict) -> float'
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: The credential to use to authenticate to the model. Only applicable
      to AzureOpenAI models.
    isRequired: true
    types:
    - <xref:azure.core.credentials.TokenCredential>
  - name: kwargs
    description: Additional keyword arguments to pass to the grader.
    isRequired: true
    types:
    - <xref:typing.Any>
  keywordOnlyParameters:
  - name: model_config
    isRequired: true
  - name: name
    isRequired: true
  - name: pass_threshold
    isRequired: true
  - name: source
    isRequired: true
  - name: image_tag
    defaultValue: None
  - name: credential
    defaultValue: None
examples:
- "Using AzureOpenAIPythonGrader for custom evaluation logic.<!--[!code-python[Main](les\\\
  evaluation_samples_common.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
  : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\ToolCache\\\
  \\Python\\\\3.12.10\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\\
  15\\\\azure_ai_evaluation-1.14.0\\\\samples\\\\evaluation_samples_common.py\", \"\
  xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation\
  \ import AzureOpenAIPythonGrader, evaluate\n   from azure.ai.evaluation._model_configurations\
  \ import AzureOpenAIModelConfiguration\n   import os\n\n   # Configure your Azure\
  \ OpenAI connection\n   model_config = AzureOpenAIModelConfiguration(\n       azure_endpoint=os.environ[\"\
  AZURE_OPENAI_ENDPOINT\"],\n       api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n\
  \       api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n       azure_deployment=os.environ[\"\
  MODEL_DEPLOYMENT_NAME\"],\n   )\n\n   # Create a Python grader with custom evaluation\
  \ logic\n   python_grader = AzureOpenAIPythonGrader(\n       model_config=model_config,\n\
  \       name=\"custom_accuracy\",\n       image_tag=\"2025-05-08\",\n       pass_threshold=0.8,\
  \  # 80% threshold for passing\n       source=\"\"\"\n   def grade(sample: dict,\
  \ item: dict) -> float:\n       \\\"\\\"\\\"\n       Custom grading logic that compares\
  \ model output to expected label.\n       \n       Args:\n           sample: Dictionary\
  \ that is typically empty in Azure AI Evaluation\n           item: Dictionary containing\
  \ ALL the data including model output and ground truth\n       \n       Returns:\n\
  \           Float score between 0.0 and 1.0\n       \\\"\\\"\\\"\n       # Important:\
  \ In Azure AI Evaluation, all data is in 'item', not 'sample'\n       # The 'sample'\
  \ parameter is typically an empty dictionary\n       \n       # Get the model's\
  \ response/output from item\n       output = item.get(\"response\", \"\") or item.get(\"\
  output\", \"\") or item.get(\"output_text\", \"\")\n       output = output.lower()\n\
  \       \n       # Get the expected label/ground truth from item\n       label =\
  \ item.get(\"ground_truth\", \"\") or item.get(\"label\", \"\") or item.get(\"expected\"\
  , \"\")\n       label = label.lower()\n       \n       # Handle empty cases\n  \
  \     if not output or not label:\n           return 0.0\n       \n       # Exact\
  \ match gets full score\n       if output == label:\n           return 1.0\n   \
  \    \n       # Partial match logic (customize as needed)\n       if output in label\
  \ or label in output:\n           return 0.5\n       \n       return 0.0\n   \"\"\
  \",\n   )\n\n   # Run evaluation\n   evaluation_result = evaluate(\n       data=\"\
  evaluation_data.jsonl\",  # JSONL file with columns: query, response, ground_truth,\
  \ etc.\n       evaluators={\"custom_accuracy\": python_grader},\n   )\n\n   # Access\
  \ results\n   print(f\"Pass rate: {evaluation_result['metrics']['custom_accuracy.pass_rate']}\"\
  )\n\n   ````\n"
methods:
- uid: azure.ai.evaluation.AzureOpenAIPythonGrader.get_client
  name: get_client
  summary: 'Construct an appropriate OpenAI client using this grader''s model configuration.

    Returns a slightly different client depending on whether or not this grader''s
    model

    configuration is for Azure OpenAI or OpenAI.'
  signature: get_client() -> Any
  return:
    description: The OpenAI client.
    types:
    - '[<xref:openai.OpenAI>, <xref:openai.AzureOpenAI>]'
attributes:
- uid: azure.ai.evaluation.AzureOpenAIPythonGrader.id
  name: id
  signature: id = 'azureai://built-in/evaluators/azure-openai/python_grader'
