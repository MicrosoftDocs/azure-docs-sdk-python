### YamlMime:PythonClass
uid: azure.ai.evaluation.GleuScoreEvaluator
name: GleuScoreEvaluator
fullName: azure.ai.evaluation.GleuScoreEvaluator
module: azure.ai.evaluation
summary: 'Calculates the GLEU (Google-BLEU) score between a response and the ground
  truth.


  The GLEU (Google-BLEU) score evaluator measures the similarity between generated
  and reference texts by

  evaluating n-gram overlap, considering both precision and recall. This balanced
  evaluation, designed for

  sentence-level assessment, makes it ideal for detailed analysis of translation quality.
  GLEU is well-suited for

  use cases such as machine translation, text summarization, and text generation.


  GLEU scores range from 0 to 1, where a value of 1 represents perfect overlap between
  the response and

  the ground truth and a value of 0 indicates no overlap.'
constructor:
  syntax: GleuScoreEvaluator()
examples:
- "Initialize and call a GleuScoreEvaluator.<!--[!code-python[Main](les\\evaluation_samples_evaluate.py\
  \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
  : [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\\Python\\\
  \\3.11.10\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\14\\\\azure_ai_evaluation-1.2.0\\\
  \\samples\\\\evaluation_samples_evaluate.py\", \"xml:space\": \"preserve\", \"force\"\
  : false, \"language\": \"python\", \"highlight_args\": {\"linenostart\": 1}, \"\
  linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation import GleuScoreEvaluator\n\
  \n   gleu_evaluator = GleuScoreEvaluator()\n   gleu_evaluator(response=\"Paris is\
  \ the capital of France.\", ground_truth=\"France's capital is Paris.\")\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.GleuScoreEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'azureml://registries/azureml/models/Gleu-Score-Evaluator/versions/3'
