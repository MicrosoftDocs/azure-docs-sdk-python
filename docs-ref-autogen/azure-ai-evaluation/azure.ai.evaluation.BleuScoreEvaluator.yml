### YamlMime:PythonClass
uid: azure.ai.evaluation.BleuScoreEvaluator
name: BleuScoreEvaluator
fullName: azure.ai.evaluation.BleuScoreEvaluator
module: azure.ai.evaluation
summary: 'Calculate the BLEU score for a given response and ground truth.


  BLEU (Bilingual Evaluation Understudy) score is commonly used in natural language
  processing (NLP) and machine

  translation. It is widely used in text summarization and text generation use cases.


  Use the BLEU score when you want to evaluate the similarity between the generated
  text and reference text,

  especially in tasks such as machine translation or text summarization, where n-gram
  overlap is a significant

  indicator of quality.


  The BLEU score ranges from 0 to 1, with higher scores indicating better quality.'
constructor:
  syntax: BleuScoreEvaluator()
examples:
- "Initialize and call an BleuScoreEvaluator.<!--[!code-python[Main](les\\evaluation_samples_evaluate.py\
  \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
  : [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\\windows\\\\Python\\\
  \\3.11.10\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\\14\\\\azure_ai_evaluation-1.2.0\\\
  \\samples\\\\evaluation_samples_evaluate.py\", \"xml:space\": \"preserve\", \"force\"\
  : false, \"language\": \"python\", \"highlight_args\": {\"linenostart\": 1}, \"\
  linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation import BleuScoreEvaluator\n\
  \n   bleu_evaluator = BleuScoreEvaluator()\n   bleu_evaluator(response=\"Lyon is\
  \ the capital of France.\", ground_truth=\"Paris is the capital of France.\")\n\n\
  \   ````\n"
attributes:
- uid: azure.ai.evaluation.BleuScoreEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'azureml://registries/azureml/models/Bleu-Score-Evaluator/versions/3'
