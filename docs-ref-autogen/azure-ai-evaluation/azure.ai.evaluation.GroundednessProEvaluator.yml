### YamlMime:PythonClass
uid: azure.ai.evaluation.GroundednessProEvaluator
name: GroundednessProEvaluator
fullName: azure.ai.evaluation.GroundednessProEvaluator
module: azure.ai.evaluation
inheritances:
- azure.ai.evaluation._evaluators._common._base_rai_svc_eval.RaiServiceEvaluatorBase
summary: '> [!NOTE]

  > This is an experimental class, and may change at any time. Please see [https://aka.ms/azuremlexperimental](https://aka.ms/azuremlexperimental)
  for more information.

  >


  Evaluates service-based groundedness score for a given response, context, and query
  or a multi-turn conversation,

  including reasoning.


  The groundedness measure calls Azure AI Evaluation service to assess how well the
  AI-generated answer is grounded

  in the source context. Even if the responses from LLM are factually correct, they''ll
  be considered ungrounded if

  they can''t be verified against the provided sources (such as your input source
  or your database).


  Service-based groundedness scores are boolean values, where True indicates that
  the response is grounded.


  > [!NOTE]

  > If this evaluator is supplied to the evaluate function, the aggregated metric

  >

  > for the groundedness pro label will be "groundedness_pro_passing_rate".

  >'
constructor:
  syntax: GroundednessProEvaluator(credential, azure_ai_project, **kwargs)
  parameters:
  - name: credential
    description: The credential for connecting to Azure AI project. Required
    isRequired: true
    types:
    - <xref:azure.core.credentials.TokenCredential>
  - name: azure_ai_project
    description: 'The scope of the Azure AI project.

      It contains subscription id, resource group, and project name.'
    isRequired: true
    types:
    - <xref:azure.ai.evaluation.AzureAIProject>
  - name: kwargs
    description: Additional arguments to pass to the evaluator.
    isRequired: true
    types:
    - <xref:typing.Any>
examples:
- "Initialize and call a GroundednessProEvaluator with a query, response, and context.<!--[!code-python[Main](les\\\
  evaluation_samples_evaluate.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
  : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"C:\\\\hostedtoolcache\\\
  \\windows\\\\Python\\\\3.11.9\\\\x64\\\\Lib\\\\site-packages\\\\py2docfx\\\\dist_temp\\\
  \\4\\\\azure_ai_evaluation-1.2.0\\\\samples\\\\evaluation_samples_evaluate.py\"\
  , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\"\
  : {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\n   import os\n\
  \   from azure.identity import DefaultAzureCredential\n   from azure.ai.evaluation\
  \ import GroundednessProEvaluator\n\n   azure_ai_project = {\n       \"subscription_id\"\
  : os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n       \"resource_group_name\": os.environ.get(\"\
  AZURE_RESOURCE_GROUP_NAME\"),\n       \"project_name\": os.environ.get(\"AZURE_PROJECT_NAME\"\
  ),\n   }\n   credential = DefaultAzureCredential()\n\n   groundedness_pro_eval =\
  \ GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=credential)\n\
  \   groundedness_pro_eval(\n       query=\"What shape has 4 equilateral sides?\"\
  ,\n       response=\"Rhombus\",\n       context=\"Rhombus is a shape with 4 equilateral\
  \ sides.\",\n   )\n\n   ````\n"
attributes:
- uid: azure.ai.evaluation.GroundednessProEvaluator.id
  name: id
  summary: Evaluator identifier, experimental and to be used only with evaluation
    in cloud.
  signature: id = 'azureml://registries/azureml/models/Groundedness-Pro-Evaluator/versions/1'
