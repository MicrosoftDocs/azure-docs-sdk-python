### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.datalake.store.transfer.ADLTransferClient.active
  - azure.datalake.store.transfer.ADLTransferClient.monitor
  - azure.datalake.store.transfer.ADLTransferClient.progress
  - azure.datalake.store.transfer.ADLTransferClient.run
  - azure.datalake.store.transfer.ADLTransferClient.save
  - azure.datalake.store.transfer.ADLTransferClient.shutdown
  - azure.datalake.store.transfer.ADLTransferClient.submit
  - azure.datalake.store.transfer.ADLTransferClient.successful
  class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient
  inheritance:
  - type: builtins.object
  langs:
  - python
  module: azure.datalake.store.transfer
  name: ADLTransferClient
  seealsoContent: "See also: @azure.datalake.store.multithread.ADLDownloader, @azure.datalake.store.multithread.ADLUploader\
    \ \n"
  source:
    id: ADLTransferClient
    path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
    remote:
      branch: master
      path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 114
  summary: 'Client for transferring data from/to Azure DataLake Store


    This is intended as the underlying class for *ADLDownloader* and

    *ADLUploader*. If necessary, it can be used directly for additional

    control.










    '
  syntax:
    content: ADLTransferClient(adlfs, transfer, merge=None, nthreads=None, chunksize=268435456,
      blocksize=33554432, chunked=True, unique_temporary=True, delimiter=None, parent=None,
      verbose=False, buffersize=33554432, progress_callback=None)
    parameters:
    - description: ''
      id: adlfs
      type:
      - ADL filesystem instance
    - description: 'Unique ID used for persistence.

        '
      id: name
      type:
      - str
    - description: 'Function or callable object invoked when transferring chunks.
        See

        `Function Signatures`.

        '
      id: transfer
      type:
      - callable
    - description: 'Function or callable object invoked when merging chunks. For each
        file

        containing only one chunk, no merge function will be called, even if

        provided. If None, then merging is skipped. See

        `Function Signatures`.

        '
      id: merge
      type:
      - callable [None]
    - description: 'Number of threads to use (minimum is 1). If None, uses the number
        of

        cores.

        '
      id: nthreads
      type:
      - int [None]
    - description: 'Number of bytes for a chunk. Large files are split into chunks.
        Files

        smaller than this number will always be transferred in a single thread.

        '
      id: chunksize
      type:
      - int [2**28]
    - description: 'Number of bytes for internal buffer. This block cannot be bigger
        than

        a chunk and cannot be smaller than a block.

        '
      id: buffersize
      type:
      - int [2**25]
    - description: 'Number of bytes for a block. Within each chunk, we write a smaller

        block for each API call. This block cannot be bigger than a chunk.

        '
      id: blocksize
      type:
      - int [2**25]
    - description: 'If set, each transferred chunk is stored in a separate file until

        chunks are gathered into a single file. Otherwise, each chunk will be

        written into the same destination file.

        '
      id: chunked
      type:
      - bool [True]
    - description: 'If set, transferred chunks are written into a unique temporary

        directory.

        '
      id: unique_temporary
      type:
      - bool [True]
    - description: 'Path used for persisting a client''s state. If None, then *save()*

        and *load()* will be empty operations.

        '
      id: persist_path
      type:
      - str [None]
    - description: 'If set, will transfer blocks using delimiters, as well as split

        files for transferring on that delimiter.

        '
      id: delimiter
      type:
      - byte(s)
      - None
    - description: 'In typical usage, the transfer client is created in the context
        of an

        upload or download, which can be persisted between sessions.

        '
      id: parent
      type:
      - ADLDownloader, ADLUploader
      - None
    - description: 'Callback for progress with signature function(current, total)
        where

        current is the number of bytes transferred so far, and total is the

        size of the blob, or None if the total size is unknown.

        '
      id: progress_callback
      type:
      - callable [None]
    - description: ''
      id: Files
      type:
      - Temporary
    - description: ''
      id: '---------------'
    - description: ''
      id: a merge step is available, the client will write chunks to temporary
      type:
      - When
    - description: ''
      id: before merging. The exact temporary file looks like this in
      type:
      - files
    - description: ''
      id: pseudo-BNF
    - description: ''
      id: '# {dirname}/{basename}.segments[.{unique_str}]/{basename}_{offset}'
      type:
      - '>>>'
    - description: ''
      id: Signatures
      type:
      - Function
    - description: ''
      id: '-------------------'
    - description: ''
      id: perform the actual work needed by the client, the user must pass in two
      type:
      - To
    - description: ''
      id: transfer and merge. If merge is not provided, then the
      type:
      - callables,
    - description: ''
      id: step will be skipped.
      type:
      - merge
    - description: ''
      id: transfer callable has the function signature,
      type:
      - The
    - description: ''
      id: src, dst, offset, size, buffersize, blocksize, shutdown_event).
      type:
      - fn(adlfs,
    - description: ''
      id: is the ADL filesystem instance. src and dst refer to the source
      type:
      - adlfs
    - description: ''
      id: destination of the respective file transfer. offset is the location
      type:
      - and
    - description: ''
      id: src to read size bytes from. buffersize is the number of bytes
      type:
      - in
    - description: ''
      id: for internal buffering before transfer. blocksize is the number of
      type:
      - used
    - description: ''
      id: in a chunk to write at one time. The callable should return an
      type:
      - bytes
    - description: ''
      id: representing the number of bytes written.
      type:
      - integer
    - description: ''
      id: merge callable has the function signature,
      type:
      - The
    - description: ''
      id: outfile, files, shutdown_event). adlfs is the ADL filesystem
      type:
      - fn(adlfs,
    - description: ''
      id: outfile is the result of merging files.
      type:
      - instance.
    - description: ''
      id: both transfer callables, shutdown_event is optional. In particular,
      type:
      - For
    - description: ''
      id: is a threading.Event that is passed to the callable.
      type:
      - shutdown_event
    - description: ''
      id: event will be set when a shutdown is requested. It is good practice
      type:
      - The
    - description: ''
      id: listen for this.
      type:
      - to
    - description: ''
      id: State
      type:
      - Internal
    - description: ''
      id: '--------------'
    - description: 'This captures the current state of each transferred file.

        '
      id: self._fstates
      type:
      - StateManager
    - description: 'Using a tuple of the file source/destination as the key, this

        dictionary stores the file metadata and all chunk states. The

        dictionary key is *(src, dst)* and the value is

        *dict(length, cstates, exception)*.

        '
      id: self._files
      type:
      - dict
    - description: 'Using a tuple of the chunk name/offset as the key, this dictionary

        stores the chunk metadata and has a reference to the chunk''s parent

        file. The dictionary key is *(name, offset)* and the value is

        *dict(parent=(src, dst), expected, actual, exception)*.

        '
      id: self._chunks
      type:
      - dict
    - description: 'Using a Future object as the key, this dictionary provides a reverse

        lookup for the file associated with the given future. The returned

        value is the file''s primary key, *(src, dst)*.

        '
      id: self._ffutures
      type:
      - dict
    - description: 'Using a Future object as the key, this dictionary provides a reverse

        lookup for the chunk associated with the given future. The returned

        value is the chunk''s primary key, *(name, offset)*.

        '
      id: self._cfutures
      type:
      - dict
  type: class
  uid: azure.datalake.store.transfer.ADLTransferClient
- class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient.active
  langs:
  - python
  module: azure.datalake.store.transfer
  name: active
  source:
    id: active
    path: null
    remote:
      branch: master
      path: null
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: null
  summary: 'Return whether the transfer is active

    '
  syntax: {}
  type: attribute
  uid: azure.datalake.store.transfer.ADLTransferClient.active
- class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient.monitor
  langs:
  - python
  module: azure.datalake.store.transfer
  name: monitor
  source:
    id: monitor
    path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
    remote:
      branch: master
      path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 554
  summary: 'Wait for download to happen

    '
  syntax:
    content: monitor(poll=0.1, timeout=0)
    parameters:
    - defaultValue: '0.1'
      id: poll
    - defaultValue: '0'
      id: timeout
  type: method
  uid: azure.datalake.store.transfer.ADLTransferClient.monitor
- class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient.progress
  langs:
  - python
  module: azure.datalake.store.transfer
  name: progress
  source:
    id: progress
    path: null
    remote:
      branch: master
      path: null
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: null
  summary: 'Return a summary of all transferred file/chunks

    '
  syntax: {}
  type: attribute
  uid: azure.datalake.store.transfer.ADLTransferClient.progress
- class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient.run
  langs:
  - python
  module: azure.datalake.store.transfer
  name: run
  source:
    id: run
    path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
    remote:
      branch: master
      path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 486
  syntax:
    content: run(nthreads=None, monitor=True, before_start=None)
    parameters:
    - defaultValue: None
      id: nthreads
    - defaultValue: 'True'
      id: monitor
    - defaultValue: None
      id: before_start
  type: method
  uid: azure.datalake.store.transfer.ADLTransferClient.run
- class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient.save
  langs:
  - python
  module: azure.datalake.store.transfer
  name: save
  source:
    id: save
    path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
    remote:
      branch: master
      path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 580
  syntax:
    content: save(keep=True)
    parameters:
    - defaultValue: 'True'
      id: keep
  type: method
  uid: azure.datalake.store.transfer.ADLTransferClient.save
- class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient.shutdown
  langs:
  - python
  module: azure.datalake.store.transfer
  name: shutdown
  source:
    id: shutdown
    path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
    remote:
      branch: master
      path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 534
  summary: 'Shutdown task threads in an orderly fashion.


    Within the context of this method, we disable Ctrl+C keystroke events

    until all threads have exited. We re-enable Ctrl+C keystroke events

    before leaving.

    '
  syntax:
    content: shutdown()
    parameters: []
  type: method
  uid: azure.datalake.store.transfer.ADLTransferClient.shutdown
- class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient.submit
  langs:
  - python
  module: azure.datalake.store.transfer
  name: submit
  source:
    id: submit
    path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
    remote:
      branch: master
      path: azure_datalake_store-0.0.21-py3.6.egg\azure\datalake\store\transfer.py
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: 264
  summary: 'Split a given file into chunks.


    All submitted files/chunks start in the *pending* state until *run()*

    is called.

    '
  syntax:
    content: submit(src, dst, length)
    parameters:
    - id: src
    - id: dst
    - id: length
  type: method
  uid: azure.datalake.store.transfer.ADLTransferClient.submit
- class: azure.datalake.store.transfer.ADLTransferClient
  fullName: azure.datalake.store.transfer.ADLTransferClient.successful
  langs:
  - python
  module: azure.datalake.store.transfer
  name: successful
  source:
    id: successful
    path: null
    remote:
      branch: master
      path: null
      repo: https://github.com/Azure/azure-data-lake-store-python
    startLine: null
  summary: 'Return whether the transfer completed successfully.


    It will raise AssertionError if the transfer is active.

    '
  syntax: {}
  type: attribute
  uid: azure.datalake.store.transfer.ADLTransferClient.successful
references:
- fullName: azure.datalake.store.transfer.ADLTransferClient.active
  isExternal: false
  name: active
  parent: azure.datalake.store.transfer.ADLTransferClient
  uid: azure.datalake.store.transfer.ADLTransferClient.active
- fullName: azure.datalake.store.transfer.ADLTransferClient.monitor
  isExternal: false
  name: monitor
  parent: azure.datalake.store.transfer.ADLTransferClient
  uid: azure.datalake.store.transfer.ADLTransferClient.monitor
- fullName: azure.datalake.store.transfer.ADLTransferClient.progress
  isExternal: false
  name: progress
  parent: azure.datalake.store.transfer.ADLTransferClient
  uid: azure.datalake.store.transfer.ADLTransferClient.progress
- fullName: azure.datalake.store.transfer.ADLTransferClient.run
  isExternal: false
  name: run
  parent: azure.datalake.store.transfer.ADLTransferClient
  uid: azure.datalake.store.transfer.ADLTransferClient.run
- fullName: azure.datalake.store.transfer.ADLTransferClient.save
  isExternal: false
  name: save
  parent: azure.datalake.store.transfer.ADLTransferClient
  uid: azure.datalake.store.transfer.ADLTransferClient.save
- fullName: azure.datalake.store.transfer.ADLTransferClient.shutdown
  isExternal: false
  name: shutdown
  parent: azure.datalake.store.transfer.ADLTransferClient
  uid: azure.datalake.store.transfer.ADLTransferClient.shutdown
- fullName: azure.datalake.store.transfer.ADLTransferClient.submit
  isExternal: false
  name: submit
  parent: azure.datalake.store.transfer.ADLTransferClient
  uid: azure.datalake.store.transfer.ADLTransferClient.submit
- fullName: azure.datalake.store.transfer.ADLTransferClient.successful
  isExternal: false
  name: successful
  parent: azure.datalake.store.transfer.ADLTransferClient
  uid: azure.datalake.store.transfer.ADLTransferClient.successful
- fullName: callable [None]
  name: callable [None]
  spec.python:
  - fullName: 'callable '
    name: 'callable '
    uid: 'callable '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: callable [None]
- fullName: int [None]
  name: int [None]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: int [None]
- fullName: int [2**28]
  name: int [2**28]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**28
    name: 2**28
    uid: 2**28
  - fullName: ']'
    name: ']'
  uid: int [2**28]
- fullName: int [2**25]
  name: int [2**25]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**25
    name: 2**25
    uid: 2**25
  - fullName: ']'
    name: ']'
  uid: int [2**25]
- fullName: int [2**25]
  name: int [2**25]
  spec.python:
  - fullName: 'int '
    name: 'int '
    uid: 'int '
  - fullName: '['
    name: '['
  - fullName: 2**25
    name: 2**25
    uid: 2**25
  - fullName: ']'
    name: ']'
  uid: int [2**25]
- fullName: bool [True]
  name: bool [True]
  spec.python:
  - fullName: 'bool '
    name: 'bool '
    uid: 'bool '
  - fullName: '['
    name: '['
  - fullName: 'True'
    name: 'True'
    uid: 'True'
  - fullName: ']'
    name: ']'
  uid: bool [True]
- fullName: bool [True]
  name: bool [True]
  spec.python:
  - fullName: 'bool '
    name: 'bool '
    uid: 'bool '
  - fullName: '['
    name: '['
  - fullName: 'True'
    name: 'True'
    uid: 'True'
  - fullName: ']'
    name: ']'
  uid: bool [True]
- fullName: str [None]
  name: str [None]
  spec.python:
  - fullName: 'str '
    name: 'str '
    uid: 'str '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: str [None]
- fullName: byte(s)
  name: byte(s)
  spec.python:
  - fullName: byte
    name: byte
    uid: byte
  - fullName: (
    name: (
  - fullName: s
    name: s
    uid: s
  - fullName: )
    name: )
  uid: byte(s)
- fullName: ADLDownloader, ADLUploader
  name: ADLDownloader, ADLUploader
  spec.python:
  - fullName: ADLDownloader
    name: ADLDownloader
    uid: ADLDownloader
  - fullName: ', '
    name: ', '
  - fullName: ADLUploader
    name: ADLUploader
    uid: ADLUploader
  uid: ADLDownloader, ADLUploader
- fullName: callable [None]
  name: callable [None]
  spec.python:
  - fullName: 'callable '
    name: 'callable '
    uid: 'callable '
  - fullName: '['
    name: '['
  - fullName: None
    name: None
    uid: None
  - fullName: ']'
    name: ']'
  uid: callable [None]
- fullName: fn(adlfs,
  name: fn(adlfs,
  spec.python:
  - fullName: fn
    name: fn
    uid: fn
  - fullName: (
    name: (
  - fullName: adlfs,
    name: adlfs,
    uid: adlfs,
  uid: fn(adlfs,
- fullName: fn(adlfs,
  name: fn(adlfs,
  spec.python:
  - fullName: fn
    name: fn
    uid: fn
  - fullName: (
    name: (
  - fullName: adlfs,
    name: adlfs,
    uid: adlfs,
  uid: fn(adlfs,
